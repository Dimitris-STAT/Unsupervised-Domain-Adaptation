{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fEVUVVTFfvB8","executionInfo":{"status":"ok","timestamp":1719592537612,"user_tz":-180,"elapsed":77315,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["%%capture\n","%pip install torchvision\n","%pip install torchsummary\n","%pip install torchviz\n","%pip install hiddenlayer"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FhMG3MXfvB9","executionInfo":{"status":"ok","timestamp":1719592562424,"user_tz":-180,"elapsed":24824,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"b17a6598-6531-47ec-8ea0-48ffdc142a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Deep_Learning\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Deep_Learning"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RbnqL7rKfvB-","executionInfo":{"status":"ok","timestamp":1719592567715,"user_tz":-180,"elapsed":5294,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader, Subset\n","from torchsummary import summary\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOdZDt2sfvB-","executionInfo":{"status":"ok","timestamp":1719592567715,"user_tz":-180,"elapsed":14,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"d9075aba-6d65-4d09-f764-5e787fbe7b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Jun 28 16:36:07 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"c7SizK3bkrRb"}},{"cell_type":"code","source":["#model params\n","params = {\n","    'batch_size':32,\n","    'mul': 30,\n","    'num_classes':10,\n","    'channels':3,\n","    'img_dim':32,\n","    'device':torch.device('cuda') if torch.cuda.is_available() else 'cpu',\n","    'fliplr':True,\n","    'num_epochs':50,\n","    'decay_epoch':50,\n","    'lr':0.00002,    #learning rate for generator\n","    'beta1':0.5 ,    #beta1 for Adam optimizer\n","    'beta2':0.999 ,  #beta2 for Adam optimizer\n","    'lambdaA':10 ,   #lambdaA for cycle loss\n","    'lambdaB':10  ,  #lambdaB for cycle loss\n","}\n"],"metadata":{"id":"Gz98n0M8G_WU","executionInfo":{"status":"ok","timestamp":1719592567715,"user_tz":-180,"elapsed":2,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to load and preprocess datasets\n","def load_dataset(dataset_cls, split, root, download, normalize=False, subset_size=None, batch_size=params['batch_size']):\n","    \"\"\"\n","      Load and preprocess a dataset.\n","      :param dataset_cls: Dataset class (e.g., datasets.MNIST, datasets.SVHN)\n","      :param split: Data split ('train' or 'test')\n","      :param root: Root directory for dataset storage/download\n","      :param download: Flag to download the dataset if not present\n","      :param normalize: Flag to normalize the dataset\n","      :param subset_size: Size of the subset to use (if specified)\n","      :param batch_size: Batch size for DataLoader\n","      :return: DataLoader object\n","    \"\"\"\n","    if dataset_cls == datasets.MNIST:\n","        dataset = dataset_cls(root=root, train=(split=='train'), download=download, transform=transforms.ToTensor())\n","    else:\n","        dataset = dataset_cls(root=root, split=split, download=download, transform=transforms.ToTensor())\n","\n","    if normalize:\n","        # mean, std = compute_mean_and_std(dataset)\n","\n","        if dataset_cls == datasets.MNIST:\n","\n","            print(f\"Dataset is MNIST: {dataset_cls}\")\n","            transform = transforms.Compose([\n","                transforms.Resize(( params['img_dim'],params['img_dim']) ),\n","                transforms.Grayscale(params['channels']),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5]*params['channels'], std=[0.5]*params['channels'])\n","            ])\\\n","\n","        else:\n","            print(f\"Dataset is SVHN: {dataset_cls}\")\n","            transform = transforms.Compose([\n","                transforms.Resize(( params['img_dim'],params['img_dim'] )),\n","                transforms.Grayscale(params['channels']),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5]*params['channels'], std=[0.5]*params['channels'])\n","            ])\n","\n","        # Apply the computed normalization\n","        dataset.transform = transform\n","\n","    if subset_size:\n","        indices = np.random.choice(len(dataset), subset_size, replace=False)\n","        print(dataset.transform)\n","        dataset = Subset(dataset, indices)\n","\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True if split == 'train' else False, num_workers=1)\n","    return loader\n","\n","# Load MNIST datasets\n","mnist_trainloader = load_dataset(datasets.MNIST, 'train', './data/', True, normalize=True,subset_size=params['batch_size']*params['mul'] )\n","mnist_testloader = load_dataset(datasets.MNIST, 'test', './data/', True, normalize=True)\n","\n","# Load SVHN datasets\n","svhn_trainloader = load_dataset(datasets.SVHN, 'train', './data/', True, normalize=True,subset_size=params['batch_size']*params['mul'])\n","svhn_testloader = load_dataset(datasets.SVHN, 'test', './data/', True, normalize=True)\n","\n","# Optionally, load the extra SVHN dataset if needed\n","# svhn_extraloader = load_dataset(datasets.SVHN, 'extra', './data/', True, normalize=True, subset_size=20000)\n","\n","# Verify the sizes of the datasets\n","print(f'MNIST train subset size: {len(mnist_trainloader.dataset)}')\n","# print(f'SVHN train subset size: {len(svhn_trainloader.dataset)}')\n","# print(f'SVHN extra size: {len(svhn_extraloader.dataset)}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCm5YC5CkxpO","executionInfo":{"status":"ok","timestamp":1719592600261,"user_tz":-180,"elapsed":32548,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"affb0e6b-aee9-4a71-97b6-1d089d57a37e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset is MNIST: <class 'torchvision.datasets.mnist.MNIST'>\n","Compose(\n","    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n","    Grayscale(num_output_channels=3)\n","    ToTensor()\n","    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",")\n","Dataset is MNIST: <class 'torchvision.datasets.mnist.MNIST'>\n","Using downloaded and verified file: ./data/train_32x32.mat\n","Dataset is SVHN: <class 'torchvision.datasets.svhn.SVHN'>\n","Compose(\n","    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n","    Grayscale(num_output_channels=3)\n","    ToTensor()\n","    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",")\n","Using downloaded and verified file: ./data/test_32x32.mat\n","Dataset is SVHN: <class 'torchvision.datasets.svhn.SVHN'>\n","MNIST train subset size: 960\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Define the transformations (same as during training)\n","transform = transforms.Compose([\n","    transforms.Grayscale(3),  # Ensure images are single-channel grayscale\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*params['channels'], [0.5]*params['channels'])  # Assuming normalization parameters used during training\n","])\n","\n","# Define the path to the transformed images\n","transformed_images_path = 'transformed_mnist_train'\n","\n","# Create the dataset\n","transformed_dataset = datasets.ImageFolder(root=transformed_images_path, transform=transform)\n","\n","# Create the DataLoader\n","transformed_dataloader = DataLoader(transformed_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=8)"],"metadata":{"id":"c2plUzc8y30g","executionInfo":{"status":"ok","timestamp":1719592607393,"user_tz":-180,"elapsed":7143,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["### Baseline Model\n","\n","Our baseline model here is a simple CNN."],"metadata":{"id":"h793IQ9Ak0pp"}},{"cell_type":"code","source":["class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Use a dummy input to determine the size of the flattened features\n","        dummy_input = torch.randn(1, 3, 32, 32)  # Change the input size if necessary\n","        dummy_output = self.feature_extractor(dummy_input)\n","        flattened_size = dummy_output.view(-1).size(0)\n","\n","        # Now use the calculated flattened size for the classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(flattened_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10)\n","        )\n","\n","    def forward(self, x):\n","        features = self.feature_extractor(x)\n","        features = features.view(features.size(0), -1)\n","        output = self.classifier(features)\n","        return output\n","\n","class DomainDiscriminator(nn.Module):\n","    def __init__(self, feature_size):\n","        super(DomainDiscriminator, self).__init__()\n","        self.domain_classifier = nn.Sequential(\n","            nn.Linear(feature_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 2)  # Two classes: source and target\n","        )\n","\n","    def forward(self, x):\n","        output = self.domain_classifier(x)\n","        return output\n","\n","# Initialize the model and print the flattened feature size\n","model = SimpleCNN()\n","print(model)\n"],"metadata":{"id":"eMK4S4tDlArM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1719592607394,"user_tz":-180,"elapsed":11,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"9899883c-f917-498f-acdd-3e5adac6ead9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (feature_extractor): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=2048, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["feature_extractor = SimpleCNN().to(params['device'])\n","flattened_size = feature_extractor.classifier[0].in_features  # Get the input feature size of the first Linear layer\n","domain_discriminator = DomainDiscriminator(flattened_size).to(params['device'])\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(list(feature_extractor.parameters()) + list(domain_discriminator.parameters()), lr=0.00001)\n"],"metadata":{"id":"ivbVo77CnOUv","executionInfo":{"status":"ok","timestamp":1719592607394,"user_tz":-180,"elapsed":10,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["#### Baseline model Training"],"metadata":{"id":"QCSd9yHYnUoa"}},{"cell_type":"code","source":["def train(model, domain_model, source_loader, target_loader, criterion, optimizer, num_epochs=20):\n","    \"\"\"\n","      Train the models.\n","      :param model: Main model\n","      :param domain_model: Domain discriminator model\n","      :param source_loader: DataLoader for source data\n","      :param target_loader: DataLoader for target data\n","      :param criterion: Loss function\n","      :param optimizer: Optimizer for model parameters\n","      :param num_epochs: Number of training epochs\n","      :return: None\n","    \"\"\"\n","    model.train()\n","    domain_model.train()\n","\n","    for epoch in range(num_epochs):\n","        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n","            source_data, source_labels = source_data.to(params['device']), source_labels.to(params['device'])\n","            target_data = target_data.to(params['device'])\n","\n","            # Train feature extractor and classifier on source data\n","            optimizer.zero_grad()\n","            source_features = model.feature_extractor(source_data)\n","            source_output = model.classifier(source_features.view(source_features.size(0), -1))\n","            source_loss = criterion(source_output, source_labels)\n","            source_loss.backward()\n","            optimizer.step()\n","\n","            # Train domain discriminator on source and target data\n","            optimizer.zero_grad()\n","            domain_labels = torch.cat((torch.zeros(source_data.size(0)), torch.ones(target_data.size(0)))).long().to(params['device'])\n","            combined_data = torch.cat((source_data, target_data))\n","            combined_features = model.feature_extractor(combined_data)\n","            domain_output = domain_model(combined_features.view(combined_features.size(0), -1))\n","            domain_loss = criterion(domain_output, domain_labels)\n","            domain_loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Source Loss: {source_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}')\n","\n","# Train the model\n","train(feature_extractor, domain_discriminator, mnist_trainloader, svhn_trainloader, criterion, optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuSLvHL9nT2J","executionInfo":{"status":"ok","timestamp":1719592619274,"user_tz":-180,"elapsed":11346,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"8da79a3c-0c61-492c-df44-304b88a962e0"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Source Loss: 2.0953, Domain Loss: 0.1693\n","Epoch [2/20], Source Loss: 1.9892, Domain Loss: 0.0528\n","Epoch [3/20], Source Loss: 1.6882, Domain Loss: 0.0260\n","Epoch [4/20], Source Loss: 1.6248, Domain Loss: 0.0136\n","Epoch [5/20], Source Loss: 1.3863, Domain Loss: 0.0111\n","Epoch [6/20], Source Loss: 1.3057, Domain Loss: 0.0093\n","Epoch [7/20], Source Loss: 1.0706, Domain Loss: 0.0064\n","Epoch [8/20], Source Loss: 1.1067, Domain Loss: 0.0043\n","Epoch [9/20], Source Loss: 1.0098, Domain Loss: 0.0047\n","Epoch [10/20], Source Loss: 0.8393, Domain Loss: 0.0033\n","Epoch [11/20], Source Loss: 0.7522, Domain Loss: 0.0035\n","Epoch [12/20], Source Loss: 0.6173, Domain Loss: 0.0025\n","Epoch [13/20], Source Loss: 0.6056, Domain Loss: 0.0026\n","Epoch [14/20], Source Loss: 0.5125, Domain Loss: 0.0022\n","Epoch [15/20], Source Loss: 0.5702, Domain Loss: 0.0030\n","Epoch [16/20], Source Loss: 0.5472, Domain Loss: 0.0013\n","Epoch [17/20], Source Loss: 0.4248, Domain Loss: 0.0022\n","Epoch [18/20], Source Loss: 0.2203, Domain Loss: 0.0019\n","Epoch [19/20], Source Loss: 0.4215, Domain Loss: 0.0007\n","Epoch [20/20], Source Loss: 0.3799, Domain Loss: 0.0009\n"]}]},{"cell_type":"code","source":["def evaluate(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, labels in loader:\n","            data, labels = data.to(params['device']), labels.to(params['device'])\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","source_accuracy = evaluate(feature_extractor, mnist_testloader, params['device'])\n","target_accuracy = evaluate(feature_extractor, svhn_testloader, params['device'])\n","print(f'Source (MNIST) Accuracy: {source_accuracy:.2f}%')\n","print(f'Target (SVHN) Accuracy: {target_accuracy:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPIR0pNbCmKH","executionInfo":{"status":"ok","timestamp":1719592633910,"user_tz":-180,"elapsed":14645,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"2277ef13-cfe1-444b-a3b5-9804d47bb432"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Source (MNIST) Accuracy: 93.00%\n","Target (SVHN) Accuracy: 20.31%\n"]}]},{"cell_type":"markdown","source":["### VGG16 architexture"],"metadata":{"id":"t9LielkWngZx"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"EZjZdudjfvB_","executionInfo":{"status":"ok","timestamp":1719592633910,"user_tz":-180,"elapsed":11,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["# Define the VGG-16 architecture without pretrained weights\n","class VGG16Custom(nn.Module):\n","    def __init__(self, num_classes=params['num_classes']):\n","        super(VGG16Custom, self).__init__()\n","        # Load the VGG-16 model\n","        self.features = models.vgg16(weights=None).features\n","        # Modify the first convolutional layer to accept 1-channel input\n","        self.features[0] = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","        # Add a custom classifier (fully connected layers)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 1 * 1, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), 512 * 1 * 1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"YcrgcgmjfvB_","executionInfo":{"status":"ok","timestamp":1719592633910,"user_tz":-180,"elapsed":11,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["def CORAL(source, target):\n","    d = source.data.shape[1]\n","\n","    # source covariance\n","    xm = torch.mean(source, 0, keepdim=True) - source\n","    xc = xm.t() @ xm\n","\n","    # target covariance\n","    xmt = torch.mean(target, 0, keepdim=True) - target\n","    xct = xmt.t() @ xmt\n","\n","    # frobenius norm between source and target\n","    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n","    loss = loss/(4*d*d)\n","\n","    return loss\n"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"7Qi6OJkofvCA","executionInfo":{"status":"ok","timestamp":1719593797994,"user_tz":-180,"elapsed":3342,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dac8645d-b434-4ec7-e01b-9321310c202d"},"outputs":[{"output_type":"stream","name":"stdout","text":["VGG16Custom(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=512, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["class DeepCORAL(nn.Module):\n","    def __init__(self, num_classes=params['num_classes']):\n","        super(DeepCORAL, self).__init__()\n","        self.sharedNet = VGG16Custom()\n","        self.fc = nn.Linear(512, num_classes)\n","\n","        # initialize according to CORAL paper experiment\n","        self.fc.weight.data.normal_(0, 0.005)\n","\n","    def forward(self, source, target):\n","        source = self.sharedNet(source)\n","        source = self.fc(source)\n","\n","        target = self.sharedNet(target)\n","        target = self.fc(target)\n","        return source, target\n","\n","# Function to load pretrained weights into the DeepCORAL model\n","def load_pretrained(model, pretrained_path):\n","    pretrained_vgg = torch.load(pretrained_path)\n","    model_dict = model.state_dict()\n","    pretrained_dict = {k: v for k, v in pretrained_vgg.items() if k in model_dict and not k.startswith('classifier.6')}\n","    model_dict.update(pretrained_dict)\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","# Example of loading pretrained weights into DeepCORAL\n","pretrained_path = 'model_checkpoints/3channeled_vgg16_mnist.pth'  # Specify your pre-trained VGG-16 model path\n","deepcoral = DeepCORAL(num_classes=10)\n","deepcoral = load_pretrained(deepcoral.sharedNet, pretrained_path)\n","print(deepcoral)\n"]},{"cell_type":"code","source":["# Move model to device\n","deepcoral.to(params['device'])\n","\n","optimizer = optim.Adam(deepcoral.parameters(), lr=0.00002)"],"metadata":{"id":"8sr_vqdcgqGb","executionInfo":{"status":"ok","timestamp":1719593797994,"user_tz":-180,"elapsed":3,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["source_trainloader = mnist_trainloader\n","target_trainloader = svhn_trainloader\n","\n","# Training function\n","def train(model, optimizer, epoch, _lambda, params):\n","    model.train()\n","    result = []\n","    source, target = list(iter(source_trainloader)), list(iter(target_trainloader))\n","    train_steps = min(len(source), len(target))\n","\n","    for batch_idx in range(train_steps):\n","        source_data, source_label = source[batch_idx]\n","        target_data, _ = target[batch_idx]\n","\n","        source_data, source_label = Variable(source_data.to(params['device'])), Variable(source_label.to(params['device']))\n","        target_data = Variable(target_data.to(params['device']))\n","\n","        optimizer.zero_grad()\n","        out1, out2 = model(source_data, target_data)\n","        classification_loss = F.cross_entropy(out1, source_label)\n","        coral_loss = CORAL(out1, out2)  # Assuming CORAL is defined elsewhere\n","        total_loss = _lambda * coral_loss + classification_loss\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        result.append({\n","            'epoch': epoch,\n","            'step': batch_idx + 1,\n","            'total_steps': train_steps,\n","            'lambda': _lambda,\n","            'coral_loss': coral_loss.item(),\n","            'classification_loss': classification_loss.item(),\n","            'total_loss': total_loss.item()\n","        })\n","\n","        print('Train Epoch: {:2d} [{:2d}/{:2d}]\\t'\n","              'Lambda: {:.4f}, Class: {:.6f}, CORAL: {:.6f}, Total_Loss: {:.6f}'.format(\n","            epoch, batch_idx + 1, train_steps, _lambda,\n","            classification_loss.item(), coral_loss.item(), total_loss.item()))\n","\n","    return result\n","\n","# Testing function\n","def test(model, dataset_loader, epoch, params):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in dataset_loader:\n","            data, target = data.to(params['device']), target.to(params['device'])\n","            out = model(data)\n","            test_loss += F.cross_entropy(out, target, reduction='sum').item()\n","            pred = out.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(dataset_loader.dataset)\n","    accuracy = 100. * correct / len(dataset_loader.dataset)\n","\n","    print('\\nTest Epoch: {} Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        epoch, test_loss, correct, len(dataset_loader.dataset), accuracy))\n","\n","    return {\n","        'epoch': epoch,\n","        'average_loss': test_loss,\n","        'correct': correct,\n","        'total': len(dataset_loader.dataset),\n","        'accuracy': accuracy\n","    }\n","\n","# Main training and testing loop\n","args = {\n","    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    # 'transformed_dataloader': transformed_dataloader,\n","    'svhn_trainloader': svhn_trainloader,\n","    'mnist_trainloader': mnist_testloader,\n","}\n","\n","best_accuracy = 0.0\n","\n","for epoch in range(1, 5 + 1):\n","    train_results = train(deepcoral, optimizer, epoch, _lambda=0.5, params=args)\n","    test_results = test(deepcoral, svhn_testloader, epoch, params=args)\n","    print(f\"Test Epoch: {epoch}, Accuracy: {test_results['accuracy']:.2f}%\")\n","\n","    # Save the model if the accuracy improves\n","    if test_results['accuracy'] > best_accuracy:\n","        best_accuracy = test_results['accuracy']\n","        torch.save(deepcoral.state_dict(), 'model_checkpoints/best_model.pth')\n","        print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n"],"metadata":{"id":"e5fVhvHBg8nG","colab":{"base_uri":"https://localhost:8080/","height":375},"executionInfo":{"status":"error","timestamp":1719594066445,"user_tz":-180,"elapsed":1643,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"f331f05b-ee75-432e-cc4d-d99e1ff82279"},"execution_count":30,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x10 and 512x10)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-adcf1933a453>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcoral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcoral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvhn_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Epoch: {epoch}, Accuracy: {test_results['accuracy']:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-adcf1933a453>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epoch, _lambda, params)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mclassification_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcoral_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCORAL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Assuming CORAL is defined elsewhere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-5ffcc20c0d69>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x10 and 512x10)"]}]},{"cell_type":"code","source":["prms = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","\n","deepcoral = DeepCORAL()  # Replace with your actual model initialization\n","deepcoral.to(prms['device'])\n","\n","# Load the best model\n","deepcoral.load_state_dict(torch.load('model_checkpoints/best_model.pth'))\n","\n","test_results = test(deepcoral, svhn_testloader, epoch=1, params=prms)\n","\n","print(test_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":720},"id":"yh0PkYfn5RhL","executionInfo":{"status":"error","timestamp":1719594039188,"user_tz":-180,"elapsed":1809,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"10a11cc1-84be-45f9-efca-266d2a590750"},"execution_count":28,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Error(s) in loading state_dict for DeepCORAL:\n\tMissing key(s) in state_dict: \"sharedNet.features.0.weight\", \"sharedNet.features.0.bias\", \"sharedNet.features.2.weight\", \"sharedNet.features.2.bias\", \"sharedNet.features.5.weight\", \"sharedNet.features.5.bias\", \"sharedNet.features.7.weight\", \"sharedNet.features.7.bias\", \"sharedNet.features.10.weight\", \"sharedNet.features.10.bias\", \"sharedNet.features.12.weight\", \"sharedNet.features.12.bias\", \"sharedNet.features.14.weight\", \"sharedNet.features.14.bias\", \"sharedNet.features.17.weight\", \"sharedNet.features.17.bias\", \"sharedNet.features.19.weight\", \"sharedNet.features.19.bias\", \"sharedNet.features.21.weight\", \"sharedNet.features.21.bias\", \"sharedNet.features.24.weight\", \"sharedNet.features.24.bias\", \"sharedNet.features.26.weight\", \"sharedNet.features.26.bias\", \"sharedNet.features.28.weight\", \"sharedNet.features.28.bias\", \"sharedNet.classifier.0.weight\", \"sharedNet.classifier.0.bias\", \"sharedNet.classifier.3.weight\", \"sharedNet.classifier.3.bias\", \"sharedNet.classifier.6.weight\", \"sharedNet.classifier.6.bias\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.2.weight\", \"features.2.bias\", \"features.5.weight\", \"features.5.bias\", \"features.7.weight\", \"features.7.bias\", \"features.10.weight\", \"features.10.bias\", \"features.12.weight\", \"features.12.bias\", \"features.14.weight\", \"features.14.bias\", \"features.17.weight\", \"features.17.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-9444c3a13a7b>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdeepcoral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_checkpoints/best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcoral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvhn_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2189\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   2190\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   2191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DeepCORAL:\n\tMissing key(s) in state_dict: \"sharedNet.features.0.weight\", \"sharedNet.features.0.bias\", \"sharedNet.features.2.weight\", \"sharedNet.features.2.bias\", \"sharedNet.features.5.weight\", \"sharedNet.features.5.bias\", \"sharedNet.features.7.weight\", \"sharedNet.features.7.bias\", \"sharedNet.features.10.weight\", \"sharedNet.features.10.bias\", \"sharedNet.features.12.weight\", \"sharedNet.features.12.bias\", \"sharedNet.features.14.weight\", \"sharedNet.features.14.bias\", \"sharedNet.features.17.weight\", \"sharedNet.features.17.bias\", \"sharedNet.features.19.weight\", \"sharedNet.features.19.bias\", \"sharedNet.features.21.weight\", \"sharedNet.features.21.bias\", \"sharedNet.features.24.weight\", \"sharedNet.features.24.bias\", \"sharedNet.features.26.weight\", \"sharedNet.features.26.bias\", \"sharedNet.features.28.weight\", \"sharedNet.features.28.bias\", \"sharedNet.classifier.0.weight\", \"sharedNet.classifier.0.bias\", \"sharedNet.classifier.3.weight\", \"sharedNet.classifier.3.bias\", \"sharedNet.classifier.6.weight\", \"sharedNet.classifier.6.bias\", \"fc.weight\", \"fc.bias\". \n\tUnexpected key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.2.weight\", \"features.2.bias\", \"features.5.weight\", \"features.5.bias\", \"features.7.weight\", \"features.7.bias\", \"features.10.weight\", \"features.10.bias\", \"features.12.weight\", \"features.12.bias\", \"features.14.weight\", \"features.14.bias\", \"features.17.weight\", \"features.17.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". "]}]},{"cell_type":"code","source":["### UDA using SVHN pre-trained VGG16"],"metadata":{"id":"sOfyi61_Db_7","executionInfo":{"status":"aborted","timestamp":1719592643020,"user_tz":-180,"elapsed":4,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracy = 0.0\n","# Example of loading pretrained weights into DeepCORAL\n","pretrained_path = 'model_checkpoints/3channeled_vgg16_svhn.pth'  # Specify your pre-trained VGG-16 model path\n","deepcoral = DeepCORAL(num_classes=10)\n","deepcoral = load_pretrained(deepcoral.sharedNet, pretrained_path)\n","print(deepcoral)\n","\n","# Move model to device\n","deepcoral.to(params['device'])\n","\n","optimizer = optim.Adam(deepcoral.parameters(), lr=0.00002)\n","\n","for epoch in range(1, 50 + 1):\n","    train_results = train(deepcoral, optimizer, epoch, _lambda=0.5, params=args)\n","    test_results = test(deepcoral, mnist_testloader, epoch, params=args)\n","    print(f\"Test Epoch: {epoch}, Accuracy: {test_results['accuracy']:.2f}%\")\n","\n","    # Save the model if the accuracy improves\n","    if test_results['accuracy'] > best_accuracy:\n","        best_accuracy = test_results['accuracy']\n","        torch.save(deepcoral.state_dict(), 'model_checkpoints/best_model_svhn.pth')\n","        print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n","\n","test_results_source = test(deepcoral, svhn_testloader, epoch=1, params=args)\n","print(test_results_source)"],"metadata":{"id":"XCReUi70T37N","executionInfo":{"status":"aborted","timestamp":1719592643020,"user_tz":-180,"elapsed":4,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prms = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","\n","# deepcoral = DeepCORAL()  # Replace with your actual model initialization\n","# deepcoral.to(prms['device'])\n","\n","# Load the best model\n","# deepcoral.load_state_dict(torch.load('model_checkpoints/best_model_svhn.pth'))\n","\n","\n","\n","print(test_results)"],"metadata":{"id":"qf02wbbb72vC","executionInfo":{"status":"aborted","timestamp":1719592643020,"user_tz":-180,"elapsed":4,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cfKPzDOL-lvJ","executionInfo":{"status":"aborted","timestamp":1719592643021,"user_tz":-180,"elapsed":5,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}