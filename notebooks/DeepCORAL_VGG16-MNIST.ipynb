{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"fEVUVVTFfvB8","executionInfo":{"status":"ok","timestamp":1718908403386,"user_tz":-180,"elapsed":72678,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["%%capture\n","%pip install torchvision\n","%pip install torchsummary\n","%pip install torchviz\n","%pip install hiddenlayer"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3FhMG3MXfvB9","executionInfo":{"status":"ok","timestamp":1718908429768,"user_tz":-180,"elapsed":26384,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"cadefed2-eca3-490a-b73f-13df02753d71"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Deep_Learning\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/Othercomputers/My Computer/Masters_Staff/trimester_3/Deep_Learning"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RbnqL7rKfvB-","executionInfo":{"status":"ok","timestamp":1718908435534,"user_tz":-180,"elapsed":5768,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["import torch\n","import torchvision\n","import torchvision.models as models\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","\n","from torch.utils.data import DataLoader, Subset\n","from torchsummary import summary\n","from torch.autograd import Variable\n","from torchvision import datasets, transforms"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOdZDt2sfvB-","executionInfo":{"status":"ok","timestamp":1718908435534,"user_tz":-180,"elapsed":11,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"51e343fa-e7f9-4baa-9736-35e7b32efdf3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Jun 20 18:33:54 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"]},{"cell_type":"markdown","source":["### Dataset Loading"],"metadata":{"id":"c7SizK3bkrRb"}},{"cell_type":"code","source":["#model params\n","params = {\n","    'batch_size':32,\n","    'mul': 30,\n","    'num_classes':10,\n","    'channels':3,\n","    'img_dim':32,\n","    'device':torch.device('cuda') if torch.cuda.is_available() else 'cpu',\n","    'fliplr':True,\n","    'num_epochs':50,\n","    'decay_epoch':50,\n","    'lr':0.00002,    #learning rate for generator\n","    'beta1':0.5 ,    #beta1 for Adam optimizer\n","    'beta2':0.999 ,  #beta2 for Adam optimizer\n","    'lambdaA':10 ,   #lambdaA for cycle loss\n","    'lambdaB':10  ,  #lambdaB for cycle loss\n","}\n"],"metadata":{"id":"Gz98n0M8G_WU","executionInfo":{"status":"ok","timestamp":1718908435535,"user_tz":-180,"elapsed":3,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to load and preprocess datasets\n","def load_dataset(dataset_cls, split, root, download, normalize=False, subset_size=None, batch_size=params['batch_size']):\n","    if dataset_cls == datasets.MNIST:\n","        dataset = dataset_cls(root=root, train=(split=='train'), download=download, transform=transforms.ToTensor())\n","    else:\n","        dataset = dataset_cls(root=root, split=split, download=download, transform=transforms.ToTensor())\n","\n","    if normalize:\n","        # mean, std = compute_mean_and_std(dataset)\n","\n","        if dataset_cls == datasets.MNIST:\n","\n","            print(f\"Dataset is MNIST: {dataset_cls}\")\n","            transform = transforms.Compose([\n","                transforms.Resize(( params['img_dim'],params['img_dim']) ),\n","                transforms.Grayscale(params['channels']),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5]*params['channels'], std=[0.5]*params['channels'])\n","            ])\\\n","\n","        else:\n","            print(f\"Dataset is SVHN: {dataset_cls}\")\n","            transform = transforms.Compose([\n","                transforms.Resize(( params['img_dim'],params['img_dim'] )),\n","                transforms.Grayscale(params['channels']),\n","                transforms.ToTensor(),\n","                transforms.Normalize(mean=[0.5]*params['channels'], std=[0.5]*params['channels'])\n","            ])\n","\n","        # Apply the computed normalization\n","        dataset.transform = transform\n","\n","    if subset_size:\n","        indices = np.random.choice(len(dataset), subset_size, replace=False)\n","        print(dataset.transform)\n","        dataset = Subset(dataset, indices)\n","\n","    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True if split == 'train' else False, num_workers=1)\n","    return loader\n","\n","# Load MNIST datasets\n","mnist_trainloader = load_dataset(datasets.MNIST, 'train', './data/', True, normalize=True,subset_size=params['batch_size']*params['mul'] )\n","mnist_testloader = load_dataset(datasets.MNIST, 'test', './data/', True, normalize=True)\n","\n","# Load SVHN datasets\n","svhn_trainloader = load_dataset(datasets.SVHN, 'train', './data/', True, normalize=True,subset_size=params['batch_size']*params['mul'])\n","svhn_testloader = load_dataset(datasets.SVHN, 'test', './data/', True, normalize=True)\n","\n","# Optionally, load the extra SVHN dataset if needed\n","# svhn_extraloader = load_dataset(datasets.SVHN, 'extra', './data/', True, normalize=True, subset_size=20000)\n","\n","# Verify the sizes of the datasets\n","print(f'MNIST train subset size: {len(mnist_trainloader.dataset)}')\n","# print(f'SVHN train subset size: {len(svhn_trainloader.dataset)}')\n","# print(f'SVHN extra size: {len(svhn_extraloader.dataset)}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCm5YC5CkxpO","executionInfo":{"status":"ok","timestamp":1718908469145,"user_tz":-180,"elapsed":33612,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"1867a53f-5af2-4acc-edf0-a17961192ffc"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset is MNIST: <class 'torchvision.datasets.mnist.MNIST'>\n","Compose(\n","    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n","    Grayscale(num_output_channels=3)\n","    ToTensor()\n","    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",")\n","Dataset is MNIST: <class 'torchvision.datasets.mnist.MNIST'>\n","Using downloaded and verified file: ./data/train_32x32.mat\n","Dataset is SVHN: <class 'torchvision.datasets.svhn.SVHN'>\n","Compose(\n","    Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n","    Grayscale(num_output_channels=3)\n","    ToTensor()\n","    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",")\n","Using downloaded and verified file: ./data/test_32x32.mat\n","Dataset is SVHN: <class 'torchvision.datasets.svhn.SVHN'>\n","MNIST train subset size: 960\n"]}]},{"cell_type":"code","source":["import torch\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","\n","# Define the transformations (same as during training)\n","transform = transforms.Compose([\n","    transforms.Grayscale(3),  # Ensure images are single-channel grayscale\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*params['channels'], [0.5]*params['channels'])  # Assuming normalization parameters used during training\n","])\n","\n","# Define the path to the transformed images\n","transformed_images_path = 'transformed_mnist_train'\n","\n","# Create the dataset\n","transformed_dataset = datasets.ImageFolder(root=transformed_images_path, transform=transform)\n","\n","# Create the DataLoader\n","transformed_dataloader = DataLoader(transformed_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=8)"],"metadata":{"id":"c2plUzc8y30g","executionInfo":{"status":"ok","timestamp":1718897688775,"user_tz":-180,"elapsed":9449,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Baseline Model\n","\n","Our baseline model here is a simple CNN."],"metadata":{"id":"h793IQ9Ak0pp"}},{"cell_type":"code","source":["class SimpleCNN(nn.Module):\n","    def __init__(self):\n","        super(SimpleCNN, self).__init__()\n","        self.feature_extractor = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(64),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Use a dummy input to determine the size of the flattened features\n","        dummy_input = torch.randn(1, 3, 32, 32)  # Change the input size if necessary\n","        dummy_output = self.feature_extractor(dummy_input)\n","        flattened_size = dummy_output.view(-1).size(0)\n","\n","        # Now use the calculated flattened size for the classifier\n","        self.classifier = nn.Sequential(\n","            nn.Linear(flattened_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10)\n","        )\n","\n","    def forward(self, x):\n","        features = self.feature_extractor(x)\n","        features = features.view(features.size(0), -1)\n","        output = self.classifier(features)\n","        return output\n","\n","class DomainDiscriminator(nn.Module):\n","    def __init__(self, feature_size):\n","        super(DomainDiscriminator, self).__init__()\n","        self.domain_classifier = nn.Sequential(\n","            nn.Linear(feature_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 2)  # Two classes: source and target\n","        )\n","\n","    def forward(self, x):\n","        output = self.domain_classifier(x)\n","        return output\n","\n","# Initialize the model and print the flattened feature size\n","model = SimpleCNN()\n","print(model)\n"],"metadata":{"id":"eMK4S4tDlArM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718899841499,"user_tz":-180,"elapsed":682,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"5dcf14d1-f2f8-4ade-e28b-30c51122ff67"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["SimpleCNN(\n","  (feature_extractor): Sequential(\n","    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (2): ReLU()\n","    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (6): ReLU()\n","    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (10): ReLU()\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=2048, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"code","source":["feature_extractor = SimpleCNN().to(params['device'])\n","flattened_size = feature_extractor.classifier[0].in_features  # Get the input feature size of the first Linear layer\n","domain_discriminator = DomainDiscriminator(flattened_size).to(params['device'])\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(list(feature_extractor.parameters()) + list(domain_discriminator.parameters()), lr=0.00001)\n"],"metadata":{"id":"ivbVo77CnOUv","executionInfo":{"status":"ok","timestamp":1718899842056,"user_tz":-180,"elapsed":1,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":["#### Baseline model Training"],"metadata":{"id":"QCSd9yHYnUoa"}},{"cell_type":"code","source":["def train(model, domain_model, source_loader, target_loader, criterion, optimizer, num_epochs=20):\n","    model.train()\n","    domain_model.train()\n","\n","    for epoch in range(num_epochs):\n","        for (source_data, source_labels), (target_data, _) in zip(source_loader, target_loader):\n","            source_data, source_labels = source_data.to(params['device']), source_labels.to(params['device'])\n","            target_data = target_data.to(params['device'])\n","\n","            # Train feature extractor and classifier on source data\n","            optimizer.zero_grad()\n","            source_features = model.feature_extractor(source_data)\n","            source_output = model.classifier(source_features.view(source_features.size(0), -1))\n","            source_loss = criterion(source_output, source_labels)\n","            source_loss.backward()\n","            optimizer.step()\n","\n","            # Train domain discriminator on source and target data\n","            optimizer.zero_grad()\n","            domain_labels = torch.cat((torch.zeros(source_data.size(0)), torch.ones(target_data.size(0)))).long().to(params['device'])\n","            combined_data = torch.cat((source_data, target_data))\n","            combined_features = model.feature_extractor(combined_data)\n","            domain_output = domain_model(combined_features.view(combined_features.size(0), -1))\n","            domain_loss = criterion(domain_output, domain_labels)\n","            domain_loss.backward()\n","            optimizer.step()\n","\n","        print(f'Epoch [{epoch+1}/{num_epochs}], Source Loss: {source_loss.item():.4f}, Domain Loss: {domain_loss.item():.4f}')\n","\n","# Train the model\n","train(feature_extractor, domain_discriminator, mnist_trainloader, svhn_trainloader, criterion, optimizer)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuSLvHL9nT2J","executionInfo":{"status":"ok","timestamp":1718899856123,"user_tz":-180,"elapsed":14068,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"0c65a16b-f6d6-47c3-95e2-2e6e757733d2"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/20], Source Loss: 2.1359, Domain Loss: 0.1791\n","Epoch [2/20], Source Loss: 1.8585, Domain Loss: 0.0606\n","Epoch [3/20], Source Loss: 1.7965, Domain Loss: 0.0355\n","Epoch [4/20], Source Loss: 1.4949, Domain Loss: 0.0169\n","Epoch [5/20], Source Loss: 1.4236, Domain Loss: 0.0116\n","Epoch [6/20], Source Loss: 1.1547, Domain Loss: 0.0096\n","Epoch [7/20], Source Loss: 1.0290, Domain Loss: 0.0074\n","Epoch [8/20], Source Loss: 0.8511, Domain Loss: 0.0054\n","Epoch [9/20], Source Loss: 0.5698, Domain Loss: 0.0045\n","Epoch [10/20], Source Loss: 0.8540, Domain Loss: 0.0045\n","Epoch [11/20], Source Loss: 0.7951, Domain Loss: 0.0036\n","Epoch [12/20], Source Loss: 0.4575, Domain Loss: 0.0045\n","Epoch [13/20], Source Loss: 0.5633, Domain Loss: 0.0039\n","Epoch [14/20], Source Loss: 0.5967, Domain Loss: 0.0032\n","Epoch [15/20], Source Loss: 0.4068, Domain Loss: 0.0018\n","Epoch [16/20], Source Loss: 0.4641, Domain Loss: 0.0030\n","Epoch [17/20], Source Loss: 0.4258, Domain Loss: 0.0014\n","Epoch [18/20], Source Loss: 0.3900, Domain Loss: 0.0017\n","Epoch [19/20], Source Loss: 0.3718, Domain Loss: 0.0024\n","Epoch [20/20], Source Loss: 0.2043, Domain Loss: 0.0013\n"]}]},{"cell_type":"code","source":["def evaluate(model, loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for data, labels in loader:\n","            data, labels = data.to(params['device']), labels.to(params['device'])\n","            outputs = model(data)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","    accuracy = 100 * correct / total\n","    return accuracy\n","\n","source_accuracy = evaluate(feature_extractor, mnist_testloader, params['device'])\n","target_accuracy = evaluate(feature_extractor, svhn_testloader, params['device'])\n","print(f'Source (MNIST) Accuracy: {source_accuracy:.2f}%')\n","print(f'Target (SVHN) Accuracy: {target_accuracy:.2f}%')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPIR0pNbCmKH","executionInfo":{"status":"ok","timestamp":1718899872056,"user_tz":-180,"elapsed":15942,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"354f99e6-ed95-46a4-b4c4-2c42efbec565"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Source (MNIST) Accuracy: 94.26%\n","Target (SVHN) Accuracy: 19.99%\n"]}]},{"cell_type":"markdown","source":["### VGG16 architexture"],"metadata":{"id":"t9LielkWngZx"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"EZjZdudjfvB_","executionInfo":{"status":"ok","timestamp":1718908475240,"user_tz":-180,"elapsed":488,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["# Define the VGG-16 architecture without pretrained weights\n","class VGG16Custom(nn.Module):\n","    def __init__(self, num_classes=params['num_classes']):\n","        super(VGG16Custom, self).__init__()\n","        # Load the VGG-16 model\n","        self.features = models.vgg16(weights=None).features\n","        # Modify the first convolutional layer to accept 1-channel input\n","        self.features[0] = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n","        # Add a custom classifier (fully connected layers)\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 1 * 1, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), 512 * 1 * 1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"YcrgcgmjfvB_","executionInfo":{"status":"ok","timestamp":1718908475240,"user_tz":-180,"elapsed":1,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"outputs":[],"source":["def CORAL(source, target):\n","    d = source.data.shape[1]\n","\n","    # source covariance\n","    xm = torch.mean(source, 0, keepdim=True) - source\n","    xc = xm.t() @ xm\n","\n","    # target covariance\n","    xmt = torch.mean(target, 0, keepdim=True) - target\n","    xct = xmt.t() @ xmt\n","\n","    # frobenius norm between source and target\n","    loss = torch.mean(torch.mul((xc - xct), (xc - xct)))\n","    loss = loss/(4*d*d)\n","\n","    return loss\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"7Qi6OJkofvCA","executionInfo":{"status":"ok","timestamp":1718908485434,"user_tz":-180,"elapsed":10195,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a7516f02-5e45-4b02-a167-ed2159f48e8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["VGG16Custom(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=512, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")\n"]}],"source":["class DeepCORAL(nn.Module):\n","    def __init__(self, num_classes=params['num_classes']):\n","        super(DeepCORAL, self).__init__()\n","        self.sharedNet = VGG16Custom()\n","        self.fc = nn.Linear(4096, num_classes)\n","\n","        # initialize according to CORAL paper experiment\n","        self.fc.weight.data.normal_(0, 0.005)\n","\n","    def forward(self, source, target):\n","        source = self.sharedNet(source)\n","        source = self.fc(source)\n","\n","        target = self.sharedNet(target)\n","        target = self.fc(target)\n","        return source, target\n","\n","# Function to load pretrained weights into the DeepCORAL model\n","def load_pretrained(model, pretrained_path):\n","    pretrained_vgg = torch.load(pretrained_path)\n","    model_dict = model.state_dict()\n","    pretrained_dict = {k: v for k, v in pretrained_vgg.items() if k in model_dict and not k.startswith('classifier.6')}\n","    model_dict.update(pretrained_dict)\n","    model.load_state_dict(model_dict)\n","    return model\n","\n","# Example of loading pretrained weights into DeepCORAL\n","pretrained_path = 'model_checkpoints/3channeled_vgg16_mnist.pth'  # Specify your pre-trained VGG-16 model path\n","deepcoral = DeepCORAL(num_classes=10)\n","deepcoral = load_pretrained(deepcoral.sharedNet, pretrained_path)\n","print(deepcoral)\n"]},{"cell_type":"code","source":["# Move model to device\n","deepcoral.to(params['device'])\n","\n","optimizer = optim.Adam(deepcoral.parameters(), lr=0.00002)"],"metadata":{"id":"8sr_vqdcgqGb","executionInfo":{"status":"ok","timestamp":1718899273151,"user_tz":-180,"elapsed":2,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# Training function\n","def train(model, optimizer, epoch, _lambda, params):\n","    model.train()\n","    result = []\n","    source, target = list(iter(params['svhn_trainloader'])), list(iter(params['mnist_trainloader']))\n","    train_steps = min(len(source), len(target))\n","\n","    for batch_idx in range(train_steps):\n","        source_data, source_label = source[batch_idx]\n","        target_data, _ = target[batch_idx]\n","\n","        source_data, source_label = Variable(source_data.to(params['device'])), Variable(source_label.to(params['device']))\n","        target_data = Variable(target_data.to(params['device']))\n","\n","        optimizer.zero_grad()\n","        out1, out2 = model(source_data), model(target_data)\n","        classification_loss = F.cross_entropy(out1, source_label)\n","        coral_loss = CORAL(out1, out2)  # Assuming CORAL is defined elsewhere\n","        total_loss = _lambda * coral_loss + classification_loss\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        result.append({\n","            'epoch': epoch,\n","            'step': batch_idx + 1,\n","            'total_steps': train_steps,\n","            'lambda': _lambda,\n","            'coral_loss': coral_loss.item(),\n","            'classification_loss': classification_loss.item(),\n","            'total_loss': total_loss.item()\n","        })\n","\n","        print('Train Epoch: {:2d} [{:2d}/{:2d}]\\t'\n","              'Lambda: {:.4f}, Class: {:.6f}, CORAL: {:.6f}, Total_Loss: {:.6f}'.format(\n","            epoch, batch_idx + 1, train_steps, _lambda,\n","            classification_loss.item(), coral_loss.item(), total_loss.item()))\n","\n","    return result\n","\n","# Testing function\n","def test(model, dataset_loader, epoch, params):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in dataset_loader:\n","            data, target = data.to(params['device']), target.to(params['device'])\n","            out = model(data)\n","            test_loss += F.cross_entropy(out, target, reduction='sum').item()\n","            pred = out.argmax(dim=1, keepdim=True)\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(dataset_loader.dataset)\n","    accuracy = 100. * correct / len(dataset_loader.dataset)\n","\n","    print('\\nTest Epoch: {} Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        epoch, test_loss, correct, len(dataset_loader.dataset), accuracy))\n","\n","    return {\n","        'epoch': epoch,\n","        'average_loss': test_loss,\n","        'correct': correct,\n","        'total': len(dataset_loader.dataset),\n","        'accuracy': accuracy\n","    }\n","\n","# Main training and testing loop\n","args = {\n","    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n","    # 'transformed_dataloader': transformed_dataloader,\n","    'svhn_trainloader': svhn_trainloader,\n","    'mnist_trainloader': mnist_testloader,\n","}\n","\n","# best_accuracy = 0.0\n","\n","# for epoch in range(1, 5 + 1):\n","#     train_results = train(deepcoral, optimizer, epoch, _lambda=0.5, params=args)\n","#     test_results = test(deepcoral, svhn_testloader, epoch, params=args)\n","#     print(f\"Test Epoch: {epoch}, Accuracy: {test_results['accuracy']:.2f}%\")\n","\n","#     # Save the model if the accuracy improves\n","#     if test_results['accuracy'] > best_accuracy:\n","#         best_accuracy = test_results['accuracy']\n","#         torch.save(deepcoral.state_dict(), 'model_checkpoints/best_model.pth')\n","#         print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n"],"metadata":{"id":"e5fVhvHBg8nG","executionInfo":{"status":"ok","timestamp":1718910555109,"user_tz":-180,"elapsed":495,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["prms = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","\n","deepcoral = DeepCORAL()  # Replace with your actual model initialization\n","deepcoral.to(prms['device'])\n","\n","# Load the best model\n","deepcoral.load_state_dict(torch.load('best_model.pth'))\n","\n","test_results = test(deepcoral, svhn_testloader, epoch=1, params=prms)\n","\n","print(test_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"yh0PkYfn5RhL","executionInfo":{"status":"error","timestamp":1718910558402,"user_tz":-180,"elapsed":2473,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"e76aa2f1-d3fe-4dcb-b84c-4e113fc6ec30"},"execution_count":30,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'best_model.pth'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-f943ef7120ef>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdeepcoral\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcoral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvhn_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 997\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    998\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model.pth'"]}]},{"cell_type":"code","source":["### UDA using SVHN pre-trained VGG16"],"metadata":{"id":"sOfyi61_Db_7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["best_accuracy = 0.0\n","# Example of loading pretrained weights into DeepCORAL\n","pretrained_path = 'model_checkpoints/3channeled_vgg16_svhn.pth'  # Specify your pre-trained VGG-16 model path\n","deepcoral = DeepCORAL(num_classes=10)\n","deepcoral = load_pretrained(deepcoral.sharedNet, pretrained_path)\n","print(deepcoral)\n","\n","# Move model to device\n","deepcoral.to(params['device'])\n","\n","optimizer = optim.Adam(deepcoral.parameters(), lr=0.00002)\n","\n","for epoch in range(1, 50 + 1):\n","    train_results = train(deepcoral, optimizer, epoch, _lambda=0.5, params=args)\n","    test_results = test(deepcoral, mnist_testloader, epoch, params=args)\n","    print(f\"Test Epoch: {epoch}, Accuracy: {test_results['accuracy']:.2f}%\")\n","\n","    # Save the model if the accuracy improves\n","    if test_results['accuracy'] > best_accuracy:\n","        best_accuracy = test_results['accuracy']\n","        torch.save(deepcoral.state_dict(), 'model_checkpoints/best_model_svhn.pth')\n","        print(f\"New best model saved with accuracy: {best_accuracy:.2f}%\")\n","\n","test_results_source = test(deepcoral, svhn_testloader, epoch=1, params=args)\n","print(test_results_source)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XCReUi70T37N","executionInfo":{"status":"ok","timestamp":1718911099617,"user_tz":-180,"elapsed":36073,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"9d761dfc-7d80-43b7-82ef-69da77e30a8b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["VGG16Custom(\n","  (features): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU(inplace=True)\n","    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (3): ReLU(inplace=True)\n","    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (6): ReLU(inplace=True)\n","    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (8): ReLU(inplace=True)\n","    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (11): ReLU(inplace=True)\n","    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (13): ReLU(inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (15): ReLU(inplace=True)\n","    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (18): ReLU(inplace=True)\n","    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (20): ReLU(inplace=True)\n","    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (22): ReLU(inplace=True)\n","    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (25): ReLU(inplace=True)\n","    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (27): ReLU(inplace=True)\n","    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (29): ReLU(inplace=True)\n","    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=512, out_features=4096, bias=True)\n","    (1): ReLU(inplace=True)\n","    (2): Dropout(p=0.5, inplace=False)\n","    (3): Linear(in_features=4096, out_features=4096, bias=True)\n","    (4): ReLU(inplace=True)\n","    (5): Dropout(p=0.5, inplace=False)\n","    (6): Linear(in_features=4096, out_features=10, bias=True)\n","  )\n",")\n","Train Epoch:  1 [ 1/30]\tLambda: 0.5000, Class: 2.319497, CORAL: 0.008566, Total_Loss: 2.323780\n","Train Epoch:  1 [ 2/30]\tLambda: 0.5000, Class: 2.193875, CORAL: 0.026117, Total_Loss: 2.206934\n","Train Epoch:  1 [ 3/30]\tLambda: 0.5000, Class: 2.257497, CORAL: 0.005223, Total_Loss: 2.260109\n","Train Epoch:  1 [ 4/30]\tLambda: 0.5000, Class: 2.276431, CORAL: 0.008834, Total_Loss: 2.280848\n","Train Epoch:  1 [ 5/30]\tLambda: 0.5000, Class: 2.203041, CORAL: 0.003028, Total_Loss: 2.204555\n","Train Epoch:  1 [ 6/30]\tLambda: 0.5000, Class: 2.148355, CORAL: 0.003613, Total_Loss: 2.150162\n","Train Epoch:  1 [ 7/30]\tLambda: 0.5000, Class: 2.152968, CORAL: 0.003926, Total_Loss: 2.154931\n","Train Epoch:  1 [ 8/30]\tLambda: 0.5000, Class: 2.070028, CORAL: 0.003351, Total_Loss: 2.071704\n","Train Epoch:  1 [ 9/30]\tLambda: 0.5000, Class: 2.150834, CORAL: 0.004700, Total_Loss: 2.153184\n","Train Epoch:  1 [10/30]\tLambda: 0.5000, Class: 2.140597, CORAL: 0.004877, Total_Loss: 2.143035\n","Train Epoch:  1 [11/30]\tLambda: 0.5000, Class: 2.094439, CORAL: 0.010280, Total_Loss: 2.099579\n","Train Epoch:  1 [12/30]\tLambda: 0.5000, Class: 1.952029, CORAL: 0.018387, Total_Loss: 1.961222\n","Train Epoch:  1 [13/30]\tLambda: 0.5000, Class: 2.003442, CORAL: 0.004427, Total_Loss: 2.005656\n","Train Epoch:  1 [14/30]\tLambda: 0.5000, Class: 2.046485, CORAL: 0.010474, Total_Loss: 2.051723\n","Train Epoch:  1 [15/30]\tLambda: 0.5000, Class: 1.913472, CORAL: 0.005834, Total_Loss: 1.916389\n","Train Epoch:  1 [16/30]\tLambda: 0.5000, Class: 1.974828, CORAL: 0.006294, Total_Loss: 1.977975\n","Train Epoch:  1 [17/30]\tLambda: 0.5000, Class: 1.754562, CORAL: 0.017384, Total_Loss: 1.763255\n","Train Epoch:  1 [18/30]\tLambda: 0.5000, Class: 1.830478, CORAL: 0.008068, Total_Loss: 1.834513\n","Train Epoch:  1 [19/30]\tLambda: 0.5000, Class: 1.728898, CORAL: 0.026725, Total_Loss: 1.742261\n","Train Epoch:  1 [20/30]\tLambda: 0.5000, Class: 1.802119, CORAL: 0.018850, Total_Loss: 1.811544\n","Train Epoch:  1 [21/30]\tLambda: 0.5000, Class: 1.642524, CORAL: 0.031289, Total_Loss: 1.658169\n","Train Epoch:  1 [22/30]\tLambda: 0.5000, Class: 1.694050, CORAL: 0.013994, Total_Loss: 1.701047\n","Train Epoch:  1 [23/30]\tLambda: 0.5000, Class: 1.599705, CORAL: 0.037756, Total_Loss: 1.618583\n","Train Epoch:  1 [24/30]\tLambda: 0.5000, Class: 1.646846, CORAL: 0.023871, Total_Loss: 1.658782\n","Train Epoch:  1 [25/30]\tLambda: 0.5000, Class: 1.339409, CORAL: 0.061223, Total_Loss: 1.370020\n","Train Epoch:  1 [26/30]\tLambda: 0.5000, Class: 1.478281, CORAL: 0.081962, Total_Loss: 1.519262\n","Train Epoch:  1 [27/30]\tLambda: 0.5000, Class: 1.072854, CORAL: 0.102294, Total_Loss: 1.124002\n","Train Epoch:  1 [28/30]\tLambda: 0.5000, Class: 1.398662, CORAL: 0.338372, Total_Loss: 1.567848\n","Train Epoch:  1 [29/30]\tLambda: 0.5000, Class: 1.288655, CORAL: 0.144648, Total_Loss: 1.360979\n","Train Epoch:  1 [30/30]\tLambda: 0.5000, Class: 1.399790, CORAL: 0.079757, Total_Loss: 1.439669\n","\n","Test Epoch: 1 Average loss: 1.5867, Accuracy: 6142/10000 (61.42%)\n","\n","Test Epoch: 1, Accuracy: 61.42%\n","New best model saved with accuracy: 61.42%\n","Train Epoch:  2 [ 1/30]\tLambda: 0.5000, Class: 1.124073, CORAL: 0.191388, Total_Loss: 1.219767\n","Train Epoch:  2 [ 2/30]\tLambda: 0.5000, Class: 0.977772, CORAL: 0.271537, Total_Loss: 1.113541\n","Train Epoch:  2 [ 3/30]\tLambda: 0.5000, Class: 1.053772, CORAL: 0.187291, Total_Loss: 1.147418\n","Train Epoch:  2 [ 4/30]\tLambda: 0.5000, Class: 1.064061, CORAL: 0.378635, Total_Loss: 1.253379\n","Train Epoch:  2 [ 5/30]\tLambda: 0.5000, Class: 0.822711, CORAL: 0.283274, Total_Loss: 0.964348\n","Train Epoch:  2 [ 6/30]\tLambda: 0.5000, Class: 1.021087, CORAL: 0.328088, Total_Loss: 1.185132\n","Train Epoch:  2 [ 7/30]\tLambda: 0.5000, Class: 0.763177, CORAL: 1.231354, Total_Loss: 1.378854\n","Train Epoch:  2 [ 8/30]\tLambda: 0.5000, Class: 0.845785, CORAL: 0.422674, Total_Loss: 1.057122\n","Train Epoch:  2 [ 9/30]\tLambda: 0.5000, Class: 0.701810, CORAL: 0.250150, Total_Loss: 0.826885\n","Train Epoch:  2 [10/30]\tLambda: 0.5000, Class: 0.800731, CORAL: 0.363300, Total_Loss: 0.982381\n","Train Epoch:  2 [11/30]\tLambda: 0.5000, Class: 0.702660, CORAL: 0.652212, Total_Loss: 1.028766\n","Train Epoch:  2 [12/30]\tLambda: 0.5000, Class: 0.726917, CORAL: 0.661077, Total_Loss: 1.057456\n","Train Epoch:  2 [13/30]\tLambda: 0.5000, Class: 0.651482, CORAL: 0.848684, Total_Loss: 1.075824\n","Train Epoch:  2 [14/30]\tLambda: 0.5000, Class: 0.711612, CORAL: 0.291616, Total_Loss: 0.857421\n","Train Epoch:  2 [15/30]\tLambda: 0.5000, Class: 0.735382, CORAL: 0.384965, Total_Loss: 0.927864\n","Train Epoch:  2 [16/30]\tLambda: 0.5000, Class: 0.735454, CORAL: 0.605714, Total_Loss: 1.038311\n","Train Epoch:  2 [17/30]\tLambda: 0.5000, Class: 0.697376, CORAL: 0.732123, Total_Loss: 1.063438\n","Train Epoch:  2 [18/30]\tLambda: 0.5000, Class: 0.819694, CORAL: 0.414262, Total_Loss: 1.026825\n","Train Epoch:  2 [19/30]\tLambda: 0.5000, Class: 0.814772, CORAL: 0.244108, Total_Loss: 0.936826\n","Train Epoch:  2 [20/30]\tLambda: 0.5000, Class: 0.885760, CORAL: 0.334179, Total_Loss: 1.052849\n","Train Epoch:  2 [21/30]\tLambda: 0.5000, Class: 0.797103, CORAL: 0.232279, Total_Loss: 0.913242\n","Train Epoch:  2 [22/30]\tLambda: 0.5000, Class: 0.807429, CORAL: 0.208632, Total_Loss: 0.911745\n","Train Epoch:  2 [23/30]\tLambda: 0.5000, Class: 0.981520, CORAL: 0.307156, Total_Loss: 1.135098\n","Train Epoch:  2 [24/30]\tLambda: 0.5000, Class: 0.711632, CORAL: 0.265350, Total_Loss: 0.844307\n","Train Epoch:  2 [25/30]\tLambda: 0.5000, Class: 0.776911, CORAL: 0.246096, Total_Loss: 0.899959\n","Train Epoch:  2 [26/30]\tLambda: 0.5000, Class: 0.700853, CORAL: 0.222770, Total_Loss: 0.812238\n","Train Epoch:  2 [27/30]\tLambda: 0.5000, Class: 0.974669, CORAL: 0.191348, Total_Loss: 1.070343\n","Train Epoch:  2 [28/30]\tLambda: 0.5000, Class: 0.759961, CORAL: 0.295072, Total_Loss: 0.907497\n","Train Epoch:  2 [29/30]\tLambda: 0.5000, Class: 0.725455, CORAL: 0.187766, Total_Loss: 0.819338\n","Train Epoch:  2 [30/30]\tLambda: 0.5000, Class: 0.934714, CORAL: 0.096190, Total_Loss: 0.982809\n","\n","Test Epoch: 2 Average loss: 1.4185, Accuracy: 6185/10000 (61.85%)\n","\n","Test Epoch: 2, Accuracy: 61.85%\n","New best model saved with accuracy: 61.85%\n","Train Epoch:  3 [ 1/30]\tLambda: 0.5000, Class: 0.704731, CORAL: 0.294970, Total_Loss: 0.852216\n","Train Epoch:  3 [ 2/30]\tLambda: 0.5000, Class: 0.679958, CORAL: 0.331165, Total_Loss: 0.845540\n","Train Epoch:  3 [ 3/30]\tLambda: 0.5000, Class: 0.609579, CORAL: 0.367333, Total_Loss: 0.793245\n","Train Epoch:  3 [ 4/30]\tLambda: 0.5000, Class: 0.648291, CORAL: 0.486918, Total_Loss: 0.891749\n","Train Epoch:  3 [ 5/30]\tLambda: 0.5000, Class: 0.647758, CORAL: 0.303795, Total_Loss: 0.799656\n","Train Epoch:  3 [ 6/30]\tLambda: 0.5000, Class: 0.868872, CORAL: 0.557232, Total_Loss: 1.147488\n","Train Epoch:  3 [ 7/30]\tLambda: 0.5000, Class: 0.599163, CORAL: 0.273385, Total_Loss: 0.735855\n","Train Epoch:  3 [ 8/30]\tLambda: 0.5000, Class: 0.701719, CORAL: 0.162439, Total_Loss: 0.782938\n","Train Epoch:  3 [ 9/30]\tLambda: 0.5000, Class: 0.652274, CORAL: 0.264953, Total_Loss: 0.784750\n","Train Epoch:  3 [10/30]\tLambda: 0.5000, Class: 0.457942, CORAL: 0.538297, Total_Loss: 0.727091\n","Train Epoch:  3 [11/30]\tLambda: 0.5000, Class: 0.628525, CORAL: 0.232250, Total_Loss: 0.744651\n","Train Epoch:  3 [12/30]\tLambda: 0.5000, Class: 0.601793, CORAL: 0.303074, Total_Loss: 0.753330\n","Train Epoch:  3 [13/30]\tLambda: 0.5000, Class: 0.637079, CORAL: 0.464592, Total_Loss: 0.869375\n","Train Epoch:  3 [14/30]\tLambda: 0.5000, Class: 0.373174, CORAL: 0.774508, Total_Loss: 0.760428\n","Train Epoch:  3 [15/30]\tLambda: 0.5000, Class: 0.616475, CORAL: 0.289026, Total_Loss: 0.760988\n","Train Epoch:  3 [16/30]\tLambda: 0.5000, Class: 0.466972, CORAL: 0.344108, Total_Loss: 0.639025\n","Train Epoch:  3 [17/30]\tLambda: 0.5000, Class: 0.505048, CORAL: 0.754534, Total_Loss: 0.882314\n","Train Epoch:  3 [18/30]\tLambda: 0.5000, Class: 0.549451, CORAL: 0.462339, Total_Loss: 0.780621\n","Train Epoch:  3 [19/30]\tLambda: 0.5000, Class: 0.660640, CORAL: 0.281415, Total_Loss: 0.801348\n","Train Epoch:  3 [20/30]\tLambda: 0.5000, Class: 0.618336, CORAL: 0.326420, Total_Loss: 0.781546\n","Train Epoch:  3 [21/30]\tLambda: 0.5000, Class: 0.656310, CORAL: 0.289607, Total_Loss: 0.801113\n","Train Epoch:  3 [22/30]\tLambda: 0.5000, Class: 0.652016, CORAL: 0.243433, Total_Loss: 0.773733\n","Train Epoch:  3 [23/30]\tLambda: 0.5000, Class: 0.497324, CORAL: 0.666221, Total_Loss: 0.830435\n","Train Epoch:  3 [24/30]\tLambda: 0.5000, Class: 0.469917, CORAL: 0.387642, Total_Loss: 0.663738\n","Train Epoch:  3 [25/30]\tLambda: 0.5000, Class: 0.596426, CORAL: 0.310773, Total_Loss: 0.751813\n","Train Epoch:  3 [26/30]\tLambda: 0.5000, Class: 0.577475, CORAL: 0.180045, Total_Loss: 0.667498\n","Train Epoch:  3 [27/30]\tLambda: 0.5000, Class: 0.503101, CORAL: 0.426574, Total_Loss: 0.716389\n","Train Epoch:  3 [28/30]\tLambda: 0.5000, Class: 0.456006, CORAL: 0.411211, Total_Loss: 0.661611\n","Train Epoch:  3 [29/30]\tLambda: 0.5000, Class: 0.411810, CORAL: 0.596136, Total_Loss: 0.709878\n","Train Epoch:  3 [30/30]\tLambda: 0.5000, Class: 0.518460, CORAL: 0.394245, Total_Loss: 0.715582\n","\n","Test Epoch: 3 Average loss: 1.3530, Accuracy: 6332/10000 (63.32%)\n","\n","Test Epoch: 3, Accuracy: 63.32%\n","New best model saved with accuracy: 63.32%\n","Train Epoch:  4 [ 1/30]\tLambda: 0.5000, Class: 0.401980, CORAL: 0.577204, Total_Loss: 0.690582\n","Train Epoch:  4 [ 2/30]\tLambda: 0.5000, Class: 0.568084, CORAL: 0.203898, Total_Loss: 0.670033\n","Train Epoch:  4 [ 3/30]\tLambda: 0.5000, Class: 0.603270, CORAL: 0.216258, Total_Loss: 0.711399\n","Train Epoch:  4 [ 4/30]\tLambda: 0.5000, Class: 0.634666, CORAL: 0.194047, Total_Loss: 0.731689\n","Train Epoch:  4 [ 5/30]\tLambda: 0.5000, Class: 0.481462, CORAL: 0.243621, Total_Loss: 0.603273\n","Train Epoch:  4 [ 6/30]\tLambda: 0.5000, Class: 0.562417, CORAL: 0.188458, Total_Loss: 0.656646\n","Train Epoch:  4 [ 7/30]\tLambda: 0.5000, Class: 0.483946, CORAL: 0.277057, Total_Loss: 0.622475\n","Train Epoch:  4 [ 8/30]\tLambda: 0.5000, Class: 0.525124, CORAL: 0.239015, Total_Loss: 0.644632\n","Train Epoch:  4 [ 9/30]\tLambda: 0.5000, Class: 0.636339, CORAL: 0.309759, Total_Loss: 0.791218\n","Train Epoch:  4 [10/30]\tLambda: 0.5000, Class: 0.604511, CORAL: 0.271976, Total_Loss: 0.740499\n","Train Epoch:  4 [11/30]\tLambda: 0.5000, Class: 0.425541, CORAL: 0.400375, Total_Loss: 0.625728\n","Train Epoch:  4 [12/30]\tLambda: 0.5000, Class: 0.407373, CORAL: 0.224347, Total_Loss: 0.519547\n","Train Epoch:  4 [13/30]\tLambda: 0.5000, Class: 0.529494, CORAL: 0.390716, Total_Loss: 0.724852\n","Train Epoch:  4 [14/30]\tLambda: 0.5000, Class: 0.573872, CORAL: 0.209405, Total_Loss: 0.678575\n","Train Epoch:  4 [15/30]\tLambda: 0.5000, Class: 0.503047, CORAL: 0.185308, Total_Loss: 0.595701\n","Train Epoch:  4 [16/30]\tLambda: 0.5000, Class: 0.579804, CORAL: 0.273816, Total_Loss: 0.716712\n","Train Epoch:  4 [17/30]\tLambda: 0.5000, Class: 0.407239, CORAL: 0.337070, Total_Loss: 0.575774\n","Train Epoch:  4 [18/30]\tLambda: 0.5000, Class: 0.400631, CORAL: 0.306992, Total_Loss: 0.554127\n","Train Epoch:  4 [19/30]\tLambda: 0.5000, Class: 0.407890, CORAL: 0.477697, Total_Loss: 0.646738\n","Train Epoch:  4 [20/30]\tLambda: 0.5000, Class: 0.516765, CORAL: 0.408844, Total_Loss: 0.721187\n","Train Epoch:  4 [21/30]\tLambda: 0.5000, Class: 0.317275, CORAL: 0.377611, Total_Loss: 0.506080\n","Train Epoch:  4 [22/30]\tLambda: 0.5000, Class: 0.451765, CORAL: 0.266839, Total_Loss: 0.585185\n","Train Epoch:  4 [23/30]\tLambda: 0.5000, Class: 0.358343, CORAL: 0.605006, Total_Loss: 0.660846\n","Train Epoch:  4 [24/30]\tLambda: 0.5000, Class: 0.311300, CORAL: 0.689778, Total_Loss: 0.656189\n","Train Epoch:  4 [25/30]\tLambda: 0.5000, Class: 0.335820, CORAL: 0.292999, Total_Loss: 0.482320\n","Train Epoch:  4 [26/30]\tLambda: 0.5000, Class: 0.481064, CORAL: 0.501384, Total_Loss: 0.731756\n","Train Epoch:  4 [27/30]\tLambda: 0.5000, Class: 0.393589, CORAL: 0.444379, Total_Loss: 0.615778\n","Train Epoch:  4 [28/30]\tLambda: 0.5000, Class: 0.391968, CORAL: 0.483393, Total_Loss: 0.633665\n","Train Epoch:  4 [29/30]\tLambda: 0.5000, Class: 0.352697, CORAL: 0.538532, Total_Loss: 0.621963\n","Train Epoch:  4 [30/30]\tLambda: 0.5000, Class: 0.451070, CORAL: 0.327016, Total_Loss: 0.614578\n","\n","Test Epoch: 4 Average loss: 1.2833, Accuracy: 6484/10000 (64.84%)\n","\n","Test Epoch: 4, Accuracy: 64.84%\n","New best model saved with accuracy: 64.84%\n","Train Epoch:  5 [ 1/30]\tLambda: 0.5000, Class: 0.432135, CORAL: 0.441898, Total_Loss: 0.653084\n","Train Epoch:  5 [ 2/30]\tLambda: 0.5000, Class: 0.486392, CORAL: 0.190572, Total_Loss: 0.581678\n","Train Epoch:  5 [ 3/30]\tLambda: 0.5000, Class: 0.520129, CORAL: 0.189554, Total_Loss: 0.614906\n","Train Epoch:  5 [ 4/30]\tLambda: 0.5000, Class: 0.523416, CORAL: 0.396511, Total_Loss: 0.721672\n","Train Epoch:  5 [ 5/30]\tLambda: 0.5000, Class: 0.363482, CORAL: 0.310279, Total_Loss: 0.518622\n","Train Epoch:  5 [ 6/30]\tLambda: 0.5000, Class: 0.476887, CORAL: 0.197421, Total_Loss: 0.575598\n","Train Epoch:  5 [ 7/30]\tLambda: 0.5000, Class: 0.364006, CORAL: 0.243134, Total_Loss: 0.485573\n","Train Epoch:  5 [ 8/30]\tLambda: 0.5000, Class: 0.352926, CORAL: 0.412663, Total_Loss: 0.559257\n","Train Epoch:  5 [ 9/30]\tLambda: 0.5000, Class: 0.499336, CORAL: 0.155332, Total_Loss: 0.577002\n","Train Epoch:  5 [10/30]\tLambda: 0.5000, Class: 0.479382, CORAL: 0.333350, Total_Loss: 0.646057\n","Train Epoch:  5 [11/30]\tLambda: 0.5000, Class: 0.349490, CORAL: 0.437253, Total_Loss: 0.568117\n","Train Epoch:  5 [12/30]\tLambda: 0.5000, Class: 0.394846, CORAL: 0.434518, Total_Loss: 0.612105\n","Train Epoch:  5 [13/30]\tLambda: 0.5000, Class: 0.475591, CORAL: 0.207351, Total_Loss: 0.579266\n","Train Epoch:  5 [14/30]\tLambda: 0.5000, Class: 0.331115, CORAL: 0.477247, Total_Loss: 0.569739\n","Train Epoch:  5 [15/30]\tLambda: 0.5000, Class: 0.419071, CORAL: 0.196297, Total_Loss: 0.517220\n","Train Epoch:  5 [16/30]\tLambda: 0.5000, Class: 0.424745, CORAL: 0.263870, Total_Loss: 0.556680\n","Train Epoch:  5 [17/30]\tLambda: 0.5000, Class: 0.477780, CORAL: 0.135793, Total_Loss: 0.545676\n","Train Epoch:  5 [18/30]\tLambda: 0.5000, Class: 0.381380, CORAL: 0.380231, Total_Loss: 0.571495\n","Train Epoch:  5 [19/30]\tLambda: 0.5000, Class: 0.417689, CORAL: 0.271156, Total_Loss: 0.553267\n","Train Epoch:  5 [20/30]\tLambda: 0.5000, Class: 0.359997, CORAL: 0.406179, Total_Loss: 0.563087\n","Train Epoch:  5 [21/30]\tLambda: 0.5000, Class: 0.414243, CORAL: 0.232472, Total_Loss: 0.530479\n","Train Epoch:  5 [22/30]\tLambda: 0.5000, Class: 0.279558, CORAL: 0.560502, Total_Loss: 0.559809\n","Train Epoch:  5 [23/30]\tLambda: 0.5000, Class: 0.322609, CORAL: 0.271702, Total_Loss: 0.458460\n","Train Epoch:  5 [24/30]\tLambda: 0.5000, Class: 0.308328, CORAL: 0.368625, Total_Loss: 0.492640\n","Train Epoch:  5 [25/30]\tLambda: 0.5000, Class: 0.469123, CORAL: 0.388563, Total_Loss: 0.663404\n","Train Epoch:  5 [26/30]\tLambda: 0.5000, Class: 0.372574, CORAL: 0.432158, Total_Loss: 0.588653\n","Train Epoch:  5 [27/30]\tLambda: 0.5000, Class: 0.328799, CORAL: 0.634390, Total_Loss: 0.645995\n","Train Epoch:  5 [28/30]\tLambda: 0.5000, Class: 0.316142, CORAL: 0.549473, Total_Loss: 0.590879\n","Train Epoch:  5 [29/30]\tLambda: 0.5000, Class: 0.473770, CORAL: 0.295294, Total_Loss: 0.621417\n","Train Epoch:  5 [30/30]\tLambda: 0.5000, Class: 0.330288, CORAL: 0.435739, Total_Loss: 0.548157\n","\n","Test Epoch: 5 Average loss: 1.2372, Accuracy: 6582/10000 (65.82%)\n","\n","Test Epoch: 5, Accuracy: 65.82%\n","New best model saved with accuracy: 65.82%\n","Train Epoch:  6 [ 1/30]\tLambda: 0.5000, Class: 0.393496, CORAL: 0.159649, Total_Loss: 0.473320\n","Train Epoch:  6 [ 2/30]\tLambda: 0.5000, Class: 0.387898, CORAL: 0.257070, Total_Loss: 0.516433\n","Train Epoch:  6 [ 3/30]\tLambda: 0.5000, Class: 0.427877, CORAL: 0.415464, Total_Loss: 0.635610\n","Train Epoch:  6 [ 4/30]\tLambda: 0.5000, Class: 0.301000, CORAL: 0.351024, Total_Loss: 0.476513\n","Train Epoch:  6 [ 5/30]\tLambda: 0.5000, Class: 0.340013, CORAL: 0.171314, Total_Loss: 0.425670\n","Train Epoch:  6 [ 6/30]\tLambda: 0.5000, Class: 0.464332, CORAL: 0.217386, Total_Loss: 0.573025\n","Train Epoch:  6 [ 7/30]\tLambda: 0.5000, Class: 0.560914, CORAL: 0.165868, Total_Loss: 0.643848\n","Train Epoch:  6 [ 8/30]\tLambda: 0.5000, Class: 0.452609, CORAL: 0.306664, Total_Loss: 0.605941\n","Train Epoch:  6 [ 9/30]\tLambda: 0.5000, Class: 0.360086, CORAL: 0.330197, Total_Loss: 0.525184\n","Train Epoch:  6 [10/30]\tLambda: 0.5000, Class: 0.416176, CORAL: 0.310497, Total_Loss: 0.571425\n","Train Epoch:  6 [11/30]\tLambda: 0.5000, Class: 0.470412, CORAL: 0.537975, Total_Loss: 0.739400\n","Train Epoch:  6 [12/30]\tLambda: 0.5000, Class: 0.379919, CORAL: 0.215250, Total_Loss: 0.487544\n","Train Epoch:  6 [13/30]\tLambda: 0.5000, Class: 0.359313, CORAL: 0.481830, Total_Loss: 0.600228\n","Train Epoch:  6 [14/30]\tLambda: 0.5000, Class: 0.359878, CORAL: 0.285608, Total_Loss: 0.502682\n","Train Epoch:  6 [15/30]\tLambda: 0.5000, Class: 0.306058, CORAL: 0.427375, Total_Loss: 0.519746\n","Train Epoch:  6 [16/30]\tLambda: 0.5000, Class: 0.286917, CORAL: 0.366162, Total_Loss: 0.469998\n","Train Epoch:  6 [17/30]\tLambda: 0.5000, Class: 0.414427, CORAL: 0.240094, Total_Loss: 0.534474\n","Train Epoch:  6 [18/30]\tLambda: 0.5000, Class: 0.351985, CORAL: 0.465619, Total_Loss: 0.584795\n","Train Epoch:  6 [19/30]\tLambda: 0.5000, Class: 0.355462, CORAL: 0.311612, Total_Loss: 0.511268\n","Train Epoch:  6 [20/30]\tLambda: 0.5000, Class: 0.430226, CORAL: 0.473840, Total_Loss: 0.667146\n","Train Epoch:  6 [21/30]\tLambda: 0.5000, Class: 0.329596, CORAL: 0.159045, Total_Loss: 0.409119\n","Train Epoch:  6 [22/30]\tLambda: 0.5000, Class: 0.332628, CORAL: 0.186223, Total_Loss: 0.425739\n","Train Epoch:  6 [23/30]\tLambda: 0.5000, Class: 0.260932, CORAL: 0.388008, Total_Loss: 0.454936\n","Train Epoch:  6 [24/30]\tLambda: 0.5000, Class: 0.427567, CORAL: 0.162862, Total_Loss: 0.508998\n","Train Epoch:  6 [25/30]\tLambda: 0.5000, Class: 0.298478, CORAL: 0.249883, Total_Loss: 0.423419\n","Train Epoch:  6 [26/30]\tLambda: 0.5000, Class: 0.318159, CORAL: 0.208228, Total_Loss: 0.422273\n","Train Epoch:  6 [27/30]\tLambda: 0.5000, Class: 0.242520, CORAL: 0.388399, Total_Loss: 0.436719\n","Train Epoch:  6 [28/30]\tLambda: 0.5000, Class: 0.274479, CORAL: 0.425087, Total_Loss: 0.487022\n","Train Epoch:  6 [29/30]\tLambda: 0.5000, Class: 0.292623, CORAL: 0.464604, Total_Loss: 0.524925\n","Train Epoch:  6 [30/30]\tLambda: 0.5000, Class: 0.227472, CORAL: 0.313709, Total_Loss: 0.384326\n","\n","Test Epoch: 6 Average loss: 1.2492, Accuracy: 6512/10000 (65.12%)\n","\n","Test Epoch: 6, Accuracy: 65.12%\n","Train Epoch:  7 [ 1/30]\tLambda: 0.5000, Class: 0.401200, CORAL: 0.261822, Total_Loss: 0.532111\n","Train Epoch:  7 [ 2/30]\tLambda: 0.5000, Class: 0.221258, CORAL: 0.633003, Total_Loss: 0.537760\n","Train Epoch:  7 [ 3/30]\tLambda: 0.5000, Class: 0.262091, CORAL: 0.339182, Total_Loss: 0.431682\n","Train Epoch:  7 [ 4/30]\tLambda: 0.5000, Class: 0.217360, CORAL: 0.576765, Total_Loss: 0.505742\n","Train Epoch:  7 [ 5/30]\tLambda: 0.5000, Class: 0.267619, CORAL: 0.269826, Total_Loss: 0.402532\n","Train Epoch:  7 [ 6/30]\tLambda: 0.5000, Class: 0.316520, CORAL: 0.221011, Total_Loss: 0.427025\n","Train Epoch:  7 [ 7/30]\tLambda: 0.5000, Class: 0.384669, CORAL: 0.363378, Total_Loss: 0.566358\n","Train Epoch:  7 [ 8/30]\tLambda: 0.5000, Class: 0.346006, CORAL: 0.170240, Total_Loss: 0.431126\n","Train Epoch:  7 [ 9/30]\tLambda: 0.5000, Class: 0.292683, CORAL: 0.339701, Total_Loss: 0.462534\n","Train Epoch:  7 [10/30]\tLambda: 0.5000, Class: 0.364651, CORAL: 0.141404, Total_Loss: 0.435353\n","Train Epoch:  7 [11/30]\tLambda: 0.5000, Class: 0.274660, CORAL: 0.260268, Total_Loss: 0.404794\n","Train Epoch:  7 [12/30]\tLambda: 0.5000, Class: 0.317544, CORAL: 0.141904, Total_Loss: 0.388496\n","Train Epoch:  7 [13/30]\tLambda: 0.5000, Class: 0.382512, CORAL: 0.131933, Total_Loss: 0.448478\n","Train Epoch:  7 [14/30]\tLambda: 0.5000, Class: 0.306686, CORAL: 0.307679, Total_Loss: 0.460525\n","Train Epoch:  7 [15/30]\tLambda: 0.5000, Class: 0.431101, CORAL: 0.177562, Total_Loss: 0.519882\n","Train Epoch:  7 [16/30]\tLambda: 0.5000, Class: 0.261312, CORAL: 0.385644, Total_Loss: 0.454134\n","Train Epoch:  7 [17/30]\tLambda: 0.5000, Class: 0.234081, CORAL: 0.261332, Total_Loss: 0.364747\n","Train Epoch:  7 [18/30]\tLambda: 0.5000, Class: 0.272851, CORAL: 0.565317, Total_Loss: 0.555509\n","Train Epoch:  7 [19/30]\tLambda: 0.5000, Class: 0.297621, CORAL: 0.364500, Total_Loss: 0.479871\n","Train Epoch:  7 [20/30]\tLambda: 0.5000, Class: 0.383505, CORAL: 0.234442, Total_Loss: 0.500726\n","Train Epoch:  7 [21/30]\tLambda: 0.5000, Class: 0.249364, CORAL: 0.215882, Total_Loss: 0.357305\n","Train Epoch:  7 [22/30]\tLambda: 0.5000, Class: 0.321491, CORAL: 0.302318, Total_Loss: 0.472650\n","Train Epoch:  7 [23/30]\tLambda: 0.5000, Class: 0.281191, CORAL: 0.351224, Total_Loss: 0.456803\n","Train Epoch:  7 [24/30]\tLambda: 0.5000, Class: 0.363055, CORAL: 0.318892, Total_Loss: 0.522501\n","Train Epoch:  7 [25/30]\tLambda: 0.5000, Class: 0.224176, CORAL: 0.444771, Total_Loss: 0.446561\n","Train Epoch:  7 [26/30]\tLambda: 0.5000, Class: 0.222642, CORAL: 0.430175, Total_Loss: 0.437729\n","Train Epoch:  7 [27/30]\tLambda: 0.5000, Class: 0.291852, CORAL: 0.676103, Total_Loss: 0.629904\n","Train Epoch:  7 [28/30]\tLambda: 0.5000, Class: 0.286342, CORAL: 0.558623, Total_Loss: 0.565653\n","Train Epoch:  7 [29/30]\tLambda: 0.5000, Class: 0.314396, CORAL: 0.242030, Total_Loss: 0.435411\n","Train Epoch:  7 [30/30]\tLambda: 0.5000, Class: 0.212366, CORAL: 0.305031, Total_Loss: 0.364881\n","\n","Test Epoch: 7 Average loss: 1.1825, Accuracy: 6707/10000 (67.07%)\n","\n","Test Epoch: 7, Accuracy: 67.07%\n","New best model saved with accuracy: 67.07%\n","Train Epoch:  8 [ 1/30]\tLambda: 0.5000, Class: 0.246928, CORAL: 0.317926, Total_Loss: 0.405891\n","Train Epoch:  8 [ 2/30]\tLambda: 0.5000, Class: 0.328545, CORAL: 0.262339, Total_Loss: 0.459715\n","Train Epoch:  8 [ 3/30]\tLambda: 0.5000, Class: 0.337785, CORAL: 0.245078, Total_Loss: 0.460324\n","Train Epoch:  8 [ 4/30]\tLambda: 0.5000, Class: 0.329391, CORAL: 0.305295, Total_Loss: 0.482038\n","Train Epoch:  8 [ 5/30]\tLambda: 0.5000, Class: 0.295007, CORAL: 0.170355, Total_Loss: 0.380185\n","Train Epoch:  8 [ 6/30]\tLambda: 0.5000, Class: 0.321768, CORAL: 0.325920, Total_Loss: 0.484728\n","Train Epoch:  8 [ 7/30]\tLambda: 0.5000, Class: 0.258152, CORAL: 0.342938, Total_Loss: 0.429621\n","Train Epoch:  8 [ 8/30]\tLambda: 0.5000, Class: 0.273997, CORAL: 0.194432, Total_Loss: 0.371212\n","Train Epoch:  8 [ 9/30]\tLambda: 0.5000, Class: 0.198669, CORAL: 0.469065, Total_Loss: 0.433201\n","Train Epoch:  8 [10/30]\tLambda: 0.5000, Class: 0.275594, CORAL: 0.187239, Total_Loss: 0.369214\n","Train Epoch:  8 [11/30]\tLambda: 0.5000, Class: 0.200709, CORAL: 0.329310, Total_Loss: 0.365364\n","Train Epoch:  8 [12/30]\tLambda: 0.5000, Class: 0.332958, CORAL: 0.157971, Total_Loss: 0.411944\n","Train Epoch:  8 [13/30]\tLambda: 0.5000, Class: 0.236251, CORAL: 0.182987, Total_Loss: 0.327745\n","Train Epoch:  8 [14/30]\tLambda: 0.5000, Class: 0.245551, CORAL: 0.407934, Total_Loss: 0.449518\n","Train Epoch:  8 [15/30]\tLambda: 0.5000, Class: 0.274929, CORAL: 0.230401, Total_Loss: 0.390130\n","Train Epoch:  8 [16/30]\tLambda: 0.5000, Class: 0.306944, CORAL: 0.293788, Total_Loss: 0.453838\n","Train Epoch:  8 [17/30]\tLambda: 0.5000, Class: 0.359150, CORAL: 0.319243, Total_Loss: 0.518771\n","Train Epoch:  8 [18/30]\tLambda: 0.5000, Class: 0.273815, CORAL: 0.342795, Total_Loss: 0.445212\n","Train Epoch:  8 [19/30]\tLambda: 0.5000, Class: 0.277751, CORAL: 0.306732, Total_Loss: 0.431117\n","Train Epoch:  8 [20/30]\tLambda: 0.5000, Class: 0.267804, CORAL: 0.465940, Total_Loss: 0.500774\n","Train Epoch:  8 [21/30]\tLambda: 0.5000, Class: 0.302830, CORAL: 0.389434, Total_Loss: 0.497547\n","Train Epoch:  8 [22/30]\tLambda: 0.5000, Class: 0.219450, CORAL: 0.226076, Total_Loss: 0.332488\n","Train Epoch:  8 [23/30]\tLambda: 0.5000, Class: 0.236449, CORAL: 0.325717, Total_Loss: 0.399308\n","Train Epoch:  8 [24/30]\tLambda: 0.5000, Class: 0.283309, CORAL: 0.191931, Total_Loss: 0.379275\n","Train Epoch:  8 [25/30]\tLambda: 0.5000, Class: 0.218497, CORAL: 0.392915, Total_Loss: 0.414955\n","Train Epoch:  8 [26/30]\tLambda: 0.5000, Class: 0.266171, CORAL: 0.266475, Total_Loss: 0.399409\n","Train Epoch:  8 [27/30]\tLambda: 0.5000, Class: 0.303725, CORAL: 0.267351, Total_Loss: 0.437401\n","Train Epoch:  8 [28/30]\tLambda: 0.5000, Class: 0.282782, CORAL: 0.434468, Total_Loss: 0.500016\n","Train Epoch:  8 [29/30]\tLambda: 0.5000, Class: 0.232906, CORAL: 0.280976, Total_Loss: 0.373394\n","Train Epoch:  8 [30/30]\tLambda: 0.5000, Class: 0.294762, CORAL: 0.225388, Total_Loss: 0.407456\n","\n","Test Epoch: 8 Average loss: 1.1442, Accuracy: 6771/10000 (67.71%)\n","\n","Test Epoch: 8, Accuracy: 67.71%\n","New best model saved with accuracy: 67.71%\n","Train Epoch:  9 [ 1/30]\tLambda: 0.5000, Class: 0.226537, CORAL: 0.363067, Total_Loss: 0.408071\n","Train Epoch:  9 [ 2/30]\tLambda: 0.5000, Class: 0.274952, CORAL: 0.289027, Total_Loss: 0.419465\n","Train Epoch:  9 [ 3/30]\tLambda: 0.5000, Class: 0.352191, CORAL: 0.250506, Total_Loss: 0.477444\n","Train Epoch:  9 [ 4/30]\tLambda: 0.5000, Class: 0.244342, CORAL: 0.229061, Total_Loss: 0.358872\n","Train Epoch:  9 [ 5/30]\tLambda: 0.5000, Class: 0.306471, CORAL: 0.256565, Total_Loss: 0.434753\n","Train Epoch:  9 [ 6/30]\tLambda: 0.5000, Class: 0.182309, CORAL: 0.228710, Total_Loss: 0.296664\n","Train Epoch:  9 [ 7/30]\tLambda: 0.5000, Class: 0.228141, CORAL: 0.553734, Total_Loss: 0.505009\n","Train Epoch:  9 [ 8/30]\tLambda: 0.5000, Class: 0.233445, CORAL: 0.510031, Total_Loss: 0.488461\n","Train Epoch:  9 [ 9/30]\tLambda: 0.5000, Class: 0.199420, CORAL: 0.237875, Total_Loss: 0.318358\n","Train Epoch:  9 [10/30]\tLambda: 0.5000, Class: 0.260153, CORAL: 0.360004, Total_Loss: 0.440156\n","Train Epoch:  9 [11/30]\tLambda: 0.5000, Class: 0.211163, CORAL: 0.436366, Total_Loss: 0.429346\n","Train Epoch:  9 [12/30]\tLambda: 0.5000, Class: 0.325742, CORAL: 0.192061, Total_Loss: 0.421773\n","Train Epoch:  9 [13/30]\tLambda: 0.5000, Class: 0.243934, CORAL: 0.256896, Total_Loss: 0.372382\n","Train Epoch:  9 [14/30]\tLambda: 0.5000, Class: 0.204799, CORAL: 0.218953, Total_Loss: 0.314276\n","Train Epoch:  9 [15/30]\tLambda: 0.5000, Class: 0.234643, CORAL: 0.317100, Total_Loss: 0.393193\n","Train Epoch:  9 [16/30]\tLambda: 0.5000, Class: 0.258738, CORAL: 0.220620, Total_Loss: 0.369048\n","Train Epoch:  9 [17/30]\tLambda: 0.5000, Class: 0.248048, CORAL: 0.309779, Total_Loss: 0.402937\n","Train Epoch:  9 [18/30]\tLambda: 0.5000, Class: 0.297461, CORAL: 0.341057, Total_Loss: 0.467989\n","Train Epoch:  9 [19/30]\tLambda: 0.5000, Class: 0.230280, CORAL: 0.451243, Total_Loss: 0.455901\n","Train Epoch:  9 [20/30]\tLambda: 0.5000, Class: 0.326726, CORAL: 0.345246, Total_Loss: 0.499349\n","Train Epoch:  9 [21/30]\tLambda: 0.5000, Class: 0.218420, CORAL: 0.459538, Total_Loss: 0.448189\n","Train Epoch:  9 [22/30]\tLambda: 0.5000, Class: 0.200579, CORAL: 0.335542, Total_Loss: 0.368349\n","Train Epoch:  9 [23/30]\tLambda: 0.5000, Class: 0.282137, CORAL: 0.244553, Total_Loss: 0.404413\n","Train Epoch:  9 [24/30]\tLambda: 0.5000, Class: 0.278706, CORAL: 0.271951, Total_Loss: 0.414681\n","Train Epoch:  9 [25/30]\tLambda: 0.5000, Class: 0.229271, CORAL: 0.398418, Total_Loss: 0.428479\n","Train Epoch:  9 [26/30]\tLambda: 0.5000, Class: 0.203072, CORAL: 0.144225, Total_Loss: 0.275184\n","Train Epoch:  9 [27/30]\tLambda: 0.5000, Class: 0.208943, CORAL: 0.252486, Total_Loss: 0.335186\n","Train Epoch:  9 [28/30]\tLambda: 0.5000, Class: 0.236198, CORAL: 0.340081, Total_Loss: 0.406239\n","Train Epoch:  9 [29/30]\tLambda: 0.5000, Class: 0.334627, CORAL: 0.343016, Total_Loss: 0.506135\n","Train Epoch:  9 [30/30]\tLambda: 0.5000, Class: 0.235259, CORAL: 0.323908, Total_Loss: 0.397213\n","\n","Test Epoch: 9 Average loss: 1.2010, Accuracy: 6658/10000 (66.58%)\n","\n","Test Epoch: 9, Accuracy: 66.58%\n","Train Epoch: 10 [ 1/30]\tLambda: 0.5000, Class: 0.381763, CORAL: 0.154209, Total_Loss: 0.458867\n","Train Epoch: 10 [ 2/30]\tLambda: 0.5000, Class: 0.265185, CORAL: 0.250825, Total_Loss: 0.390598\n","Train Epoch: 10 [ 3/30]\tLambda: 0.5000, Class: 0.259687, CORAL: 0.248314, Total_Loss: 0.383844\n","Train Epoch: 10 [ 4/30]\tLambda: 0.5000, Class: 0.249535, CORAL: 0.348437, Total_Loss: 0.423753\n","Train Epoch: 10 [ 5/30]\tLambda: 0.5000, Class: 0.251859, CORAL: 0.258764, Total_Loss: 0.381241\n","Train Epoch: 10 [ 6/30]\tLambda: 0.5000, Class: 0.230450, CORAL: 0.273092, Total_Loss: 0.366996\n","Train Epoch: 10 [ 7/30]\tLambda: 0.5000, Class: 0.228468, CORAL: 0.250546, Total_Loss: 0.353741\n","Train Epoch: 10 [ 8/30]\tLambda: 0.5000, Class: 0.156143, CORAL: 0.406367, Total_Loss: 0.359326\n","Train Epoch: 10 [ 9/30]\tLambda: 0.5000, Class: 0.218964, CORAL: 0.469672, Total_Loss: 0.453800\n","Train Epoch: 10 [10/30]\tLambda: 0.5000, Class: 0.234191, CORAL: 0.240815, Total_Loss: 0.354599\n","Train Epoch: 10 [11/30]\tLambda: 0.5000, Class: 0.211412, CORAL: 0.175250, Total_Loss: 0.299037\n","Train Epoch: 10 [12/30]\tLambda: 0.5000, Class: 0.264586, CORAL: 0.207927, Total_Loss: 0.368550\n","Train Epoch: 10 [13/30]\tLambda: 0.5000, Class: 0.242947, CORAL: 0.303951, Total_Loss: 0.394922\n","Train Epoch: 10 [14/30]\tLambda: 0.5000, Class: 0.187152, CORAL: 0.484151, Total_Loss: 0.429228\n","Train Epoch: 10 [15/30]\tLambda: 0.5000, Class: 0.224913, CORAL: 0.312515, Total_Loss: 0.381171\n","Train Epoch: 10 [16/30]\tLambda: 0.5000, Class: 0.220503, CORAL: 0.272278, Total_Loss: 0.356643\n","Train Epoch: 10 [17/30]\tLambda: 0.5000, Class: 0.167461, CORAL: 0.639216, Total_Loss: 0.487069\n","Train Epoch: 10 [18/30]\tLambda: 0.5000, Class: 0.194085, CORAL: 0.510603, Total_Loss: 0.449386\n","Train Epoch: 10 [19/30]\tLambda: 0.5000, Class: 0.211972, CORAL: 0.446011, Total_Loss: 0.434977\n","Train Epoch: 10 [20/30]\tLambda: 0.5000, Class: 0.241199, CORAL: 0.259354, Total_Loss: 0.370876\n","Train Epoch: 10 [21/30]\tLambda: 0.5000, Class: 0.204311, CORAL: 0.288392, Total_Loss: 0.348507\n","Train Epoch: 10 [22/30]\tLambda: 0.5000, Class: 0.199727, CORAL: 0.414564, Total_Loss: 0.407009\n","Train Epoch: 10 [23/30]\tLambda: 0.5000, Class: 0.286791, CORAL: 0.222719, Total_Loss: 0.398150\n","Train Epoch: 10 [24/30]\tLambda: 0.5000, Class: 0.216735, CORAL: 0.240344, Total_Loss: 0.336907\n","Train Epoch: 10 [25/30]\tLambda: 0.5000, Class: 0.254503, CORAL: 0.274899, Total_Loss: 0.391953\n","Train Epoch: 10 [26/30]\tLambda: 0.5000, Class: 0.289801, CORAL: 0.322579, Total_Loss: 0.451091\n","Train Epoch: 10 [27/30]\tLambda: 0.5000, Class: 0.247913, CORAL: 0.130556, Total_Loss: 0.313191\n","Train Epoch: 10 [28/30]\tLambda: 0.5000, Class: 0.282287, CORAL: 0.136799, Total_Loss: 0.350686\n","Train Epoch: 10 [29/30]\tLambda: 0.5000, Class: 0.218127, CORAL: 0.256476, Total_Loss: 0.346365\n","Train Epoch: 10 [30/30]\tLambda: 0.5000, Class: 0.292440, CORAL: 0.241166, Total_Loss: 0.413024\n","\n","Test Epoch: 10 Average loss: 1.1496, Accuracy: 6743/10000 (67.43%)\n","\n","Test Epoch: 10, Accuracy: 67.43%\n","Train Epoch: 11 [ 1/30]\tLambda: 0.5000, Class: 0.242087, CORAL: 0.320584, Total_Loss: 0.402379\n","Train Epoch: 11 [ 2/30]\tLambda: 0.5000, Class: 0.245840, CORAL: 0.224501, Total_Loss: 0.358090\n","Train Epoch: 11 [ 3/30]\tLambda: 0.5000, Class: 0.215898, CORAL: 0.179743, Total_Loss: 0.305769\n","Train Epoch: 11 [ 4/30]\tLambda: 0.5000, Class: 0.170573, CORAL: 0.334502, Total_Loss: 0.337824\n","Train Epoch: 11 [ 5/30]\tLambda: 0.5000, Class: 0.207316, CORAL: 0.318301, Total_Loss: 0.366467\n","Train Epoch: 11 [ 6/30]\tLambda: 0.5000, Class: 0.188767, CORAL: 0.199006, Total_Loss: 0.288270\n","Train Epoch: 11 [ 7/30]\tLambda: 0.5000, Class: 0.272673, CORAL: 0.273158, Total_Loss: 0.409252\n","Train Epoch: 11 [ 8/30]\tLambda: 0.5000, Class: 0.233139, CORAL: 0.136641, Total_Loss: 0.301459\n","Train Epoch: 11 [ 9/30]\tLambda: 0.5000, Class: 0.232917, CORAL: 0.472644, Total_Loss: 0.469239\n","Train Epoch: 11 [10/30]\tLambda: 0.5000, Class: 0.221466, CORAL: 0.203208, Total_Loss: 0.323069\n","Train Epoch: 11 [11/30]\tLambda: 0.5000, Class: 0.223141, CORAL: 0.237874, Total_Loss: 0.342078\n","Train Epoch: 11 [12/30]\tLambda: 0.5000, Class: 0.244687, CORAL: 0.243571, Total_Loss: 0.366472\n","Train Epoch: 11 [13/30]\tLambda: 0.5000, Class: 0.224340, CORAL: 0.233197, Total_Loss: 0.340939\n","Train Epoch: 11 [14/30]\tLambda: 0.5000, Class: 0.184571, CORAL: 0.209394, Total_Loss: 0.289268\n","Train Epoch: 11 [15/30]\tLambda: 0.5000, Class: 0.184784, CORAL: 0.344809, Total_Loss: 0.357189\n","Train Epoch: 11 [16/30]\tLambda: 0.5000, Class: 0.158993, CORAL: 0.605945, Total_Loss: 0.461965\n","Train Epoch: 11 [17/30]\tLambda: 0.5000, Class: 0.259448, CORAL: 0.427768, Total_Loss: 0.473332\n","Train Epoch: 11 [18/30]\tLambda: 0.5000, Class: 0.168578, CORAL: 0.462118, Total_Loss: 0.399637\n","Train Epoch: 11 [19/30]\tLambda: 0.5000, Class: 0.272326, CORAL: 0.298901, Total_Loss: 0.421777\n","Train Epoch: 11 [20/30]\tLambda: 0.5000, Class: 0.190917, CORAL: 0.266894, Total_Loss: 0.324365\n","Train Epoch: 11 [21/30]\tLambda: 0.5000, Class: 0.172380, CORAL: 0.344812, Total_Loss: 0.344786\n","Train Epoch: 11 [22/30]\tLambda: 0.5000, Class: 0.242247, CORAL: 0.210363, Total_Loss: 0.347428\n","Train Epoch: 11 [23/30]\tLambda: 0.5000, Class: 0.204188, CORAL: 0.248148, Total_Loss: 0.328262\n","Train Epoch: 11 [24/30]\tLambda: 0.5000, Class: 0.222484, CORAL: 0.226335, Total_Loss: 0.335651\n","Train Epoch: 11 [25/30]\tLambda: 0.5000, Class: 0.145449, CORAL: 0.306021, Total_Loss: 0.298460\n","Train Epoch: 11 [26/30]\tLambda: 0.5000, Class: 0.189975, CORAL: 0.449051, Total_Loss: 0.414500\n","Train Epoch: 11 [27/30]\tLambda: 0.5000, Class: 0.213595, CORAL: 0.359029, Total_Loss: 0.393110\n","Train Epoch: 11 [28/30]\tLambda: 0.5000, Class: 0.157712, CORAL: 0.326655, Total_Loss: 0.321040\n","Train Epoch: 11 [29/30]\tLambda: 0.5000, Class: 0.208587, CORAL: 0.276350, Total_Loss: 0.346762\n","Train Epoch: 11 [30/30]\tLambda: 0.5000, Class: 0.223075, CORAL: 0.201498, Total_Loss: 0.323824\n","\n","Test Epoch: 11 Average loss: 1.2078, Accuracy: 6569/10000 (65.69%)\n","\n","Test Epoch: 11, Accuracy: 65.69%\n","Train Epoch: 12 [ 1/30]\tLambda: 0.5000, Class: 0.176359, CORAL: 0.221513, Total_Loss: 0.287116\n","Train Epoch: 12 [ 2/30]\tLambda: 0.5000, Class: 0.205959, CORAL: 0.436019, Total_Loss: 0.423969\n","Train Epoch: 12 [ 3/30]\tLambda: 0.5000, Class: 0.234916, CORAL: 0.434520, Total_Loss: 0.452176\n","Train Epoch: 12 [ 4/30]\tLambda: 0.5000, Class: 0.198432, CORAL: 0.243160, Total_Loss: 0.320012\n","Train Epoch: 12 [ 5/30]\tLambda: 0.5000, Class: 0.238033, CORAL: 0.248786, Total_Loss: 0.362426\n","Train Epoch: 12 [ 6/30]\tLambda: 0.5000, Class: 0.218245, CORAL: 0.185564, Total_Loss: 0.311027\n","Train Epoch: 12 [ 7/30]\tLambda: 0.5000, Class: 0.214564, CORAL: 0.351648, Total_Loss: 0.390388\n","Train Epoch: 12 [ 8/30]\tLambda: 0.5000, Class: 0.254231, CORAL: 0.242942, Total_Loss: 0.375702\n","Train Epoch: 12 [ 9/30]\tLambda: 0.5000, Class: 0.255878, CORAL: 0.198757, Total_Loss: 0.355256\n","Train Epoch: 12 [10/30]\tLambda: 0.5000, Class: 0.283475, CORAL: 0.220079, Total_Loss: 0.393515\n","Train Epoch: 12 [11/30]\tLambda: 0.5000, Class: 0.202668, CORAL: 0.217270, Total_Loss: 0.311303\n","Train Epoch: 12 [12/30]\tLambda: 0.5000, Class: 0.208950, CORAL: 0.195514, Total_Loss: 0.306707\n","Train Epoch: 12 [13/30]\tLambda: 0.5000, Class: 0.190850, CORAL: 0.383500, Total_Loss: 0.382600\n","Train Epoch: 12 [14/30]\tLambda: 0.5000, Class: 0.180851, CORAL: 0.312779, Total_Loss: 0.337240\n","Train Epoch: 12 [15/30]\tLambda: 0.5000, Class: 0.213749, CORAL: 0.249351, Total_Loss: 0.338424\n","Train Epoch: 12 [16/30]\tLambda: 0.5000, Class: 0.238917, CORAL: 0.248188, Total_Loss: 0.363012\n","Train Epoch: 12 [17/30]\tLambda: 0.5000, Class: 0.203322, CORAL: 0.125770, Total_Loss: 0.266207\n","Train Epoch: 12 [18/30]\tLambda: 0.5000, Class: 0.265903, CORAL: 0.478919, Total_Loss: 0.505362\n","Train Epoch: 12 [19/30]\tLambda: 0.5000, Class: 0.266240, CORAL: 0.283313, Total_Loss: 0.407896\n","Train Epoch: 12 [20/30]\tLambda: 0.5000, Class: 0.172633, CORAL: 0.382049, Total_Loss: 0.363658\n","Train Epoch: 12 [21/30]\tLambda: 0.5000, Class: 0.246062, CORAL: 0.252803, Total_Loss: 0.372463\n","Train Epoch: 12 [22/30]\tLambda: 0.5000, Class: 0.183992, CORAL: 0.342251, Total_Loss: 0.355118\n","Train Epoch: 12 [23/30]\tLambda: 0.5000, Class: 0.231883, CORAL: 0.222390, Total_Loss: 0.343078\n","Train Epoch: 12 [24/30]\tLambda: 0.5000, Class: 0.200712, CORAL: 0.256642, Total_Loss: 0.329033\n","Train Epoch: 12 [25/30]\tLambda: 0.5000, Class: 0.210639, CORAL: 0.405454, Total_Loss: 0.413366\n","Train Epoch: 12 [26/30]\tLambda: 0.5000, Class: 0.190898, CORAL: 0.246480, Total_Loss: 0.314138\n","Train Epoch: 12 [27/30]\tLambda: 0.5000, Class: 0.194068, CORAL: 0.254449, Total_Loss: 0.321292\n","Train Epoch: 12 [28/30]\tLambda: 0.5000, Class: 0.166140, CORAL: 0.592652, Total_Loss: 0.462466\n","Train Epoch: 12 [29/30]\tLambda: 0.5000, Class: 0.167195, CORAL: 0.174304, Total_Loss: 0.254347\n","Train Epoch: 12 [30/30]\tLambda: 0.5000, Class: 0.198824, CORAL: 0.363940, Total_Loss: 0.380793\n","\n","Test Epoch: 12 Average loss: 1.1570, Accuracy: 6714/10000 (67.14%)\n","\n","Test Epoch: 12, Accuracy: 67.14%\n","Train Epoch: 13 [ 1/30]\tLambda: 0.5000, Class: 0.178147, CORAL: 0.259394, Total_Loss: 0.307844\n","Train Epoch: 13 [ 2/30]\tLambda: 0.5000, Class: 0.181680, CORAL: 0.272381, Total_Loss: 0.317870\n","Train Epoch: 13 [ 3/30]\tLambda: 0.5000, Class: 0.194062, CORAL: 0.368848, Total_Loss: 0.378486\n","Train Epoch: 13 [ 4/30]\tLambda: 0.5000, Class: 0.164562, CORAL: 0.500475, Total_Loss: 0.414800\n","Train Epoch: 13 [ 5/30]\tLambda: 0.5000, Class: 0.153094, CORAL: 0.376809, Total_Loss: 0.341499\n","Train Epoch: 13 [ 6/30]\tLambda: 0.5000, Class: 0.184319, CORAL: 0.343708, Total_Loss: 0.356173\n","Train Epoch: 13 [ 7/30]\tLambda: 0.5000, Class: 0.166396, CORAL: 0.295147, Total_Loss: 0.313970\n","Train Epoch: 13 [ 8/30]\tLambda: 0.5000, Class: 0.211681, CORAL: 0.233582, Total_Loss: 0.328472\n","Train Epoch: 13 [ 9/30]\tLambda: 0.5000, Class: 0.207142, CORAL: 0.212005, Total_Loss: 0.313145\n","Train Epoch: 13 [10/30]\tLambda: 0.5000, Class: 0.230975, CORAL: 0.242954, Total_Loss: 0.352452\n","Train Epoch: 13 [11/30]\tLambda: 0.5000, Class: 0.163187, CORAL: 0.284361, Total_Loss: 0.305367\n","Train Epoch: 13 [12/30]\tLambda: 0.5000, Class: 0.253364, CORAL: 0.353266, Total_Loss: 0.429997\n","Train Epoch: 13 [13/30]\tLambda: 0.5000, Class: 0.199805, CORAL: 0.139225, Total_Loss: 0.269418\n","Train Epoch: 13 [14/30]\tLambda: 0.5000, Class: 0.190346, CORAL: 0.262936, Total_Loss: 0.321814\n","Train Epoch: 13 [15/30]\tLambda: 0.5000, Class: 0.219316, CORAL: 0.316229, Total_Loss: 0.377431\n","Train Epoch: 13 [16/30]\tLambda: 0.5000, Class: 0.219232, CORAL: 0.223659, Total_Loss: 0.331062\n","Train Epoch: 13 [17/30]\tLambda: 0.5000, Class: 0.207465, CORAL: 0.123965, Total_Loss: 0.269448\n","Train Epoch: 13 [18/30]\tLambda: 0.5000, Class: 0.259525, CORAL: 0.237715, Total_Loss: 0.378382\n","Train Epoch: 13 [19/30]\tLambda: 0.5000, Class: 0.196852, CORAL: 0.579590, Total_Loss: 0.486647\n","Train Epoch: 13 [20/30]\tLambda: 0.5000, Class: 0.233186, CORAL: 0.431191, Total_Loss: 0.448781\n","Train Epoch: 13 [21/30]\tLambda: 0.5000, Class: 0.208933, CORAL: 0.496456, Total_Loss: 0.457161\n","Train Epoch: 13 [22/30]\tLambda: 0.5000, Class: 0.193206, CORAL: 0.268678, Total_Loss: 0.327545\n","Train Epoch: 13 [23/30]\tLambda: 0.5000, Class: 0.188223, CORAL: 0.306282, Total_Loss: 0.341363\n","Train Epoch: 13 [24/30]\tLambda: 0.5000, Class: 0.200873, CORAL: 0.199124, Total_Loss: 0.300435\n","Train Epoch: 13 [25/30]\tLambda: 0.5000, Class: 0.194596, CORAL: 0.274752, Total_Loss: 0.331972\n","Train Epoch: 13 [26/30]\tLambda: 0.5000, Class: 0.159495, CORAL: 0.211441, Total_Loss: 0.265216\n","Train Epoch: 13 [27/30]\tLambda: 0.5000, Class: 0.191139, CORAL: 0.169494, Total_Loss: 0.275885\n","Train Epoch: 13 [28/30]\tLambda: 0.5000, Class: 0.199206, CORAL: 0.373569, Total_Loss: 0.385990\n","Train Epoch: 13 [29/30]\tLambda: 0.5000, Class: 0.265544, CORAL: 0.209202, Total_Loss: 0.370145\n","Train Epoch: 13 [30/30]\tLambda: 0.5000, Class: 0.196668, CORAL: 0.190538, Total_Loss: 0.291937\n","\n","Test Epoch: 13 Average loss: 1.1890, Accuracy: 6634/10000 (66.34%)\n","\n","Test Epoch: 13, Accuracy: 66.34%\n","Train Epoch: 14 [ 1/30]\tLambda: 0.5000, Class: 0.205771, CORAL: 0.180593, Total_Loss: 0.296067\n","Train Epoch: 14 [ 2/30]\tLambda: 0.5000, Class: 0.201878, CORAL: 0.168513, Total_Loss: 0.286135\n","Train Epoch: 14 [ 3/30]\tLambda: 0.5000, Class: 0.214649, CORAL: 0.475128, Total_Loss: 0.452213\n","Train Epoch: 14 [ 4/30]\tLambda: 0.5000, Class: 0.193787, CORAL: 0.124352, Total_Loss: 0.255963\n","Train Epoch: 14 [ 5/30]\tLambda: 0.5000, Class: 0.184489, CORAL: 0.103566, Total_Loss: 0.236272\n","Train Epoch: 14 [ 6/30]\tLambda: 0.5000, Class: 0.168645, CORAL: 0.161630, Total_Loss: 0.249460\n","Train Epoch: 14 [ 7/30]\tLambda: 0.5000, Class: 0.238814, CORAL: 0.331920, Total_Loss: 0.404774\n","Train Epoch: 14 [ 8/30]\tLambda: 0.5000, Class: 0.170458, CORAL: 0.397782, Total_Loss: 0.369349\n","Train Epoch: 14 [ 9/30]\tLambda: 0.5000, Class: 0.163353, CORAL: 0.296764, Total_Loss: 0.311735\n","Train Epoch: 14 [10/30]\tLambda: 0.5000, Class: 0.162312, CORAL: 0.418842, Total_Loss: 0.371733\n","Train Epoch: 14 [11/30]\tLambda: 0.5000, Class: 0.161507, CORAL: 0.091537, Total_Loss: 0.207275\n","Train Epoch: 14 [12/30]\tLambda: 0.5000, Class: 0.153969, CORAL: 0.376469, Total_Loss: 0.342203\n","Train Epoch: 14 [13/30]\tLambda: 0.5000, Class: 0.167563, CORAL: 0.214844, Total_Loss: 0.274985\n","Train Epoch: 14 [14/30]\tLambda: 0.5000, Class: 0.152515, CORAL: 0.291164, Total_Loss: 0.298098\n","Train Epoch: 14 [15/30]\tLambda: 0.5000, Class: 0.176154, CORAL: 0.363011, Total_Loss: 0.357660\n","Train Epoch: 14 [16/30]\tLambda: 0.5000, Class: 0.230408, CORAL: 0.306780, Total_Loss: 0.383798\n","Train Epoch: 14 [17/30]\tLambda: 0.5000, Class: 0.163877, CORAL: 0.400837, Total_Loss: 0.364295\n","Train Epoch: 14 [18/30]\tLambda: 0.5000, Class: 0.153274, CORAL: 0.333196, Total_Loss: 0.319872\n","Train Epoch: 14 [19/30]\tLambda: 0.5000, Class: 0.187029, CORAL: 0.308031, Total_Loss: 0.341044\n","Train Epoch: 14 [20/30]\tLambda: 0.5000, Class: 0.190653, CORAL: 0.312994, Total_Loss: 0.347150\n","Train Epoch: 14 [21/30]\tLambda: 0.5000, Class: 0.215143, CORAL: 0.218559, Total_Loss: 0.324422\n","Train Epoch: 14 [22/30]\tLambda: 0.5000, Class: 0.189961, CORAL: 0.436696, Total_Loss: 0.408309\n","Train Epoch: 14 [23/30]\tLambda: 0.5000, Class: 0.170618, CORAL: 0.229342, Total_Loss: 0.285289\n","Train Epoch: 14 [24/30]\tLambda: 0.5000, Class: 0.208058, CORAL: 0.216556, Total_Loss: 0.316336\n","Train Epoch: 14 [25/30]\tLambda: 0.5000, Class: 0.181509, CORAL: 0.184522, Total_Loss: 0.273771\n","Train Epoch: 14 [26/30]\tLambda: 0.5000, Class: 0.187729, CORAL: 0.341937, Total_Loss: 0.358697\n","Train Epoch: 14 [27/30]\tLambda: 0.5000, Class: 0.193800, CORAL: 0.403844, Total_Loss: 0.395722\n","Train Epoch: 14 [28/30]\tLambda: 0.5000, Class: 0.164772, CORAL: 0.381435, Total_Loss: 0.355490\n","Train Epoch: 14 [29/30]\tLambda: 0.5000, Class: 0.162788, CORAL: 0.250575, Total_Loss: 0.288075\n","Train Epoch: 14 [30/30]\tLambda: 0.5000, Class: 0.242087, CORAL: 0.253828, Total_Loss: 0.369001\n","\n","Test Epoch: 14 Average loss: 1.1926, Accuracy: 6613/10000 (66.13%)\n","\n","Test Epoch: 14, Accuracy: 66.13%\n","Train Epoch: 15 [ 1/30]\tLambda: 0.5000, Class: 0.160224, CORAL: 0.220216, Total_Loss: 0.270332\n","Train Epoch: 15 [ 2/30]\tLambda: 0.5000, Class: 0.194544, CORAL: 0.286131, Total_Loss: 0.337609\n","Train Epoch: 15 [ 3/30]\tLambda: 0.5000, Class: 0.186778, CORAL: 0.155508, Total_Loss: 0.264532\n","Train Epoch: 15 [ 4/30]\tLambda: 0.5000, Class: 0.203290, CORAL: 0.225296, Total_Loss: 0.315938\n","Train Epoch: 15 [ 5/30]\tLambda: 0.5000, Class: 0.175365, CORAL: 0.251119, Total_Loss: 0.300925\n","Train Epoch: 15 [ 6/30]\tLambda: 0.5000, Class: 0.177283, CORAL: 0.262839, Total_Loss: 0.308702\n","Train Epoch: 15 [ 7/30]\tLambda: 0.5000, Class: 0.196967, CORAL: 0.385739, Total_Loss: 0.389837\n","Train Epoch: 15 [ 8/30]\tLambda: 0.5000, Class: 0.201532, CORAL: 0.157100, Total_Loss: 0.280082\n","Train Epoch: 15 [ 9/30]\tLambda: 0.5000, Class: 0.148545, CORAL: 0.247393, Total_Loss: 0.272242\n","Train Epoch: 15 [10/30]\tLambda: 0.5000, Class: 0.194249, CORAL: 0.192146, Total_Loss: 0.290322\n","Train Epoch: 15 [11/30]\tLambda: 0.5000, Class: 0.191363, CORAL: 0.166772, Total_Loss: 0.274749\n","Train Epoch: 15 [12/30]\tLambda: 0.5000, Class: 0.198805, CORAL: 0.255244, Total_Loss: 0.326427\n","Train Epoch: 15 [13/30]\tLambda: 0.5000, Class: 0.195790, CORAL: 0.268470, Total_Loss: 0.330024\n","Train Epoch: 15 [14/30]\tLambda: 0.5000, Class: 0.131657, CORAL: 0.493413, Total_Loss: 0.378364\n","Train Epoch: 15 [15/30]\tLambda: 0.5000, Class: 0.156763, CORAL: 0.236512, Total_Loss: 0.275019\n","Train Epoch: 15 [16/30]\tLambda: 0.5000, Class: 0.161810, CORAL: 0.316878, Total_Loss: 0.320249\n","Train Epoch: 15 [17/30]\tLambda: 0.5000, Class: 0.140875, CORAL: 0.470949, Total_Loss: 0.376350\n","Train Epoch: 15 [18/30]\tLambda: 0.5000, Class: 0.179356, CORAL: 0.454689, Total_Loss: 0.406701\n","Train Epoch: 15 [19/30]\tLambda: 0.5000, Class: 0.165715, CORAL: 0.235149, Total_Loss: 0.283290\n","Train Epoch: 15 [20/30]\tLambda: 0.5000, Class: 0.169285, CORAL: 0.488575, Total_Loss: 0.413573\n","Train Epoch: 15 [21/30]\tLambda: 0.5000, Class: 0.170366, CORAL: 0.222623, Total_Loss: 0.281677\n","Train Epoch: 15 [22/30]\tLambda: 0.5000, Class: 0.176495, CORAL: 0.439389, Total_Loss: 0.396190\n","Train Epoch: 15 [23/30]\tLambda: 0.5000, Class: 0.163147, CORAL: 0.287162, Total_Loss: 0.306728\n","Train Epoch: 15 [24/30]\tLambda: 0.5000, Class: 0.188924, CORAL: 0.257595, Total_Loss: 0.317722\n","Train Epoch: 15 [25/30]\tLambda: 0.5000, Class: 0.196358, CORAL: 0.312184, Total_Loss: 0.352450\n","Train Epoch: 15 [26/30]\tLambda: 0.5000, Class: 0.166632, CORAL: 0.340522, Total_Loss: 0.336893\n","Train Epoch: 15 [27/30]\tLambda: 0.5000, Class: 0.202318, CORAL: 0.377129, Total_Loss: 0.390883\n","Train Epoch: 15 [28/30]\tLambda: 0.5000, Class: 0.269861, CORAL: 0.265584, Total_Loss: 0.402653\n","Train Epoch: 15 [29/30]\tLambda: 0.5000, Class: 0.335489, CORAL: 0.131291, Total_Loss: 0.401135\n","Train Epoch: 15 [30/30]\tLambda: 0.5000, Class: 0.175946, CORAL: 0.297424, Total_Loss: 0.324659\n","\n","Test Epoch: 15 Average loss: 1.1397, Accuracy: 6792/10000 (67.92%)\n","\n","Test Epoch: 15, Accuracy: 67.92%\n","New best model saved with accuracy: 67.92%\n","Train Epoch: 16 [ 1/30]\tLambda: 0.5000, Class: 0.197114, CORAL: 0.240907, Total_Loss: 0.317568\n","Train Epoch: 16 [ 2/30]\tLambda: 0.5000, Class: 0.194188, CORAL: 0.293950, Total_Loss: 0.341163\n","Train Epoch: 16 [ 3/30]\tLambda: 0.5000, Class: 0.186118, CORAL: 0.263214, Total_Loss: 0.317725\n","Train Epoch: 16 [ 4/30]\tLambda: 0.5000, Class: 0.199247, CORAL: 0.237845, Total_Loss: 0.318170\n","Train Epoch: 16 [ 5/30]\tLambda: 0.5000, Class: 0.213140, CORAL: 0.191748, Total_Loss: 0.309014\n","Train Epoch: 16 [ 6/30]\tLambda: 0.5000, Class: 0.210203, CORAL: 0.182157, Total_Loss: 0.301281\n","Train Epoch: 16 [ 7/30]\tLambda: 0.5000, Class: 0.193207, CORAL: 0.252248, Total_Loss: 0.319331\n","Train Epoch: 16 [ 8/30]\tLambda: 0.5000, Class: 0.204399, CORAL: 0.169727, Total_Loss: 0.289262\n","Train Epoch: 16 [ 9/30]\tLambda: 0.5000, Class: 0.183319, CORAL: 0.313883, Total_Loss: 0.340261\n","Train Epoch: 16 [10/30]\tLambda: 0.5000, Class: 0.178215, CORAL: 0.177879, Total_Loss: 0.267155\n","Train Epoch: 16 [11/30]\tLambda: 0.5000, Class: 0.155155, CORAL: 0.229713, Total_Loss: 0.270011\n","Train Epoch: 16 [12/30]\tLambda: 0.5000, Class: 0.153569, CORAL: 0.260801, Total_Loss: 0.283970\n","Train Epoch: 16 [13/30]\tLambda: 0.5000, Class: 0.189740, CORAL: 0.199152, Total_Loss: 0.289315\n","Train Epoch: 16 [14/30]\tLambda: 0.5000, Class: 0.207931, CORAL: 0.183877, Total_Loss: 0.299869\n","Train Epoch: 16 [15/30]\tLambda: 0.5000, Class: 0.237746, CORAL: 0.326825, Total_Loss: 0.401159\n","Train Epoch: 16 [16/30]\tLambda: 0.5000, Class: 0.164970, CORAL: 0.266315, Total_Loss: 0.298128\n","Train Epoch: 16 [17/30]\tLambda: 0.5000, Class: 0.160916, CORAL: 0.305298, Total_Loss: 0.313565\n","Train Epoch: 16 [18/30]\tLambda: 0.5000, Class: 0.246999, CORAL: 0.206151, Total_Loss: 0.350075\n","Train Epoch: 16 [19/30]\tLambda: 0.5000, Class: 0.151023, CORAL: 0.412838, Total_Loss: 0.357442\n","Train Epoch: 16 [20/30]\tLambda: 0.5000, Class: 0.132965, CORAL: 0.455526, Total_Loss: 0.360728\n","Train Epoch: 16 [21/30]\tLambda: 0.5000, Class: 0.164564, CORAL: 0.245210, Total_Loss: 0.287169\n","Train Epoch: 16 [22/30]\tLambda: 0.5000, Class: 0.166367, CORAL: 0.222561, Total_Loss: 0.277647\n","Train Epoch: 16 [23/30]\tLambda: 0.5000, Class: 0.142996, CORAL: 0.170523, Total_Loss: 0.228257\n","Train Epoch: 16 [24/30]\tLambda: 0.5000, Class: 0.196794, CORAL: 0.464964, Total_Loss: 0.429276\n","Train Epoch: 16 [25/30]\tLambda: 0.5000, Class: 0.161503, CORAL: 0.321479, Total_Loss: 0.322243\n","Train Epoch: 16 [26/30]\tLambda: 0.5000, Class: 0.155703, CORAL: 0.220883, Total_Loss: 0.266144\n","Train Epoch: 16 [27/30]\tLambda: 0.5000, Class: 0.149224, CORAL: 0.207340, Total_Loss: 0.252894\n","Train Epoch: 16 [28/30]\tLambda: 0.5000, Class: 0.146404, CORAL: 0.355079, Total_Loss: 0.323944\n","Train Epoch: 16 [29/30]\tLambda: 0.5000, Class: 0.130101, CORAL: 0.329523, Total_Loss: 0.294862\n","Train Epoch: 16 [30/30]\tLambda: 0.5000, Class: 0.155031, CORAL: 0.399840, Total_Loss: 0.354951\n","\n","Test Epoch: 16 Average loss: 1.1459, Accuracy: 6742/10000 (67.42%)\n","\n","Test Epoch: 16, Accuracy: 67.42%\n","Train Epoch: 17 [ 1/30]\tLambda: 0.5000, Class: 0.155628, CORAL: 0.318102, Total_Loss: 0.314679\n","Train Epoch: 17 [ 2/30]\tLambda: 0.5000, Class: 0.164297, CORAL: 0.270954, Total_Loss: 0.299774\n","Train Epoch: 17 [ 3/30]\tLambda: 0.5000, Class: 0.165722, CORAL: 0.389591, Total_Loss: 0.360517\n","Train Epoch: 17 [ 4/30]\tLambda: 0.5000, Class: 0.195343, CORAL: 0.233827, Total_Loss: 0.312257\n","Train Epoch: 17 [ 5/30]\tLambda: 0.5000, Class: 0.157479, CORAL: 0.464799, Total_Loss: 0.389878\n","Train Epoch: 17 [ 6/30]\tLambda: 0.5000, Class: 0.157667, CORAL: 0.475243, Total_Loss: 0.395289\n","Train Epoch: 17 [ 7/30]\tLambda: 0.5000, Class: 0.160865, CORAL: 0.363420, Total_Loss: 0.342575\n","Train Epoch: 17 [ 8/30]\tLambda: 0.5000, Class: 0.191222, CORAL: 0.310908, Total_Loss: 0.346676\n","Train Epoch: 17 [ 9/30]\tLambda: 0.5000, Class: 0.189292, CORAL: 0.215910, Total_Loss: 0.297247\n","Train Epoch: 17 [10/30]\tLambda: 0.5000, Class: 0.188221, CORAL: 0.334742, Total_Loss: 0.355592\n","Train Epoch: 17 [11/30]\tLambda: 0.5000, Class: 0.198912, CORAL: 0.243104, Total_Loss: 0.320464\n","Train Epoch: 17 [12/30]\tLambda: 0.5000, Class: 0.201220, CORAL: 0.166002, Total_Loss: 0.284222\n","Train Epoch: 17 [13/30]\tLambda: 0.5000, Class: 0.258378, CORAL: 0.215246, Total_Loss: 0.366001\n","Train Epoch: 17 [14/30]\tLambda: 0.5000, Class: 0.187306, CORAL: 0.252542, Total_Loss: 0.313577\n","Train Epoch: 17 [15/30]\tLambda: 0.5000, Class: 0.157314, CORAL: 0.313497, Total_Loss: 0.314063\n","Train Epoch: 17 [16/30]\tLambda: 0.5000, Class: 0.195467, CORAL: 0.266460, Total_Loss: 0.328697\n","Train Epoch: 17 [17/30]\tLambda: 0.5000, Class: 0.196853, CORAL: 0.348293, Total_Loss: 0.370999\n","Train Epoch: 17 [18/30]\tLambda: 0.5000, Class: 0.208445, CORAL: 0.163736, Total_Loss: 0.290313\n","Train Epoch: 17 [19/30]\tLambda: 0.5000, Class: 0.260062, CORAL: 0.290055, Total_Loss: 0.405089\n","Train Epoch: 17 [20/30]\tLambda: 0.5000, Class: 0.212175, CORAL: 0.261286, Total_Loss: 0.342818\n","Train Epoch: 17 [21/30]\tLambda: 0.5000, Class: 0.220669, CORAL: 0.152472, Total_Loss: 0.296905\n","Train Epoch: 17 [22/30]\tLambda: 0.5000, Class: 0.192037, CORAL: 0.110518, Total_Loss: 0.247296\n","Train Epoch: 17 [23/30]\tLambda: 0.5000, Class: 0.186362, CORAL: 0.174702, Total_Loss: 0.273713\n","Train Epoch: 17 [24/30]\tLambda: 0.5000, Class: 0.166556, CORAL: 0.558839, Total_Loss: 0.445975\n","Train Epoch: 17 [25/30]\tLambda: 0.5000, Class: 0.182961, CORAL: 0.323470, Total_Loss: 0.344696\n","Train Epoch: 17 [26/30]\tLambda: 0.5000, Class: 0.160082, CORAL: 0.440449, Total_Loss: 0.380306\n","Train Epoch: 17 [27/30]\tLambda: 0.5000, Class: 0.190991, CORAL: 0.346349, Total_Loss: 0.364166\n","Train Epoch: 17 [28/30]\tLambda: 0.5000, Class: 0.183855, CORAL: 0.378011, Total_Loss: 0.372861\n","Train Epoch: 17 [29/30]\tLambda: 0.5000, Class: 0.207272, CORAL: 0.298984, Total_Loss: 0.356764\n","Train Epoch: 17 [30/30]\tLambda: 0.5000, Class: 0.163172, CORAL: 0.160123, Total_Loss: 0.243234\n","\n","Test Epoch: 17 Average loss: 1.1300, Accuracy: 6790/10000 (67.90%)\n","\n","Test Epoch: 17, Accuracy: 67.90%\n","Train Epoch: 18 [ 1/30]\tLambda: 0.5000, Class: 0.153731, CORAL: 0.207636, Total_Loss: 0.257549\n","Train Epoch: 18 [ 2/30]\tLambda: 0.5000, Class: 0.198346, CORAL: 0.267099, Total_Loss: 0.331895\n","Train Epoch: 18 [ 3/30]\tLambda: 0.5000, Class: 0.166764, CORAL: 0.251289, Total_Loss: 0.292408\n","Train Epoch: 18 [ 4/30]\tLambda: 0.5000, Class: 0.179624, CORAL: 0.284701, Total_Loss: 0.321975\n","Train Epoch: 18 [ 5/30]\tLambda: 0.5000, Class: 0.153981, CORAL: 0.444727, Total_Loss: 0.376344\n","Train Epoch: 18 [ 6/30]\tLambda: 0.5000, Class: 0.137080, CORAL: 0.391227, Total_Loss: 0.332693\n","Train Epoch: 18 [ 7/30]\tLambda: 0.5000, Class: 0.142768, CORAL: 0.296939, Total_Loss: 0.291238\n","Train Epoch: 18 [ 8/30]\tLambda: 0.5000, Class: 0.141439, CORAL: 0.302546, Total_Loss: 0.292712\n","Train Epoch: 18 [ 9/30]\tLambda: 0.5000, Class: 0.172254, CORAL: 0.359615, Total_Loss: 0.352062\n","Train Epoch: 18 [10/30]\tLambda: 0.5000, Class: 0.163722, CORAL: 0.196716, Total_Loss: 0.262079\n","Train Epoch: 18 [11/30]\tLambda: 0.5000, Class: 0.148437, CORAL: 0.284207, Total_Loss: 0.290541\n","Train Epoch: 18 [12/30]\tLambda: 0.5000, Class: 0.191246, CORAL: 0.301230, Total_Loss: 0.341861\n","Train Epoch: 18 [13/30]\tLambda: 0.5000, Class: 0.169071, CORAL: 0.245549, Total_Loss: 0.291845\n","Train Epoch: 18 [14/30]\tLambda: 0.5000, Class: 0.199534, CORAL: 0.224124, Total_Loss: 0.311597\n","Train Epoch: 18 [15/30]\tLambda: 0.5000, Class: 0.163827, CORAL: 0.337365, Total_Loss: 0.332510\n","Train Epoch: 18 [16/30]\tLambda: 0.5000, Class: 0.186765, CORAL: 0.188706, Total_Loss: 0.281118\n","Train Epoch: 18 [17/30]\tLambda: 0.5000, Class: 0.168116, CORAL: 0.236504, Total_Loss: 0.286369\n","Train Epoch: 18 [18/30]\tLambda: 0.5000, Class: 0.192430, CORAL: 0.209866, Total_Loss: 0.297363\n","Train Epoch: 18 [19/30]\tLambda: 0.5000, Class: 0.166588, CORAL: 0.324815, Total_Loss: 0.328996\n","Train Epoch: 18 [20/30]\tLambda: 0.5000, Class: 0.222642, CORAL: 0.259415, Total_Loss: 0.352349\n","Train Epoch: 18 [21/30]\tLambda: 0.5000, Class: 0.165244, CORAL: 0.388909, Total_Loss: 0.359699\n","Train Epoch: 18 [22/30]\tLambda: 0.5000, Class: 0.178723, CORAL: 0.130351, Total_Loss: 0.243899\n","Train Epoch: 18 [23/30]\tLambda: 0.5000, Class: 0.159354, CORAL: 0.163952, Total_Loss: 0.241330\n","Train Epoch: 18 [24/30]\tLambda: 0.5000, Class: 0.193278, CORAL: 0.374002, Total_Loss: 0.380279\n","Train Epoch: 18 [25/30]\tLambda: 0.5000, Class: 0.199489, CORAL: 0.354723, Total_Loss: 0.376850\n","Train Epoch: 18 [26/30]\tLambda: 0.5000, Class: 0.182618, CORAL: 0.236556, Total_Loss: 0.300896\n","Train Epoch: 18 [27/30]\tLambda: 0.5000, Class: 0.161094, CORAL: 0.248864, Total_Loss: 0.285527\n","Train Epoch: 18 [28/30]\tLambda: 0.5000, Class: 0.178853, CORAL: 0.350045, Total_Loss: 0.353876\n","Train Epoch: 18 [29/30]\tLambda: 0.5000, Class: 0.200953, CORAL: 0.260956, Total_Loss: 0.331431\n","Train Epoch: 18 [30/30]\tLambda: 0.5000, Class: 0.168920, CORAL: 0.289788, Total_Loss: 0.313814\n","\n","Test Epoch: 18 Average loss: 1.1566, Accuracy: 6707/10000 (67.07%)\n","\n","Test Epoch: 18, Accuracy: 67.07%\n","Train Epoch: 19 [ 1/30]\tLambda: 0.5000, Class: 0.159761, CORAL: 0.401126, Total_Loss: 0.360324\n","Train Epoch: 19 [ 2/30]\tLambda: 0.5000, Class: 0.168942, CORAL: 0.241726, Total_Loss: 0.289805\n","Train Epoch: 19 [ 3/30]\tLambda: 0.5000, Class: 0.188034, CORAL: 0.198140, Total_Loss: 0.287104\n","Train Epoch: 19 [ 4/30]\tLambda: 0.5000, Class: 0.152871, CORAL: 0.304246, Total_Loss: 0.304994\n","Train Epoch: 19 [ 5/30]\tLambda: 0.5000, Class: 0.137014, CORAL: 0.157419, Total_Loss: 0.215723\n","Train Epoch: 19 [ 6/30]\tLambda: 0.5000, Class: 0.176982, CORAL: 0.535984, Total_Loss: 0.444974\n","Train Epoch: 19 [ 7/30]\tLambda: 0.5000, Class: 0.173554, CORAL: 0.218060, Total_Loss: 0.282584\n","Train Epoch: 19 [ 8/30]\tLambda: 0.5000, Class: 0.160889, CORAL: 0.247900, Total_Loss: 0.284839\n","Train Epoch: 19 [ 9/30]\tLambda: 0.5000, Class: 0.191743, CORAL: 0.112909, Total_Loss: 0.248198\n","Train Epoch: 19 [10/30]\tLambda: 0.5000, Class: 0.167242, CORAL: 0.242085, Total_Loss: 0.288284\n","Train Epoch: 19 [11/30]\tLambda: 0.5000, Class: 0.150842, CORAL: 0.310587, Total_Loss: 0.306135\n","Train Epoch: 19 [12/30]\tLambda: 0.5000, Class: 0.195575, CORAL: 0.147879, Total_Loss: 0.269515\n","Train Epoch: 19 [13/30]\tLambda: 0.5000, Class: 0.154713, CORAL: 0.332542, Total_Loss: 0.320984\n","Train Epoch: 19 [14/30]\tLambda: 0.5000, Class: 0.158917, CORAL: 0.173060, Total_Loss: 0.245447\n","Train Epoch: 19 [15/30]\tLambda: 0.5000, Class: 0.141835, CORAL: 0.335595, Total_Loss: 0.309632\n","Train Epoch: 19 [16/30]\tLambda: 0.5000, Class: 0.150539, CORAL: 0.215125, Total_Loss: 0.258101\n","Train Epoch: 19 [17/30]\tLambda: 0.5000, Class: 0.152641, CORAL: 0.345662, Total_Loss: 0.325472\n","Train Epoch: 19 [18/30]\tLambda: 0.5000, Class: 0.166307, CORAL: 0.190548, Total_Loss: 0.261581\n","Train Epoch: 19 [19/30]\tLambda: 0.5000, Class: 0.140731, CORAL: 0.378296, Total_Loss: 0.329879\n","Train Epoch: 19 [20/30]\tLambda: 0.5000, Class: 0.158809, CORAL: 0.453110, Total_Loss: 0.385364\n","Train Epoch: 19 [21/30]\tLambda: 0.5000, Class: 0.165872, CORAL: 0.200964, Total_Loss: 0.266354\n","Train Epoch: 19 [22/30]\tLambda: 0.5000, Class: 0.171937, CORAL: 0.334554, Total_Loss: 0.339214\n","Train Epoch: 19 [23/30]\tLambda: 0.5000, Class: 0.160817, CORAL: 0.236458, Total_Loss: 0.279046\n","Train Epoch: 19 [24/30]\tLambda: 0.5000, Class: 0.167994, CORAL: 0.268790, Total_Loss: 0.302389\n","Train Epoch: 19 [25/30]\tLambda: 0.5000, Class: 0.139460, CORAL: 0.472961, Total_Loss: 0.375940\n","Train Epoch: 19 [26/30]\tLambda: 0.5000, Class: 0.141627, CORAL: 0.236384, Total_Loss: 0.259819\n","Train Epoch: 19 [27/30]\tLambda: 0.5000, Class: 0.131509, CORAL: 0.327150, Total_Loss: 0.295084\n","Train Epoch: 19 [28/30]\tLambda: 0.5000, Class: 0.170822, CORAL: 0.250786, Total_Loss: 0.296215\n","Train Epoch: 19 [29/30]\tLambda: 0.5000, Class: 0.186316, CORAL: 0.101968, Total_Loss: 0.237300\n","Train Epoch: 19 [30/30]\tLambda: 0.5000, Class: 0.154679, CORAL: 0.227584, Total_Loss: 0.268471\n","\n","Test Epoch: 19 Average loss: 1.1736, Accuracy: 6660/10000 (66.60%)\n","\n","Test Epoch: 19, Accuracy: 66.60%\n","Train Epoch: 20 [ 1/30]\tLambda: 0.5000, Class: 0.136352, CORAL: 0.145960, Total_Loss: 0.209332\n","Train Epoch: 20 [ 2/30]\tLambda: 0.5000, Class: 0.155479, CORAL: 0.378031, Total_Loss: 0.344495\n","Train Epoch: 20 [ 3/30]\tLambda: 0.5000, Class: 0.183089, CORAL: 0.164029, Total_Loss: 0.265104\n","Train Epoch: 20 [ 4/30]\tLambda: 0.5000, Class: 0.150603, CORAL: 0.194694, Total_Loss: 0.247950\n","Train Epoch: 20 [ 5/30]\tLambda: 0.5000, Class: 0.180042, CORAL: 0.385902, Total_Loss: 0.372993\n","Train Epoch: 20 [ 6/30]\tLambda: 0.5000, Class: 0.160115, CORAL: 0.193117, Total_Loss: 0.256674\n","Train Epoch: 20 [ 7/30]\tLambda: 0.5000, Class: 0.141280, CORAL: 0.172212, Total_Loss: 0.227386\n","Train Epoch: 20 [ 8/30]\tLambda: 0.5000, Class: 0.155697, CORAL: 0.212906, Total_Loss: 0.262149\n","Train Epoch: 20 [ 9/30]\tLambda: 0.5000, Class: 0.163938, CORAL: 0.229794, Total_Loss: 0.278835\n","Train Epoch: 20 [10/30]\tLambda: 0.5000, Class: 0.147962, CORAL: 0.225743, Total_Loss: 0.260833\n","Train Epoch: 20 [11/30]\tLambda: 0.5000, Class: 0.147544, CORAL: 0.486783, Total_Loss: 0.390935\n","Train Epoch: 20 [12/30]\tLambda: 0.5000, Class: 0.146928, CORAL: 0.236607, Total_Loss: 0.265232\n","Train Epoch: 20 [13/30]\tLambda: 0.5000, Class: 0.141081, CORAL: 0.292605, Total_Loss: 0.287384\n","Train Epoch: 20 [14/30]\tLambda: 0.5000, Class: 0.124993, CORAL: 0.283130, Total_Loss: 0.266558\n","Train Epoch: 20 [15/30]\tLambda: 0.5000, Class: 0.152958, CORAL: 0.222979, Total_Loss: 0.264448\n","Train Epoch: 20 [16/30]\tLambda: 0.5000, Class: 0.146930, CORAL: 0.271456, Total_Loss: 0.282658\n","Train Epoch: 20 [17/30]\tLambda: 0.5000, Class: 0.165811, CORAL: 0.235335, Total_Loss: 0.283479\n","Train Epoch: 20 [18/30]\tLambda: 0.5000, Class: 0.151107, CORAL: 0.456989, Total_Loss: 0.379602\n","Train Epoch: 20 [19/30]\tLambda: 0.5000, Class: 0.156821, CORAL: 0.273884, Total_Loss: 0.293763\n","Train Epoch: 20 [20/30]\tLambda: 0.5000, Class: 0.151262, CORAL: 0.311642, Total_Loss: 0.307083\n","Train Epoch: 20 [21/30]\tLambda: 0.5000, Class: 0.136717, CORAL: 0.230360, Total_Loss: 0.251897\n","Train Epoch: 20 [22/30]\tLambda: 0.5000, Class: 0.158987, CORAL: 0.236544, Total_Loss: 0.277259\n","Train Epoch: 20 [23/30]\tLambda: 0.5000, Class: 0.155936, CORAL: 0.170007, Total_Loss: 0.240939\n","Train Epoch: 20 [24/30]\tLambda: 0.5000, Class: 0.157464, CORAL: 0.218974, Total_Loss: 0.266950\n","Train Epoch: 20 [25/30]\tLambda: 0.5000, Class: 0.173457, CORAL: 0.296888, Total_Loss: 0.321901\n","Train Epoch: 20 [26/30]\tLambda: 0.5000, Class: 0.165419, CORAL: 0.177560, Total_Loss: 0.254199\n","Train Epoch: 20 [27/30]\tLambda: 0.5000, Class: 0.132274, CORAL: 0.410282, Total_Loss: 0.337415\n","Train Epoch: 20 [28/30]\tLambda: 0.5000, Class: 0.156041, CORAL: 0.293994, Total_Loss: 0.303038\n","Train Epoch: 20 [29/30]\tLambda: 0.5000, Class: 0.183196, CORAL: 0.392801, Total_Loss: 0.379597\n","Train Epoch: 20 [30/30]\tLambda: 0.5000, Class: 0.145924, CORAL: 0.255586, Total_Loss: 0.273717\n","\n","Test Epoch: 20 Average loss: 1.1988, Accuracy: 6630/10000 (66.30%)\n","\n","Test Epoch: 20, Accuracy: 66.30%\n","Train Epoch: 21 [ 1/30]\tLambda: 0.5000, Class: 0.160441, CORAL: 0.245052, Total_Loss: 0.282967\n","Train Epoch: 21 [ 2/30]\tLambda: 0.5000, Class: 0.140319, CORAL: 0.274591, Total_Loss: 0.277614\n","Train Epoch: 21 [ 3/30]\tLambda: 0.5000, Class: 0.154351, CORAL: 0.292546, Total_Loss: 0.300624\n","Train Epoch: 21 [ 4/30]\tLambda: 0.5000, Class: 0.165033, CORAL: 0.254543, Total_Loss: 0.292305\n","Train Epoch: 21 [ 5/30]\tLambda: 0.5000, Class: 0.147537, CORAL: 0.274524, Total_Loss: 0.284799\n","Train Epoch: 21 [ 6/30]\tLambda: 0.5000, Class: 0.140769, CORAL: 0.226779, Total_Loss: 0.254158\n","Train Epoch: 21 [ 7/30]\tLambda: 0.5000, Class: 0.158833, CORAL: 0.264859, Total_Loss: 0.291263\n","Train Epoch: 21 [ 8/30]\tLambda: 0.5000, Class: 0.150031, CORAL: 0.327029, Total_Loss: 0.313546\n","Train Epoch: 21 [ 9/30]\tLambda: 0.5000, Class: 0.179635, CORAL: 0.331755, Total_Loss: 0.345512\n","Train Epoch: 21 [10/30]\tLambda: 0.5000, Class: 0.161190, CORAL: 0.201511, Total_Loss: 0.261945\n","Train Epoch: 21 [11/30]\tLambda: 0.5000, Class: 0.176665, CORAL: 0.221285, Total_Loss: 0.287307\n","Train Epoch: 21 [12/30]\tLambda: 0.5000, Class: 0.172474, CORAL: 0.229895, Total_Loss: 0.287422\n","Train Epoch: 21 [13/30]\tLambda: 0.5000, Class: 0.172367, CORAL: 0.314916, Total_Loss: 0.329825\n","Train Epoch: 21 [14/30]\tLambda: 0.5000, Class: 0.171409, CORAL: 0.216904, Total_Loss: 0.279861\n","Train Epoch: 21 [15/30]\tLambda: 0.5000, Class: 0.157536, CORAL: 0.123665, Total_Loss: 0.219369\n","Train Epoch: 21 [16/30]\tLambda: 0.5000, Class: 0.163279, CORAL: 0.378701, Total_Loss: 0.352629\n","Train Epoch: 21 [17/30]\tLambda: 0.5000, Class: 0.167310, CORAL: 0.260342, Total_Loss: 0.297481\n","Train Epoch: 21 [18/30]\tLambda: 0.5000, Class: 0.153506, CORAL: 0.212277, Total_Loss: 0.259645\n","Train Epoch: 21 [19/30]\tLambda: 0.5000, Class: 0.149202, CORAL: 0.209294, Total_Loss: 0.253848\n","Train Epoch: 21 [20/30]\tLambda: 0.5000, Class: 0.156554, CORAL: 0.585436, Total_Loss: 0.449272\n","Train Epoch: 21 [21/30]\tLambda: 0.5000, Class: 0.126772, CORAL: 0.231827, Total_Loss: 0.242686\n","Train Epoch: 21 [22/30]\tLambda: 0.5000, Class: 0.168497, CORAL: 0.183348, Total_Loss: 0.260170\n","Train Epoch: 21 [23/30]\tLambda: 0.5000, Class: 0.154358, CORAL: 0.285031, Total_Loss: 0.296874\n","Train Epoch: 21 [24/30]\tLambda: 0.5000, Class: 0.166261, CORAL: 0.446107, Total_Loss: 0.389314\n","Train Epoch: 21 [25/30]\tLambda: 0.5000, Class: 0.149825, CORAL: 0.322346, Total_Loss: 0.310998\n","Train Epoch: 21 [26/30]\tLambda: 0.5000, Class: 0.159800, CORAL: 0.142657, Total_Loss: 0.231129\n","Train Epoch: 21 [27/30]\tLambda: 0.5000, Class: 0.161668, CORAL: 0.280001, Total_Loss: 0.301668\n","Train Epoch: 21 [28/30]\tLambda: 0.5000, Class: 0.150242, CORAL: 0.317368, Total_Loss: 0.308926\n","Train Epoch: 21 [29/30]\tLambda: 0.5000, Class: 0.163865, CORAL: 0.234466, Total_Loss: 0.281098\n","Train Epoch: 21 [30/30]\tLambda: 0.5000, Class: 0.176594, CORAL: 0.376259, Total_Loss: 0.364724\n","\n","Test Epoch: 21 Average loss: 1.1803, Accuracy: 6707/10000 (67.07%)\n","\n","Test Epoch: 21, Accuracy: 67.07%\n","Train Epoch: 22 [ 1/30]\tLambda: 0.5000, Class: 0.150272, CORAL: 0.288021, Total_Loss: 0.294282\n","Train Epoch: 22 [ 2/30]\tLambda: 0.5000, Class: 0.145198, CORAL: 0.199214, Total_Loss: 0.244805\n","Train Epoch: 22 [ 3/30]\tLambda: 0.5000, Class: 0.176094, CORAL: 0.102420, Total_Loss: 0.227304\n","Train Epoch: 22 [ 4/30]\tLambda: 0.5000, Class: 0.146006, CORAL: 0.439570, Total_Loss: 0.365791\n","Train Epoch: 22 [ 5/30]\tLambda: 0.5000, Class: 0.159768, CORAL: 0.298405, Total_Loss: 0.308970\n","Train Epoch: 22 [ 6/30]\tLambda: 0.5000, Class: 0.160311, CORAL: 0.213509, Total_Loss: 0.267066\n","Train Epoch: 22 [ 7/30]\tLambda: 0.5000, Class: 0.154076, CORAL: 0.181770, Total_Loss: 0.244960\n","Train Epoch: 22 [ 8/30]\tLambda: 0.5000, Class: 0.157238, CORAL: 0.467881, Total_Loss: 0.391179\n","Train Epoch: 22 [ 9/30]\tLambda: 0.5000, Class: 0.151409, CORAL: 0.201592, Total_Loss: 0.252205\n","Train Epoch: 22 [10/30]\tLambda: 0.5000, Class: 0.173264, CORAL: 0.205822, Total_Loss: 0.276175\n","Train Epoch: 22 [11/30]\tLambda: 0.5000, Class: 0.176868, CORAL: 0.159406, Total_Loss: 0.256571\n","Train Epoch: 22 [12/30]\tLambda: 0.5000, Class: 0.156919, CORAL: 0.196566, Total_Loss: 0.255202\n","Train Epoch: 22 [13/30]\tLambda: 0.5000, Class: 0.140483, CORAL: 0.341548, Total_Loss: 0.311257\n","Train Epoch: 22 [14/30]\tLambda: 0.5000, Class: 0.188001, CORAL: 0.364458, Total_Loss: 0.370230\n","Train Epoch: 22 [15/30]\tLambda: 0.5000, Class: 0.154547, CORAL: 0.209739, Total_Loss: 0.259417\n","Train Epoch: 22 [16/30]\tLambda: 0.5000, Class: 0.151120, CORAL: 0.181396, Total_Loss: 0.241818\n","Train Epoch: 22 [17/30]\tLambda: 0.5000, Class: 0.188889, CORAL: 0.118960, Total_Loss: 0.248369\n","Train Epoch: 22 [18/30]\tLambda: 0.5000, Class: 0.154144, CORAL: 0.245153, Total_Loss: 0.276721\n","Train Epoch: 22 [19/30]\tLambda: 0.5000, Class: 0.161757, CORAL: 0.264087, Total_Loss: 0.293800\n","Train Epoch: 22 [20/30]\tLambda: 0.5000, Class: 0.168753, CORAL: 0.567819, Total_Loss: 0.452662\n","Train Epoch: 22 [21/30]\tLambda: 0.5000, Class: 0.137348, CORAL: 0.258570, Total_Loss: 0.266633\n","Train Epoch: 22 [22/30]\tLambda: 0.5000, Class: 0.157357, CORAL: 0.354455, Total_Loss: 0.334585\n","Train Epoch: 22 [23/30]\tLambda: 0.5000, Class: 0.158755, CORAL: 0.294535, Total_Loss: 0.306023\n","Train Epoch: 22 [24/30]\tLambda: 0.5000, Class: 0.186365, CORAL: 0.314924, Total_Loss: 0.343827\n","Train Epoch: 22 [25/30]\tLambda: 0.5000, Class: 0.158750, CORAL: 0.567455, Total_Loss: 0.442478\n","Train Epoch: 22 [26/30]\tLambda: 0.5000, Class: 0.157174, CORAL: 0.291615, Total_Loss: 0.302981\n","Train Epoch: 22 [27/30]\tLambda: 0.5000, Class: 0.153469, CORAL: 0.362677, Total_Loss: 0.334807\n","Train Epoch: 22 [28/30]\tLambda: 0.5000, Class: 0.160705, CORAL: 0.226244, Total_Loss: 0.273827\n","Train Epoch: 22 [29/30]\tLambda: 0.5000, Class: 0.178261, CORAL: 0.249419, Total_Loss: 0.302970\n","Train Epoch: 22 [30/30]\tLambda: 0.5000, Class: 0.171744, CORAL: 0.204858, Total_Loss: 0.274173\n","\n","Test Epoch: 22 Average loss: 1.1718, Accuracy: 6718/10000 (67.18%)\n","\n","Test Epoch: 22, Accuracy: 67.18%\n","Train Epoch: 23 [ 1/30]\tLambda: 0.5000, Class: 0.161625, CORAL: 0.208499, Total_Loss: 0.265875\n","Train Epoch: 23 [ 2/30]\tLambda: 0.5000, Class: 0.179691, CORAL: 0.201344, Total_Loss: 0.280363\n","Train Epoch: 23 [ 3/30]\tLambda: 0.5000, Class: 0.164319, CORAL: 0.294717, Total_Loss: 0.311677\n","Train Epoch: 23 [ 4/30]\tLambda: 0.5000, Class: 0.166030, CORAL: 0.211122, Total_Loss: 0.271590\n","Train Epoch: 23 [ 5/30]\tLambda: 0.5000, Class: 0.151486, CORAL: 0.188684, Total_Loss: 0.245828\n","Train Epoch: 23 [ 6/30]\tLambda: 0.5000, Class: 0.177881, CORAL: 0.271241, Total_Loss: 0.313502\n","Train Epoch: 23 [ 7/30]\tLambda: 0.5000, Class: 0.216192, CORAL: 0.213784, Total_Loss: 0.323084\n","Train Epoch: 23 [ 8/30]\tLambda: 0.5000, Class: 0.147062, CORAL: 0.214713, Total_Loss: 0.254419\n","Train Epoch: 23 [ 9/30]\tLambda: 0.5000, Class: 0.151561, CORAL: 0.210888, Total_Loss: 0.257005\n","Train Epoch: 23 [10/30]\tLambda: 0.5000, Class: 0.157573, CORAL: 0.225576, Total_Loss: 0.270361\n","Train Epoch: 23 [11/30]\tLambda: 0.5000, Class: 0.153155, CORAL: 0.240598, Total_Loss: 0.273454\n","Train Epoch: 23 [12/30]\tLambda: 0.5000, Class: 0.149893, CORAL: 0.181476, Total_Loss: 0.240631\n","Train Epoch: 23 [13/30]\tLambda: 0.5000, Class: 0.132545, CORAL: 0.453402, Total_Loss: 0.359246\n","Train Epoch: 23 [14/30]\tLambda: 0.5000, Class: 0.141613, CORAL: 0.177860, Total_Loss: 0.230543\n","Train Epoch: 23 [15/30]\tLambda: 0.5000, Class: 0.122561, CORAL: 0.457421, Total_Loss: 0.351271\n","Train Epoch: 23 [16/30]\tLambda: 0.5000, Class: 0.176084, CORAL: 0.124023, Total_Loss: 0.238096\n","Train Epoch: 23 [17/30]\tLambda: 0.5000, Class: 0.138780, CORAL: 0.227219, Total_Loss: 0.252390\n","Train Epoch: 23 [18/30]\tLambda: 0.5000, Class: 0.143509, CORAL: 0.328858, Total_Loss: 0.307938\n","Train Epoch: 23 [19/30]\tLambda: 0.5000, Class: 0.141084, CORAL: 0.457945, Total_Loss: 0.370056\n","Train Epoch: 23 [20/30]\tLambda: 0.5000, Class: 0.164161, CORAL: 0.165429, Total_Loss: 0.246875\n","Train Epoch: 23 [21/30]\tLambda: 0.5000, Class: 0.151598, CORAL: 0.212843, Total_Loss: 0.258019\n","Train Epoch: 23 [22/30]\tLambda: 0.5000, Class: 0.151946, CORAL: 0.237240, Total_Loss: 0.270566\n","Train Epoch: 23 [23/30]\tLambda: 0.5000, Class: 0.161588, CORAL: 0.221248, Total_Loss: 0.272212\n","Train Epoch: 23 [24/30]\tLambda: 0.5000, Class: 0.150973, CORAL: 0.709058, Total_Loss: 0.505502\n","Train Epoch: 23 [25/30]\tLambda: 0.5000, Class: 0.142523, CORAL: 0.387525, Total_Loss: 0.336286\n","Train Epoch: 23 [26/30]\tLambda: 0.5000, Class: 0.141929, CORAL: 0.268885, Total_Loss: 0.276372\n","Train Epoch: 23 [27/30]\tLambda: 0.5000, Class: 0.162095, CORAL: 0.333150, Total_Loss: 0.328670\n","Train Epoch: 23 [28/30]\tLambda: 0.5000, Class: 0.159150, CORAL: 0.376959, Total_Loss: 0.347630\n","Train Epoch: 23 [29/30]\tLambda: 0.5000, Class: 0.165966, CORAL: 0.318497, Total_Loss: 0.325214\n","Train Epoch: 23 [30/30]\tLambda: 0.5000, Class: 0.187756, CORAL: 0.147606, Total_Loss: 0.261559\n","\n","Test Epoch: 23 Average loss: 1.1503, Accuracy: 6812/10000 (68.12%)\n","\n","Test Epoch: 23, Accuracy: 68.12%\n","New best model saved with accuracy: 68.12%\n","Train Epoch: 24 [ 1/30]\tLambda: 0.5000, Class: 0.165829, CORAL: 0.130718, Total_Loss: 0.231188\n","Train Epoch: 24 [ 2/30]\tLambda: 0.5000, Class: 0.209716, CORAL: 0.274060, Total_Loss: 0.346746\n","Train Epoch: 24 [ 3/30]\tLambda: 0.5000, Class: 0.190376, CORAL: 0.191603, Total_Loss: 0.286177\n","Train Epoch: 24 [ 4/30]\tLambda: 0.5000, Class: 0.165377, CORAL: 0.356638, Total_Loss: 0.343696\n","Train Epoch: 24 [ 5/30]\tLambda: 0.5000, Class: 0.187207, CORAL: 0.204995, Total_Loss: 0.289704\n","Train Epoch: 24 [ 6/30]\tLambda: 0.5000, Class: 0.184673, CORAL: 0.130964, Total_Loss: 0.250155\n","Train Epoch: 24 [ 7/30]\tLambda: 0.5000, Class: 0.157769, CORAL: 0.194305, Total_Loss: 0.254921\n","Train Epoch: 24 [ 8/30]\tLambda: 0.5000, Class: 0.180419, CORAL: 0.175812, Total_Loss: 0.268325\n","Train Epoch: 24 [ 9/30]\tLambda: 0.5000, Class: 0.160546, CORAL: 0.176480, Total_Loss: 0.248786\n","Train Epoch: 24 [10/30]\tLambda: 0.5000, Class: 0.180839, CORAL: 0.477976, Total_Loss: 0.419826\n","Train Epoch: 24 [11/30]\tLambda: 0.5000, Class: 0.150744, CORAL: 0.194362, Total_Loss: 0.247925\n","Train Epoch: 24 [12/30]\tLambda: 0.5000, Class: 0.147375, CORAL: 0.206809, Total_Loss: 0.250779\n","Train Epoch: 24 [13/30]\tLambda: 0.5000, Class: 0.162579, CORAL: 0.092752, Total_Loss: 0.208955\n","Train Epoch: 24 [14/30]\tLambda: 0.5000, Class: 0.165468, CORAL: 0.105889, Total_Loss: 0.218412\n","Train Epoch: 24 [15/30]\tLambda: 0.5000, Class: 0.136893, CORAL: 0.315962, Total_Loss: 0.294874\n","Train Epoch: 24 [16/30]\tLambda: 0.5000, Class: 0.138174, CORAL: 0.313647, Total_Loss: 0.294997\n","Train Epoch: 24 [17/30]\tLambda: 0.5000, Class: 0.166677, CORAL: 0.314856, Total_Loss: 0.324105\n","Train Epoch: 24 [18/30]\tLambda: 0.5000, Class: 0.138041, CORAL: 0.269483, Total_Loss: 0.272782\n","Train Epoch: 24 [19/30]\tLambda: 0.5000, Class: 0.166018, CORAL: 0.509366, Total_Loss: 0.420701\n","Train Epoch: 24 [20/30]\tLambda: 0.5000, Class: 0.145209, CORAL: 0.194714, Total_Loss: 0.242566\n","Train Epoch: 24 [21/30]\tLambda: 0.5000, Class: 0.151843, CORAL: 0.267044, Total_Loss: 0.285365\n","Train Epoch: 24 [22/30]\tLambda: 0.5000, Class: 0.144414, CORAL: 0.287312, Total_Loss: 0.288070\n","Train Epoch: 24 [23/30]\tLambda: 0.5000, Class: 0.145032, CORAL: 0.241557, Total_Loss: 0.265811\n","Train Epoch: 24 [24/30]\tLambda: 0.5000, Class: 0.152411, CORAL: 0.242165, Total_Loss: 0.273494\n","Train Epoch: 24 [25/30]\tLambda: 0.5000, Class: 0.137570, CORAL: 0.353070, Total_Loss: 0.314105\n","Train Epoch: 24 [26/30]\tLambda: 0.5000, Class: 0.121029, CORAL: 0.495482, Total_Loss: 0.368770\n","Train Epoch: 24 [27/30]\tLambda: 0.5000, Class: 0.128683, CORAL: 0.259295, Total_Loss: 0.258330\n","Train Epoch: 24 [28/30]\tLambda: 0.5000, Class: 0.162704, CORAL: 0.518147, Total_Loss: 0.421777\n","Train Epoch: 24 [29/30]\tLambda: 0.5000, Class: 0.150424, CORAL: 0.245369, Total_Loss: 0.273108\n","Train Epoch: 24 [30/30]\tLambda: 0.5000, Class: 0.138516, CORAL: 0.413278, Total_Loss: 0.345155\n","\n","Test Epoch: 24 Average loss: 1.1647, Accuracy: 6775/10000 (67.75%)\n","\n","Test Epoch: 24, Accuracy: 67.75%\n","Train Epoch: 25 [ 1/30]\tLambda: 0.5000, Class: 0.152675, CORAL: 0.191122, Total_Loss: 0.248236\n","Train Epoch: 25 [ 2/30]\tLambda: 0.5000, Class: 0.161482, CORAL: 0.230017, Total_Loss: 0.276491\n","Train Epoch: 25 [ 3/30]\tLambda: 0.5000, Class: 0.150150, CORAL: 0.197146, Total_Loss: 0.248723\n","Train Epoch: 25 [ 4/30]\tLambda: 0.5000, Class: 0.156382, CORAL: 0.346844, Total_Loss: 0.329804\n","Train Epoch: 25 [ 5/30]\tLambda: 0.5000, Class: 0.172826, CORAL: 0.106278, Total_Loss: 0.225965\n","Train Epoch: 25 [ 6/30]\tLambda: 0.5000, Class: 0.159207, CORAL: 0.310009, Total_Loss: 0.314211\n","Train Epoch: 25 [ 7/30]\tLambda: 0.5000, Class: 0.156017, CORAL: 0.220997, Total_Loss: 0.266515\n","Train Epoch: 25 [ 8/30]\tLambda: 0.5000, Class: 0.169902, CORAL: 0.124142, Total_Loss: 0.231973\n","Train Epoch: 25 [ 9/30]\tLambda: 0.5000, Class: 0.158812, CORAL: 0.194996, Total_Loss: 0.256310\n","Train Epoch: 25 [10/30]\tLambda: 0.5000, Class: 0.159313, CORAL: 0.317221, Total_Loss: 0.317924\n","Train Epoch: 25 [11/30]\tLambda: 0.5000, Class: 0.185961, CORAL: 0.161638, Total_Loss: 0.266780\n","Train Epoch: 25 [12/30]\tLambda: 0.5000, Class: 0.155127, CORAL: 0.219355, Total_Loss: 0.264804\n","Train Epoch: 25 [13/30]\tLambda: 0.5000, Class: 0.162015, CORAL: 0.133069, Total_Loss: 0.228549\n","Train Epoch: 25 [14/30]\tLambda: 0.5000, Class: 0.147735, CORAL: 0.285853, Total_Loss: 0.290661\n","Train Epoch: 25 [15/30]\tLambda: 0.5000, Class: 0.147675, CORAL: 0.217072, Total_Loss: 0.256211\n","Train Epoch: 25 [16/30]\tLambda: 0.5000, Class: 0.145585, CORAL: 0.252144, Total_Loss: 0.271657\n","Train Epoch: 25 [17/30]\tLambda: 0.5000, Class: 0.157655, CORAL: 0.205350, Total_Loss: 0.260330\n","Train Epoch: 25 [18/30]\tLambda: 0.5000, Class: 0.180508, CORAL: 0.311153, Total_Loss: 0.336085\n","Train Epoch: 25 [19/30]\tLambda: 0.5000, Class: 0.181679, CORAL: 0.173155, Total_Loss: 0.268257\n","Train Epoch: 25 [20/30]\tLambda: 0.5000, Class: 0.125263, CORAL: 0.375619, Total_Loss: 0.313073\n","Train Epoch: 25 [21/30]\tLambda: 0.5000, Class: 0.125334, CORAL: 0.228262, Total_Loss: 0.239465\n","Train Epoch: 25 [22/30]\tLambda: 0.5000, Class: 0.162313, CORAL: 0.435213, Total_Loss: 0.379919\n","Train Epoch: 25 [23/30]\tLambda: 0.5000, Class: 0.151524, CORAL: 0.150585, Total_Loss: 0.226817\n","Train Epoch: 25 [24/30]\tLambda: 0.5000, Class: 0.133218, CORAL: 0.206089, Total_Loss: 0.236263\n","Train Epoch: 25 [25/30]\tLambda: 0.5000, Class: 0.165321, CORAL: 0.328235, Total_Loss: 0.329439\n","Train Epoch: 25 [26/30]\tLambda: 0.5000, Class: 0.161051, CORAL: 0.514232, Total_Loss: 0.418167\n","Train Epoch: 25 [27/30]\tLambda: 0.5000, Class: 0.137399, CORAL: 0.197316, Total_Loss: 0.236057\n","Train Epoch: 25 [28/30]\tLambda: 0.5000, Class: 0.141796, CORAL: 0.351510, Total_Loss: 0.317551\n","Train Epoch: 25 [29/30]\tLambda: 0.5000, Class: 0.167076, CORAL: 0.188501, Total_Loss: 0.261326\n","Train Epoch: 25 [30/30]\tLambda: 0.5000, Class: 0.114407, CORAL: 0.294113, Total_Loss: 0.261464\n","\n","Test Epoch: 25 Average loss: 1.1356, Accuracy: 6866/10000 (68.66%)\n","\n","Test Epoch: 25, Accuracy: 68.66%\n","New best model saved with accuracy: 68.66%\n","Train Epoch: 26 [ 1/30]\tLambda: 0.5000, Class: 0.124013, CORAL: 0.475869, Total_Loss: 0.361948\n","Train Epoch: 26 [ 2/30]\tLambda: 0.5000, Class: 0.128965, CORAL: 0.279097, Total_Loss: 0.268513\n","Train Epoch: 26 [ 3/30]\tLambda: 0.5000, Class: 0.158372, CORAL: 0.112534, Total_Loss: 0.214639\n","Train Epoch: 26 [ 4/30]\tLambda: 0.5000, Class: 0.133967, CORAL: 0.292013, Total_Loss: 0.279974\n","Train Epoch: 26 [ 5/30]\tLambda: 0.5000, Class: 0.160750, CORAL: 0.154262, Total_Loss: 0.237881\n","Train Epoch: 26 [ 6/30]\tLambda: 0.5000, Class: 0.162726, CORAL: 0.342800, Total_Loss: 0.334126\n","Train Epoch: 26 [ 7/30]\tLambda: 0.5000, Class: 0.146722, CORAL: 0.392343, Total_Loss: 0.342893\n","Train Epoch: 26 [ 8/30]\tLambda: 0.5000, Class: 0.155095, CORAL: 0.281647, Total_Loss: 0.295919\n","Train Epoch: 26 [ 9/30]\tLambda: 0.5000, Class: 0.134403, CORAL: 0.550464, Total_Loss: 0.409635\n","Train Epoch: 26 [10/30]\tLambda: 0.5000, Class: 0.152364, CORAL: 0.174015, Total_Loss: 0.239372\n","Train Epoch: 26 [11/30]\tLambda: 0.5000, Class: 0.139334, CORAL: 0.129763, Total_Loss: 0.204216\n","Train Epoch: 26 [12/30]\tLambda: 0.5000, Class: 0.160243, CORAL: 0.124864, Total_Loss: 0.222675\n","Train Epoch: 26 [13/30]\tLambda: 0.5000, Class: 0.158553, CORAL: 0.195572, Total_Loss: 0.256339\n","Train Epoch: 26 [14/30]\tLambda: 0.5000, Class: 0.175028, CORAL: 0.192636, Total_Loss: 0.271346\n","Train Epoch: 26 [15/30]\tLambda: 0.5000, Class: 0.158352, CORAL: 0.226002, Total_Loss: 0.271353\n","Train Epoch: 26 [16/30]\tLambda: 0.5000, Class: 0.142642, CORAL: 0.304845, Total_Loss: 0.295065\n","Train Epoch: 26 [17/30]\tLambda: 0.5000, Class: 0.160998, CORAL: 0.191005, Total_Loss: 0.256501\n","Train Epoch: 26 [18/30]\tLambda: 0.5000, Class: 0.131984, CORAL: 0.425114, Total_Loss: 0.344541\n","Train Epoch: 26 [19/30]\tLambda: 0.5000, Class: 0.153287, CORAL: 0.265850, Total_Loss: 0.286212\n","Train Epoch: 26 [20/30]\tLambda: 0.5000, Class: 0.153132, CORAL: 0.317181, Total_Loss: 0.311723\n","Train Epoch: 26 [21/30]\tLambda: 0.5000, Class: 0.166254, CORAL: 0.262016, Total_Loss: 0.297262\n","Train Epoch: 26 [22/30]\tLambda: 0.5000, Class: 0.136879, CORAL: 0.234770, Total_Loss: 0.254264\n","Train Epoch: 26 [23/30]\tLambda: 0.5000, Class: 0.158110, CORAL: 0.225316, Total_Loss: 0.270768\n","Train Epoch: 26 [24/30]\tLambda: 0.5000, Class: 0.159202, CORAL: 0.163509, Total_Loss: 0.240956\n","Train Epoch: 26 [25/30]\tLambda: 0.5000, Class: 0.171205, CORAL: 0.416035, Total_Loss: 0.379223\n","Train Epoch: 26 [26/30]\tLambda: 0.5000, Class: 0.149341, CORAL: 0.280974, Total_Loss: 0.289828\n","Train Epoch: 26 [27/30]\tLambda: 0.5000, Class: 0.132438, CORAL: 0.319861, Total_Loss: 0.292368\n","Train Epoch: 26 [28/30]\tLambda: 0.5000, Class: 0.150795, CORAL: 0.637307, Total_Loss: 0.469448\n","Train Epoch: 26 [29/30]\tLambda: 0.5000, Class: 0.133917, CORAL: 0.422469, Total_Loss: 0.345152\n","Train Epoch: 26 [30/30]\tLambda: 0.5000, Class: 0.156279, CORAL: 0.274275, Total_Loss: 0.293417\n","\n","Test Epoch: 26 Average loss: 1.1393, Accuracy: 6858/10000 (68.58%)\n","\n","Test Epoch: 26, Accuracy: 68.58%\n","Train Epoch: 27 [ 1/30]\tLambda: 0.5000, Class: 0.161531, CORAL: 0.412057, Total_Loss: 0.367560\n","Train Epoch: 27 [ 2/30]\tLambda: 0.5000, Class: 0.183013, CORAL: 0.111281, Total_Loss: 0.238653\n","Train Epoch: 27 [ 3/30]\tLambda: 0.5000, Class: 0.178058, CORAL: 0.200444, Total_Loss: 0.278280\n","Train Epoch: 27 [ 4/30]\tLambda: 0.5000, Class: 0.183096, CORAL: 0.217106, Total_Loss: 0.291649\n","Train Epoch: 27 [ 5/30]\tLambda: 0.5000, Class: 0.167178, CORAL: 0.283054, Total_Loss: 0.308705\n","Train Epoch: 27 [ 6/30]\tLambda: 0.5000, Class: 0.154664, CORAL: 0.118366, Total_Loss: 0.213848\n","Train Epoch: 27 [ 7/30]\tLambda: 0.5000, Class: 0.192255, CORAL: 0.211288, Total_Loss: 0.297899\n","Train Epoch: 27 [ 8/30]\tLambda: 0.5000, Class: 0.172094, CORAL: 0.389688, Total_Loss: 0.366938\n","Train Epoch: 27 [ 9/30]\tLambda: 0.5000, Class: 0.200703, CORAL: 0.135332, Total_Loss: 0.268369\n","Train Epoch: 27 [10/30]\tLambda: 0.5000, Class: 0.165543, CORAL: 0.157145, Total_Loss: 0.244116\n","Train Epoch: 27 [11/30]\tLambda: 0.5000, Class: 0.161246, CORAL: 0.165337, Total_Loss: 0.243915\n","Train Epoch: 27 [12/30]\tLambda: 0.5000, Class: 0.171305, CORAL: 0.136857, Total_Loss: 0.239734\n","Train Epoch: 27 [13/30]\tLambda: 0.5000, Class: 0.155424, CORAL: 0.200275, Total_Loss: 0.255561\n","Train Epoch: 27 [14/30]\tLambda: 0.5000, Class: 0.135920, CORAL: 0.175616, Total_Loss: 0.223727\n","Train Epoch: 27 [15/30]\tLambda: 0.5000, Class: 0.157168, CORAL: 0.107036, Total_Loss: 0.210686\n","Train Epoch: 27 [16/30]\tLambda: 0.5000, Class: 0.151943, CORAL: 0.283865, Total_Loss: 0.293875\n","Train Epoch: 27 [17/30]\tLambda: 0.5000, Class: 0.151762, CORAL: 0.185155, Total_Loss: 0.244339\n","Train Epoch: 27 [18/30]\tLambda: 0.5000, Class: 0.150884, CORAL: 0.125148, Total_Loss: 0.213458\n","Train Epoch: 27 [19/30]\tLambda: 0.5000, Class: 0.117483, CORAL: 0.399149, Total_Loss: 0.317057\n","Train Epoch: 27 [20/30]\tLambda: 0.5000, Class: 0.127155, CORAL: 0.302126, Total_Loss: 0.278218\n","Train Epoch: 27 [21/30]\tLambda: 0.5000, Class: 0.100079, CORAL: 0.429826, Total_Loss: 0.314992\n","Train Epoch: 27 [22/30]\tLambda: 0.5000, Class: 0.111069, CORAL: 0.257881, Total_Loss: 0.240010\n","Train Epoch: 27 [23/30]\tLambda: 0.5000, Class: 0.126089, CORAL: 0.334938, Total_Loss: 0.293558\n","Train Epoch: 27 [24/30]\tLambda: 0.5000, Class: 0.115670, CORAL: 0.328713, Total_Loss: 0.280026\n","Train Epoch: 27 [25/30]\tLambda: 0.5000, Class: 0.120184, CORAL: 0.246524, Total_Loss: 0.243446\n","Train Epoch: 27 [26/30]\tLambda: 0.5000, Class: 0.116089, CORAL: 0.417299, Total_Loss: 0.324738\n","Train Epoch: 27 [27/30]\tLambda: 0.5000, Class: 0.130832, CORAL: 0.265340, Total_Loss: 0.263502\n","Train Epoch: 27 [28/30]\tLambda: 0.5000, Class: 0.132972, CORAL: 0.303879, Total_Loss: 0.284912\n","Train Epoch: 27 [29/30]\tLambda: 0.5000, Class: 0.116849, CORAL: 0.432587, Total_Loss: 0.333143\n","Train Epoch: 27 [30/30]\tLambda: 0.5000, Class: 0.135826, CORAL: 0.204634, Total_Loss: 0.238143\n","\n","Test Epoch: 27 Average loss: 1.1637, Accuracy: 6737/10000 (67.37%)\n","\n","Test Epoch: 27, Accuracy: 67.37%\n","Train Epoch: 28 [ 1/30]\tLambda: 0.5000, Class: 0.137912, CORAL: 0.491277, Total_Loss: 0.383551\n","Train Epoch: 28 [ 2/30]\tLambda: 0.5000, Class: 0.134583, CORAL: 0.261632, Total_Loss: 0.265399\n","Train Epoch: 28 [ 3/30]\tLambda: 0.5000, Class: 0.139128, CORAL: 0.194166, Total_Loss: 0.236210\n","Train Epoch: 28 [ 4/30]\tLambda: 0.5000, Class: 0.170360, CORAL: 0.331243, Total_Loss: 0.335981\n","Train Epoch: 28 [ 5/30]\tLambda: 0.5000, Class: 0.158297, CORAL: 0.311340, Total_Loss: 0.313967\n","Train Epoch: 28 [ 6/30]\tLambda: 0.5000, Class: 0.162194, CORAL: 0.334320, Total_Loss: 0.329354\n","Train Epoch: 28 [ 7/30]\tLambda: 0.5000, Class: 0.158862, CORAL: 0.313784, Total_Loss: 0.315754\n","Train Epoch: 28 [ 8/30]\tLambda: 0.5000, Class: 0.162529, CORAL: 0.259870, Total_Loss: 0.292464\n","Train Epoch: 28 [ 9/30]\tLambda: 0.5000, Class: 0.141016, CORAL: 0.202461, Total_Loss: 0.242247\n","Train Epoch: 28 [10/30]\tLambda: 0.5000, Class: 0.174341, CORAL: 0.148578, Total_Loss: 0.248630\n","Train Epoch: 28 [11/30]\tLambda: 0.5000, Class: 0.145424, CORAL: 0.159288, Total_Loss: 0.225068\n","Train Epoch: 28 [12/30]\tLambda: 0.5000, Class: 0.181446, CORAL: 0.240399, Total_Loss: 0.301645\n","Train Epoch: 28 [13/30]\tLambda: 0.5000, Class: 0.185308, CORAL: 0.203863, Total_Loss: 0.287239\n","Train Epoch: 28 [14/30]\tLambda: 0.5000, Class: 0.188897, CORAL: 0.209864, Total_Loss: 0.293829\n","Train Epoch: 28 [15/30]\tLambda: 0.5000, Class: 0.191644, CORAL: 0.264653, Total_Loss: 0.323971\n","Train Epoch: 28 [16/30]\tLambda: 0.5000, Class: 0.173971, CORAL: 0.181678, Total_Loss: 0.264810\n","Train Epoch: 28 [17/30]\tLambda: 0.5000, Class: 0.169537, CORAL: 0.279827, Total_Loss: 0.309450\n","Train Epoch: 28 [18/30]\tLambda: 0.5000, Class: 0.138781, CORAL: 0.174708, Total_Loss: 0.226135\n","Train Epoch: 28 [19/30]\tLambda: 0.5000, Class: 0.138927, CORAL: 0.278276, Total_Loss: 0.278065\n","Train Epoch: 28 [20/30]\tLambda: 0.5000, Class: 0.185444, CORAL: 0.346525, Total_Loss: 0.358706\n","Train Epoch: 28 [21/30]\tLambda: 0.5000, Class: 0.142935, CORAL: 0.295028, Total_Loss: 0.290449\n","Train Epoch: 28 [22/30]\tLambda: 0.5000, Class: 0.132827, CORAL: 0.389976, Total_Loss: 0.327815\n","Train Epoch: 28 [23/30]\tLambda: 0.5000, Class: 0.124960, CORAL: 0.197514, Total_Loss: 0.223717\n","Train Epoch: 28 [24/30]\tLambda: 0.5000, Class: 0.136721, CORAL: 0.311761, Total_Loss: 0.292602\n","Train Epoch: 28 [25/30]\tLambda: 0.5000, Class: 0.159793, CORAL: 0.344236, Total_Loss: 0.331911\n","Train Epoch: 28 [26/30]\tLambda: 0.5000, Class: 0.138018, CORAL: 0.126247, Total_Loss: 0.201142\n","Train Epoch: 28 [27/30]\tLambda: 0.5000, Class: 0.148457, CORAL: 0.229843, Total_Loss: 0.263379\n","Train Epoch: 28 [28/30]\tLambda: 0.5000, Class: 0.171998, CORAL: 0.189493, Total_Loss: 0.266745\n","Train Epoch: 28 [29/30]\tLambda: 0.5000, Class: 0.166445, CORAL: 0.150506, Total_Loss: 0.241698\n","Train Epoch: 28 [30/30]\tLambda: 0.5000, Class: 0.132927, CORAL: 0.191152, Total_Loss: 0.228504\n","\n","Test Epoch: 28 Average loss: 1.1487, Accuracy: 6799/10000 (67.99%)\n","\n","Test Epoch: 28, Accuracy: 67.99%\n","Train Epoch: 29 [ 1/30]\tLambda: 0.5000, Class: 0.145227, CORAL: 0.180616, Total_Loss: 0.235535\n","Train Epoch: 29 [ 2/30]\tLambda: 0.5000, Class: 0.146506, CORAL: 0.092096, Total_Loss: 0.192554\n","Train Epoch: 29 [ 3/30]\tLambda: 0.5000, Class: 0.136996, CORAL: 0.248191, Total_Loss: 0.261091\n","Train Epoch: 29 [ 4/30]\tLambda: 0.5000, Class: 0.142988, CORAL: 0.209872, Total_Loss: 0.247925\n","Train Epoch: 29 [ 5/30]\tLambda: 0.5000, Class: 0.117267, CORAL: 0.523031, Total_Loss: 0.378783\n","Train Epoch: 29 [ 6/30]\tLambda: 0.5000, Class: 0.123344, CORAL: 0.245560, Total_Loss: 0.246124\n","Train Epoch: 29 [ 7/30]\tLambda: 0.5000, Class: 0.133736, CORAL: 0.139206, Total_Loss: 0.203339\n","Train Epoch: 29 [ 8/30]\tLambda: 0.5000, Class: 0.123635, CORAL: 0.246552, Total_Loss: 0.246911\n","Train Epoch: 29 [ 9/30]\tLambda: 0.5000, Class: 0.134079, CORAL: 0.393343, Total_Loss: 0.330751\n","Train Epoch: 29 [10/30]\tLambda: 0.5000, Class: 0.114771, CORAL: 0.405925, Total_Loss: 0.317733\n","Train Epoch: 29 [11/30]\tLambda: 0.5000, Class: 0.130665, CORAL: 0.285789, Total_Loss: 0.273560\n","Train Epoch: 29 [12/30]\tLambda: 0.5000, Class: 0.122573, CORAL: 0.317659, Total_Loss: 0.281402\n","Train Epoch: 29 [13/30]\tLambda: 0.5000, Class: 0.125026, CORAL: 0.357659, Total_Loss: 0.303855\n","Train Epoch: 29 [14/30]\tLambda: 0.5000, Class: 0.132823, CORAL: 0.229780, Total_Loss: 0.247714\n","Train Epoch: 29 [15/30]\tLambda: 0.5000, Class: 0.134307, CORAL: 0.282547, Total_Loss: 0.275580\n","Train Epoch: 29 [16/30]\tLambda: 0.5000, Class: 0.118902, CORAL: 0.495918, Total_Loss: 0.366861\n","Train Epoch: 29 [17/30]\tLambda: 0.5000, Class: 0.135456, CORAL: 0.305933, Total_Loss: 0.288423\n","Train Epoch: 29 [18/30]\tLambda: 0.5000, Class: 0.169769, CORAL: 0.282357, Total_Loss: 0.310947\n","Train Epoch: 29 [19/30]\tLambda: 0.5000, Class: 0.165360, CORAL: 0.188882, Total_Loss: 0.259801\n","Train Epoch: 29 [20/30]\tLambda: 0.5000, Class: 0.125693, CORAL: 0.238415, Total_Loss: 0.244900\n","Train Epoch: 29 [21/30]\tLambda: 0.5000, Class: 0.141683, CORAL: 0.220670, Total_Loss: 0.252018\n","Train Epoch: 29 [22/30]\tLambda: 0.5000, Class: 0.167213, CORAL: 0.211262, Total_Loss: 0.272844\n","Train Epoch: 29 [23/30]\tLambda: 0.5000, Class: 0.169323, CORAL: 0.099734, Total_Loss: 0.219190\n","Train Epoch: 29 [24/30]\tLambda: 0.5000, Class: 0.164357, CORAL: 0.161590, Total_Loss: 0.245152\n","Train Epoch: 29 [25/30]\tLambda: 0.5000, Class: 0.148459, CORAL: 0.327709, Total_Loss: 0.312313\n","Train Epoch: 29 [26/30]\tLambda: 0.5000, Class: 0.154180, CORAL: 0.283051, Total_Loss: 0.295706\n","Train Epoch: 29 [27/30]\tLambda: 0.5000, Class: 0.144887, CORAL: 0.140570, Total_Loss: 0.215172\n","Train Epoch: 29 [28/30]\tLambda: 0.5000, Class: 0.167762, CORAL: 0.185808, Total_Loss: 0.260666\n","Train Epoch: 29 [29/30]\tLambda: 0.5000, Class: 0.167342, CORAL: 0.226530, Total_Loss: 0.280607\n","Train Epoch: 29 [30/30]\tLambda: 0.5000, Class: 0.130164, CORAL: 0.152820, Total_Loss: 0.206574\n","\n","Test Epoch: 29 Average loss: 1.1347, Accuracy: 6821/10000 (68.21%)\n","\n","Test Epoch: 29, Accuracy: 68.21%\n","Train Epoch: 30 [ 1/30]\tLambda: 0.5000, Class: 0.143520, CORAL: 0.261745, Total_Loss: 0.274393\n","Train Epoch: 30 [ 2/30]\tLambda: 0.5000, Class: 0.170720, CORAL: 0.229532, Total_Loss: 0.285486\n","Train Epoch: 30 [ 3/30]\tLambda: 0.5000, Class: 0.155137, CORAL: 0.234222, Total_Loss: 0.272248\n","Train Epoch: 30 [ 4/30]\tLambda: 0.5000, Class: 0.142556, CORAL: 0.340743, Total_Loss: 0.312928\n","Train Epoch: 30 [ 5/30]\tLambda: 0.5000, Class: 0.138368, CORAL: 0.148778, Total_Loss: 0.212758\n","Train Epoch: 30 [ 6/30]\tLambda: 0.5000, Class: 0.118283, CORAL: 0.236247, Total_Loss: 0.236406\n","Train Epoch: 30 [ 7/30]\tLambda: 0.5000, Class: 0.124085, CORAL: 0.339023, Total_Loss: 0.293596\n","Train Epoch: 30 [ 8/30]\tLambda: 0.5000, Class: 0.158192, CORAL: 0.393899, Total_Loss: 0.355141\n","Train Epoch: 30 [ 9/30]\tLambda: 0.5000, Class: 0.137188, CORAL: 0.123927, Total_Loss: 0.199152\n","Train Epoch: 30 [10/30]\tLambda: 0.5000, Class: 0.132335, CORAL: 0.228042, Total_Loss: 0.246356\n","Train Epoch: 30 [11/30]\tLambda: 0.5000, Class: 0.161518, CORAL: 0.336582, Total_Loss: 0.329809\n","Train Epoch: 30 [12/30]\tLambda: 0.5000, Class: 0.128915, CORAL: 0.226316, Total_Loss: 0.242072\n","Train Epoch: 30 [13/30]\tLambda: 0.5000, Class: 0.124518, CORAL: 0.153046, Total_Loss: 0.201040\n","Train Epoch: 30 [14/30]\tLambda: 0.5000, Class: 0.114965, CORAL: 0.210526, Total_Loss: 0.220228\n","Train Epoch: 30 [15/30]\tLambda: 0.5000, Class: 0.130739, CORAL: 0.268569, Total_Loss: 0.265023\n","Train Epoch: 30 [16/30]\tLambda: 0.5000, Class: 0.116423, CORAL: 0.296753, Total_Loss: 0.264799\n","Train Epoch: 30 [17/30]\tLambda: 0.5000, Class: 0.138799, CORAL: 0.428618, Total_Loss: 0.353109\n","Train Epoch: 30 [18/30]\tLambda: 0.5000, Class: 0.150229, CORAL: 0.190841, Total_Loss: 0.245650\n","Train Epoch: 30 [19/30]\tLambda: 0.5000, Class: 0.135883, CORAL: 0.192305, Total_Loss: 0.232035\n","Train Epoch: 30 [20/30]\tLambda: 0.5000, Class: 0.143301, CORAL: 0.177748, Total_Loss: 0.232175\n","Train Epoch: 30 [21/30]\tLambda: 0.5000, Class: 0.148720, CORAL: 0.292943, Total_Loss: 0.295191\n","Train Epoch: 30 [22/30]\tLambda: 0.5000, Class: 0.128307, CORAL: 0.277622, Total_Loss: 0.267118\n","Train Epoch: 30 [23/30]\tLambda: 0.5000, Class: 0.149570, CORAL: 0.409862, Total_Loss: 0.354501\n","Train Epoch: 30 [24/30]\tLambda: 0.5000, Class: 0.134941, CORAL: 0.445167, Total_Loss: 0.357524\n","Train Epoch: 30 [25/30]\tLambda: 0.5000, Class: 0.121690, CORAL: 0.281715, Total_Loss: 0.262547\n","Train Epoch: 30 [26/30]\tLambda: 0.5000, Class: 0.136035, CORAL: 0.263322, Total_Loss: 0.267696\n","Train Epoch: 30 [27/30]\tLambda: 0.5000, Class: 0.138083, CORAL: 0.270242, Total_Loss: 0.273204\n","Train Epoch: 30 [28/30]\tLambda: 0.5000, Class: 0.145875, CORAL: 0.196375, Total_Loss: 0.244063\n","Train Epoch: 30 [29/30]\tLambda: 0.5000, Class: 0.141409, CORAL: 0.263855, Total_Loss: 0.273336\n","Train Epoch: 30 [30/30]\tLambda: 0.5000, Class: 0.135282, CORAL: 0.352077, Total_Loss: 0.311320\n","\n","Test Epoch: 30 Average loss: 1.1502, Accuracy: 6795/10000 (67.95%)\n","\n","Test Epoch: 30, Accuracy: 67.95%\n","Train Epoch: 31 [ 1/30]\tLambda: 0.5000, Class: 0.155377, CORAL: 0.312080, Total_Loss: 0.311417\n","Train Epoch: 31 [ 2/30]\tLambda: 0.5000, Class: 0.131394, CORAL: 0.201083, Total_Loss: 0.231936\n","Train Epoch: 31 [ 3/30]\tLambda: 0.5000, Class: 0.156199, CORAL: 0.307144, Total_Loss: 0.309771\n","Train Epoch: 31 [ 4/30]\tLambda: 0.5000, Class: 0.171067, CORAL: 0.276435, Total_Loss: 0.309284\n","Train Epoch: 31 [ 5/30]\tLambda: 0.5000, Class: 0.155285, CORAL: 0.139921, Total_Loss: 0.225245\n","Train Epoch: 31 [ 6/30]\tLambda: 0.5000, Class: 0.156694, CORAL: 0.338457, Total_Loss: 0.325923\n","Train Epoch: 31 [ 7/30]\tLambda: 0.5000, Class: 0.144089, CORAL: 0.190576, Total_Loss: 0.239376\n","Train Epoch: 31 [ 8/30]\tLambda: 0.5000, Class: 0.153505, CORAL: 0.209079, Total_Loss: 0.258044\n","Train Epoch: 31 [ 9/30]\tLambda: 0.5000, Class: 0.133029, CORAL: 0.181484, Total_Loss: 0.223771\n","Train Epoch: 31 [10/30]\tLambda: 0.5000, Class: 0.150915, CORAL: 0.245932, Total_Loss: 0.273881\n","Train Epoch: 31 [11/30]\tLambda: 0.5000, Class: 0.160850, CORAL: 0.363360, Total_Loss: 0.342530\n","Train Epoch: 31 [12/30]\tLambda: 0.5000, Class: 0.170622, CORAL: 0.121232, Total_Loss: 0.231237\n","Train Epoch: 31 [13/30]\tLambda: 0.5000, Class: 0.142626, CORAL: 0.296756, Total_Loss: 0.291005\n","Train Epoch: 31 [14/30]\tLambda: 0.5000, Class: 0.185208, CORAL: 0.156335, Total_Loss: 0.263376\n","Train Epoch: 31 [15/30]\tLambda: 0.5000, Class: 0.149796, CORAL: 0.193273, Total_Loss: 0.246432\n","Train Epoch: 31 [16/30]\tLambda: 0.5000, Class: 0.131603, CORAL: 0.219198, Total_Loss: 0.241202\n","Train Epoch: 31 [17/30]\tLambda: 0.5000, Class: 0.125888, CORAL: 0.317996, Total_Loss: 0.284886\n","Train Epoch: 31 [18/30]\tLambda: 0.5000, Class: 0.125337, CORAL: 0.405071, Total_Loss: 0.327872\n","Train Epoch: 31 [19/30]\tLambda: 0.5000, Class: 0.139860, CORAL: 0.283434, Total_Loss: 0.281577\n","Train Epoch: 31 [20/30]\tLambda: 0.5000, Class: 0.153922, CORAL: 0.265211, Total_Loss: 0.286527\n","Train Epoch: 31 [21/30]\tLambda: 0.5000, Class: 0.155388, CORAL: 0.251305, Total_Loss: 0.281040\n","Train Epoch: 31 [22/30]\tLambda: 0.5000, Class: 0.151453, CORAL: 0.158052, Total_Loss: 0.230479\n","Train Epoch: 31 [23/30]\tLambda: 0.5000, Class: 0.131566, CORAL: 0.402398, Total_Loss: 0.332764\n","Train Epoch: 31 [24/30]\tLambda: 0.5000, Class: 0.120432, CORAL: 0.418683, Total_Loss: 0.329774\n","Train Epoch: 31 [25/30]\tLambda: 0.5000, Class: 0.142379, CORAL: 0.411181, Total_Loss: 0.347969\n","Train Epoch: 31 [26/30]\tLambda: 0.5000, Class: 0.139952, CORAL: 0.280687, Total_Loss: 0.280296\n","Train Epoch: 31 [27/30]\tLambda: 0.5000, Class: 0.157338, CORAL: 0.141035, Total_Loss: 0.227855\n","Train Epoch: 31 [28/30]\tLambda: 0.5000, Class: 0.139987, CORAL: 0.363256, Total_Loss: 0.321615\n","Train Epoch: 31 [29/30]\tLambda: 0.5000, Class: 0.158798, CORAL: 0.223849, Total_Loss: 0.270722\n","Train Epoch: 31 [30/30]\tLambda: 0.5000, Class: 0.150514, CORAL: 0.173163, Total_Loss: 0.237096\n","\n","Test Epoch: 31 Average loss: 1.1534, Accuracy: 6758/10000 (67.58%)\n","\n","Test Epoch: 31, Accuracy: 67.58%\n","Train Epoch: 32 [ 1/30]\tLambda: 0.5000, Class: 0.168257, CORAL: 0.210412, Total_Loss: 0.273463\n","Train Epoch: 32 [ 2/30]\tLambda: 0.5000, Class: 0.165542, CORAL: 0.316652, Total_Loss: 0.323868\n","Train Epoch: 32 [ 3/30]\tLambda: 0.5000, Class: 0.162257, CORAL: 0.175784, Total_Loss: 0.250149\n","Train Epoch: 32 [ 4/30]\tLambda: 0.5000, Class: 0.173427, CORAL: 0.135318, Total_Loss: 0.241086\n","Train Epoch: 32 [ 5/30]\tLambda: 0.5000, Class: 0.130730, CORAL: 0.275222, Total_Loss: 0.268341\n","Train Epoch: 32 [ 6/30]\tLambda: 0.5000, Class: 0.148532, CORAL: 0.325709, Total_Loss: 0.311386\n","Train Epoch: 32 [ 7/30]\tLambda: 0.5000, Class: 0.148011, CORAL: 0.162363, Total_Loss: 0.229192\n","Train Epoch: 32 [ 8/30]\tLambda: 0.5000, Class: 0.140273, CORAL: 0.121461, Total_Loss: 0.201003\n","Train Epoch: 32 [ 9/30]\tLambda: 0.5000, Class: 0.160550, CORAL: 0.253117, Total_Loss: 0.287108\n","Train Epoch: 32 [10/30]\tLambda: 0.5000, Class: 0.126347, CORAL: 0.336600, Total_Loss: 0.294647\n","Train Epoch: 32 [11/30]\tLambda: 0.5000, Class: 0.135583, CORAL: 0.181447, Total_Loss: 0.226307\n","Train Epoch: 32 [12/30]\tLambda: 0.5000, Class: 0.136230, CORAL: 0.198582, Total_Loss: 0.235521\n","Train Epoch: 32 [13/30]\tLambda: 0.5000, Class: 0.122636, CORAL: 0.310487, Total_Loss: 0.277880\n","Train Epoch: 32 [14/30]\tLambda: 0.5000, Class: 0.139695, CORAL: 0.110740, Total_Loss: 0.195065\n","Train Epoch: 32 [15/30]\tLambda: 0.5000, Class: 0.138121, CORAL: 0.190699, Total_Loss: 0.233471\n","Train Epoch: 32 [16/30]\tLambda: 0.5000, Class: 0.140618, CORAL: 0.159984, Total_Loss: 0.220610\n","Train Epoch: 32 [17/30]\tLambda: 0.5000, Class: 0.137600, CORAL: 0.286327, Total_Loss: 0.280764\n","Train Epoch: 32 [18/30]\tLambda: 0.5000, Class: 0.130564, CORAL: 0.205020, Total_Loss: 0.233074\n","Train Epoch: 32 [19/30]\tLambda: 0.5000, Class: 0.136691, CORAL: 0.146035, Total_Loss: 0.209709\n","Train Epoch: 32 [20/30]\tLambda: 0.5000, Class: 0.103689, CORAL: 0.330351, Total_Loss: 0.268865\n","Train Epoch: 32 [21/30]\tLambda: 0.5000, Class: 0.118765, CORAL: 0.190369, Total_Loss: 0.213950\n","Train Epoch: 32 [22/30]\tLambda: 0.5000, Class: 0.132409, CORAL: 0.332613, Total_Loss: 0.298715\n","Train Epoch: 32 [23/30]\tLambda: 0.5000, Class: 0.142887, CORAL: 0.198177, Total_Loss: 0.241976\n","Train Epoch: 32 [24/30]\tLambda: 0.5000, Class: 0.103079, CORAL: 0.294098, Total_Loss: 0.250129\n","Train Epoch: 32 [25/30]\tLambda: 0.5000, Class: 0.110958, CORAL: 0.269314, Total_Loss: 0.245615\n","Train Epoch: 32 [26/30]\tLambda: 0.5000, Class: 0.100544, CORAL: 0.325233, Total_Loss: 0.263160\n","Train Epoch: 32 [27/30]\tLambda: 0.5000, Class: 0.098220, CORAL: 0.372923, Total_Loss: 0.284681\n","Train Epoch: 32 [28/30]\tLambda: 0.5000, Class: 0.109886, CORAL: 0.438899, Total_Loss: 0.329335\n","Train Epoch: 32 [29/30]\tLambda: 0.5000, Class: 0.109941, CORAL: 0.405218, Total_Loss: 0.312550\n","Train Epoch: 32 [30/30]\tLambda: 0.5000, Class: 0.130751, CORAL: 0.195818, Total_Loss: 0.228660\n","\n","Test Epoch: 32 Average loss: 1.1646, Accuracy: 6751/10000 (67.51%)\n","\n","Test Epoch: 32, Accuracy: 67.51%\n","Train Epoch: 33 [ 1/30]\tLambda: 0.5000, Class: 0.140499, CORAL: 0.449300, Total_Loss: 0.365149\n","Train Epoch: 33 [ 2/30]\tLambda: 0.5000, Class: 0.150260, CORAL: 0.174200, Total_Loss: 0.237360\n","Train Epoch: 33 [ 3/30]\tLambda: 0.5000, Class: 0.120674, CORAL: 0.269107, Total_Loss: 0.255228\n","Train Epoch: 33 [ 4/30]\tLambda: 0.5000, Class: 0.139912, CORAL: 0.261069, Total_Loss: 0.270447\n","Train Epoch: 33 [ 5/30]\tLambda: 0.5000, Class: 0.152700, CORAL: 0.330797, Total_Loss: 0.318098\n","Train Epoch: 33 [ 6/30]\tLambda: 0.5000, Class: 0.145865, CORAL: 0.333101, Total_Loss: 0.312415\n","Train Epoch: 33 [ 7/30]\tLambda: 0.5000, Class: 0.150300, CORAL: 0.246061, Total_Loss: 0.273330\n","Train Epoch: 33 [ 8/30]\tLambda: 0.5000, Class: 0.158035, CORAL: 0.191601, Total_Loss: 0.253836\n","Train Epoch: 33 [ 9/30]\tLambda: 0.5000, Class: 0.151624, CORAL: 0.260198, Total_Loss: 0.281723\n","Train Epoch: 33 [10/30]\tLambda: 0.5000, Class: 0.156166, CORAL: 0.288679, Total_Loss: 0.300506\n","Train Epoch: 33 [11/30]\tLambda: 0.5000, Class: 0.170010, CORAL: 0.232988, Total_Loss: 0.286504\n","Train Epoch: 33 [12/30]\tLambda: 0.5000, Class: 0.165976, CORAL: 0.193536, Total_Loss: 0.262744\n","Train Epoch: 33 [13/30]\tLambda: 0.5000, Class: 0.128789, CORAL: 0.171331, Total_Loss: 0.214455\n","Train Epoch: 33 [14/30]\tLambda: 0.5000, Class: 0.153866, CORAL: 0.182983, Total_Loss: 0.245358\n","Train Epoch: 33 [15/30]\tLambda: 0.5000, Class: 0.137986, CORAL: 0.277284, Total_Loss: 0.276628\n","Train Epoch: 33 [16/30]\tLambda: 0.5000, Class: 0.176574, CORAL: 0.233783, Total_Loss: 0.293465\n","Train Epoch: 33 [17/30]\tLambda: 0.5000, Class: 0.160156, CORAL: 0.190853, Total_Loss: 0.255582\n","Train Epoch: 33 [18/30]\tLambda: 0.5000, Class: 0.146527, CORAL: 0.362652, Total_Loss: 0.327853\n","Train Epoch: 33 [19/30]\tLambda: 0.5000, Class: 0.165901, CORAL: 0.327878, Total_Loss: 0.329840\n","Train Epoch: 33 [20/30]\tLambda: 0.5000, Class: 0.148765, CORAL: 0.150159, Total_Loss: 0.223844\n","Train Epoch: 33 [21/30]\tLambda: 0.5000, Class: 0.154627, CORAL: 0.199114, Total_Loss: 0.254184\n","Train Epoch: 33 [22/30]\tLambda: 0.5000, Class: 0.170671, CORAL: 0.209033, Total_Loss: 0.275187\n","Train Epoch: 33 [23/30]\tLambda: 0.5000, Class: 0.138700, CORAL: 0.554758, Total_Loss: 0.416079\n","Train Epoch: 33 [24/30]\tLambda: 0.5000, Class: 0.161561, CORAL: 0.241066, Total_Loss: 0.282094\n","Train Epoch: 33 [25/30]\tLambda: 0.5000, Class: 0.159617, CORAL: 0.156848, Total_Loss: 0.238040\n","Train Epoch: 33 [26/30]\tLambda: 0.5000, Class: 0.140679, CORAL: 0.156510, Total_Loss: 0.218934\n","Train Epoch: 33 [27/30]\tLambda: 0.5000, Class: 0.147974, CORAL: 0.249448, Total_Loss: 0.272698\n","Train Epoch: 33 [28/30]\tLambda: 0.5000, Class: 0.149666, CORAL: 0.247895, Total_Loss: 0.273614\n","Train Epoch: 33 [29/30]\tLambda: 0.5000, Class: 0.164512, CORAL: 0.206177, Total_Loss: 0.267600\n","Train Epoch: 33 [30/30]\tLambda: 0.5000, Class: 0.136426, CORAL: 0.332639, Total_Loss: 0.302746\n","\n","Test Epoch: 33 Average loss: 1.0990, Accuracy: 6933/10000 (69.33%)\n","\n","Test Epoch: 33, Accuracy: 69.33%\n","New best model saved with accuracy: 69.33%\n","Train Epoch: 34 [ 1/30]\tLambda: 0.5000, Class: 0.148233, CORAL: 0.218294, Total_Loss: 0.257380\n","Train Epoch: 34 [ 2/30]\tLambda: 0.5000, Class: 0.129769, CORAL: 0.268459, Total_Loss: 0.263998\n","Train Epoch: 34 [ 3/30]\tLambda: 0.5000, Class: 0.137734, CORAL: 0.203541, Total_Loss: 0.239504\n","Train Epoch: 34 [ 4/30]\tLambda: 0.5000, Class: 0.135986, CORAL: 0.251880, Total_Loss: 0.261926\n","Train Epoch: 34 [ 5/30]\tLambda: 0.5000, Class: 0.133881, CORAL: 0.252285, Total_Loss: 0.260024\n","Train Epoch: 34 [ 6/30]\tLambda: 0.5000, Class: 0.131966, CORAL: 0.219498, Total_Loss: 0.241715\n","Train Epoch: 34 [ 7/30]\tLambda: 0.5000, Class: 0.114764, CORAL: 0.177351, Total_Loss: 0.203439\n","Train Epoch: 34 [ 8/30]\tLambda: 0.5000, Class: 0.137687, CORAL: 0.244389, Total_Loss: 0.259881\n","Train Epoch: 34 [ 9/30]\tLambda: 0.5000, Class: 0.127047, CORAL: 0.218137, Total_Loss: 0.236115\n","Train Epoch: 34 [10/30]\tLambda: 0.5000, Class: 0.155143, CORAL: 0.259923, Total_Loss: 0.285105\n","Train Epoch: 34 [11/30]\tLambda: 0.5000, Class: 0.115975, CORAL: 0.228022, Total_Loss: 0.229986\n","Train Epoch: 34 [12/30]\tLambda: 0.5000, Class: 0.126126, CORAL: 0.270542, Total_Loss: 0.261397\n","Train Epoch: 34 [13/30]\tLambda: 0.5000, Class: 0.126492, CORAL: 0.341064, Total_Loss: 0.297023\n","Train Epoch: 34 [14/30]\tLambda: 0.5000, Class: 0.122822, CORAL: 0.321910, Total_Loss: 0.283777\n","Train Epoch: 34 [15/30]\tLambda: 0.5000, Class: 0.133799, CORAL: 0.176445, Total_Loss: 0.222022\n","Train Epoch: 34 [16/30]\tLambda: 0.5000, Class: 0.137358, CORAL: 0.294545, Total_Loss: 0.284631\n","Train Epoch: 34 [17/30]\tLambda: 0.5000, Class: 0.134590, CORAL: 0.216578, Total_Loss: 0.242880\n","Train Epoch: 34 [18/30]\tLambda: 0.5000, Class: 0.138127, CORAL: 0.244245, Total_Loss: 0.260250\n","Train Epoch: 34 [19/30]\tLambda: 0.5000, Class: 0.145512, CORAL: 0.243768, Total_Loss: 0.267395\n","Train Epoch: 34 [20/30]\tLambda: 0.5000, Class: 0.143934, CORAL: 0.302985, Total_Loss: 0.295427\n","Train Epoch: 34 [21/30]\tLambda: 0.5000, Class: 0.135641, CORAL: 0.123186, Total_Loss: 0.197234\n","Train Epoch: 34 [22/30]\tLambda: 0.5000, Class: 0.147913, CORAL: 0.305587, Total_Loss: 0.300707\n","Train Epoch: 34 [23/30]\tLambda: 0.5000, Class: 0.130946, CORAL: 0.235857, Total_Loss: 0.248875\n","Train Epoch: 34 [24/30]\tLambda: 0.5000, Class: 0.149438, CORAL: 0.195953, Total_Loss: 0.247415\n","Train Epoch: 34 [25/30]\tLambda: 0.5000, Class: 0.138212, CORAL: 0.237500, Total_Loss: 0.256962\n","Train Epoch: 34 [26/30]\tLambda: 0.5000, Class: 0.145011, CORAL: 0.272468, Total_Loss: 0.281245\n","Train Epoch: 34 [27/30]\tLambda: 0.5000, Class: 0.111136, CORAL: 0.339271, Total_Loss: 0.280771\n","Train Epoch: 34 [28/30]\tLambda: 0.5000, Class: 0.131144, CORAL: 0.241903, Total_Loss: 0.252095\n","Train Epoch: 34 [29/30]\tLambda: 0.5000, Class: 0.124852, CORAL: 0.413780, Total_Loss: 0.331742\n","Train Epoch: 34 [30/30]\tLambda: 0.5000, Class: 0.121732, CORAL: 0.322042, Total_Loss: 0.282753\n","\n","Test Epoch: 34 Average loss: 1.0971, Accuracy: 6931/10000 (69.31%)\n","\n","Test Epoch: 34, Accuracy: 69.31%\n","Train Epoch: 35 [ 1/30]\tLambda: 0.5000, Class: 0.156873, CORAL: 0.193812, Total_Loss: 0.253779\n","Train Epoch: 35 [ 2/30]\tLambda: 0.5000, Class: 0.133344, CORAL: 0.257695, Total_Loss: 0.262192\n","Train Epoch: 35 [ 3/30]\tLambda: 0.5000, Class: 0.121758, CORAL: 0.255983, Total_Loss: 0.249750\n","Train Epoch: 35 [ 4/30]\tLambda: 0.5000, Class: 0.126434, CORAL: 0.147512, Total_Loss: 0.200190\n","Train Epoch: 35 [ 5/30]\tLambda: 0.5000, Class: 0.108733, CORAL: 0.556594, Total_Loss: 0.387029\n","Train Epoch: 35 [ 6/30]\tLambda: 0.5000, Class: 0.133865, CORAL: 0.107746, Total_Loss: 0.187738\n","Train Epoch: 35 [ 7/30]\tLambda: 0.5000, Class: 0.145038, CORAL: 0.178473, Total_Loss: 0.234274\n","Train Epoch: 35 [ 8/30]\tLambda: 0.5000, Class: 0.134288, CORAL: 0.244725, Total_Loss: 0.256650\n","Train Epoch: 35 [ 9/30]\tLambda: 0.5000, Class: 0.138128, CORAL: 0.173603, Total_Loss: 0.224930\n","Train Epoch: 35 [10/30]\tLambda: 0.5000, Class: 0.148241, CORAL: 0.212379, Total_Loss: 0.254431\n","Train Epoch: 35 [11/30]\tLambda: 0.5000, Class: 0.127633, CORAL: 0.179317, Total_Loss: 0.217292\n","Train Epoch: 35 [12/30]\tLambda: 0.5000, Class: 0.153198, CORAL: 0.180952, Total_Loss: 0.243674\n","Train Epoch: 35 [13/30]\tLambda: 0.5000, Class: 0.146988, CORAL: 0.200449, Total_Loss: 0.247212\n","Train Epoch: 35 [14/30]\tLambda: 0.5000, Class: 0.131762, CORAL: 0.283432, Total_Loss: 0.273479\n","Train Epoch: 35 [15/30]\tLambda: 0.5000, Class: 0.123724, CORAL: 0.210460, Total_Loss: 0.228954\n","Train Epoch: 35 [16/30]\tLambda: 0.5000, Class: 0.131417, CORAL: 0.237015, Total_Loss: 0.249925\n","Train Epoch: 35 [17/30]\tLambda: 0.5000, Class: 0.139316, CORAL: 0.297654, Total_Loss: 0.288143\n","Train Epoch: 35 [18/30]\tLambda: 0.5000, Class: 0.138101, CORAL: 0.184695, Total_Loss: 0.230449\n","Train Epoch: 35 [19/30]\tLambda: 0.5000, Class: 0.145246, CORAL: 0.297644, Total_Loss: 0.294068\n","Train Epoch: 35 [20/30]\tLambda: 0.5000, Class: 0.122979, CORAL: 0.267387, Total_Loss: 0.256672\n","Train Epoch: 35 [21/30]\tLambda: 0.5000, Class: 0.145693, CORAL: 0.178959, Total_Loss: 0.235173\n","Train Epoch: 35 [22/30]\tLambda: 0.5000, Class: 0.140996, CORAL: 0.288653, Total_Loss: 0.285322\n","Train Epoch: 35 [23/30]\tLambda: 0.5000, Class: 0.120826, CORAL: 0.401951, Total_Loss: 0.321802\n","Train Epoch: 35 [24/30]\tLambda: 0.5000, Class: 0.141735, CORAL: 0.238073, Total_Loss: 0.260771\n","Train Epoch: 35 [25/30]\tLambda: 0.5000, Class: 0.122703, CORAL: 0.378652, Total_Loss: 0.312029\n","Train Epoch: 35 [26/30]\tLambda: 0.5000, Class: 0.144474, CORAL: 0.333533, Total_Loss: 0.311240\n","Train Epoch: 35 [27/30]\tLambda: 0.5000, Class: 0.143091, CORAL: 0.351077, Total_Loss: 0.318629\n","Train Epoch: 35 [28/30]\tLambda: 0.5000, Class: 0.134785, CORAL: 0.221183, Total_Loss: 0.245377\n","Train Epoch: 35 [29/30]\tLambda: 0.5000, Class: 0.140797, CORAL: 0.431253, Total_Loss: 0.356423\n","Train Epoch: 35 [30/30]\tLambda: 0.5000, Class: 0.150809, CORAL: 0.215268, Total_Loss: 0.258443\n","\n","Test Epoch: 35 Average loss: 1.1614, Accuracy: 6790/10000 (67.90%)\n","\n","Test Epoch: 35, Accuracy: 67.90%\n","Train Epoch: 36 [ 1/30]\tLambda: 0.5000, Class: 0.131999, CORAL: 0.256122, Total_Loss: 0.260060\n","Train Epoch: 36 [ 2/30]\tLambda: 0.5000, Class: 0.143894, CORAL: 0.191188, Total_Loss: 0.239488\n","Train Epoch: 36 [ 3/30]\tLambda: 0.5000, Class: 0.160702, CORAL: 0.193794, Total_Loss: 0.257599\n","Train Epoch: 36 [ 4/30]\tLambda: 0.5000, Class: 0.132571, CORAL: 0.134952, Total_Loss: 0.200047\n","Train Epoch: 36 [ 5/30]\tLambda: 0.5000, Class: 0.149396, CORAL: 0.180626, Total_Loss: 0.239709\n","Train Epoch: 36 [ 6/30]\tLambda: 0.5000, Class: 0.147903, CORAL: 0.207838, Total_Loss: 0.251822\n","Train Epoch: 36 [ 7/30]\tLambda: 0.5000, Class: 0.147783, CORAL: 0.239211, Total_Loss: 0.267388\n","Train Epoch: 36 [ 8/30]\tLambda: 0.5000, Class: 0.150935, CORAL: 0.266337, Total_Loss: 0.284104\n","Train Epoch: 36 [ 9/30]\tLambda: 0.5000, Class: 0.127902, CORAL: 0.353366, Total_Loss: 0.304585\n","Train Epoch: 36 [10/30]\tLambda: 0.5000, Class: 0.136818, CORAL: 0.336370, Total_Loss: 0.305002\n","Train Epoch: 36 [11/30]\tLambda: 0.5000, Class: 0.153780, CORAL: 0.382036, Total_Loss: 0.344798\n","Train Epoch: 36 [12/30]\tLambda: 0.5000, Class: 0.140538, CORAL: 0.312606, Total_Loss: 0.296841\n","Train Epoch: 36 [13/30]\tLambda: 0.5000, Class: 0.141867, CORAL: 0.306949, Total_Loss: 0.295341\n","Train Epoch: 36 [14/30]\tLambda: 0.5000, Class: 0.130139, CORAL: 0.297900, Total_Loss: 0.279089\n","Train Epoch: 36 [15/30]\tLambda: 0.5000, Class: 0.159828, CORAL: 0.186831, Total_Loss: 0.253244\n","Train Epoch: 36 [16/30]\tLambda: 0.5000, Class: 0.121948, CORAL: 0.356446, Total_Loss: 0.300171\n","Train Epoch: 36 [17/30]\tLambda: 0.5000, Class: 0.138656, CORAL: 0.298337, Total_Loss: 0.287825\n","Train Epoch: 36 [18/30]\tLambda: 0.5000, Class: 0.182896, CORAL: 0.324512, Total_Loss: 0.345152\n","Train Epoch: 36 [19/30]\tLambda: 0.5000, Class: 0.153051, CORAL: 0.443905, Total_Loss: 0.375004\n","Train Epoch: 36 [20/30]\tLambda: 0.5000, Class: 0.163870, CORAL: 0.162330, Total_Loss: 0.245034\n","Train Epoch: 36 [21/30]\tLambda: 0.5000, Class: 0.166214, CORAL: 0.288082, Total_Loss: 0.310256\n","Train Epoch: 36 [22/30]\tLambda: 0.5000, Class: 0.146340, CORAL: 0.156530, Total_Loss: 0.224606\n","Train Epoch: 36 [23/30]\tLambda: 0.5000, Class: 0.175199, CORAL: 0.334483, Total_Loss: 0.342440\n","Train Epoch: 36 [24/30]\tLambda: 0.5000, Class: 0.173030, CORAL: 0.137580, Total_Loss: 0.241820\n","Train Epoch: 36 [25/30]\tLambda: 0.5000, Class: 0.163965, CORAL: 0.261885, Total_Loss: 0.294907\n","Train Epoch: 36 [26/30]\tLambda: 0.5000, Class: 0.180542, CORAL: 0.194507, Total_Loss: 0.277796\n","Train Epoch: 36 [27/30]\tLambda: 0.5000, Class: 0.142072, CORAL: 0.373174, Total_Loss: 0.328659\n","Train Epoch: 36 [28/30]\tLambda: 0.5000, Class: 0.147288, CORAL: 0.237214, Total_Loss: 0.265895\n","Train Epoch: 36 [29/30]\tLambda: 0.5000, Class: 0.145697, CORAL: 0.250851, Total_Loss: 0.271122\n","Train Epoch: 36 [30/30]\tLambda: 0.5000, Class: 0.146798, CORAL: 0.274675, Total_Loss: 0.284136\n","\n","Test Epoch: 36 Average loss: 1.2047, Accuracy: 6711/10000 (67.11%)\n","\n","Test Epoch: 36, Accuracy: 67.11%\n","Train Epoch: 37 [ 1/30]\tLambda: 0.5000, Class: 0.146700, CORAL: 0.276709, Total_Loss: 0.285054\n","Train Epoch: 37 [ 2/30]\tLambda: 0.5000, Class: 0.146589, CORAL: 0.123905, Total_Loss: 0.208542\n","Train Epoch: 37 [ 3/30]\tLambda: 0.5000, Class: 0.154341, CORAL: 0.208830, Total_Loss: 0.258756\n","Train Epoch: 37 [ 4/30]\tLambda: 0.5000, Class: 0.141319, CORAL: 0.155760, Total_Loss: 0.219199\n","Train Epoch: 37 [ 5/30]\tLambda: 0.5000, Class: 0.158454, CORAL: 0.410171, Total_Loss: 0.363540\n","Train Epoch: 37 [ 6/30]\tLambda: 0.5000, Class: 0.148432, CORAL: 0.305489, Total_Loss: 0.301177\n","Train Epoch: 37 [ 7/30]\tLambda: 0.5000, Class: 0.145894, CORAL: 0.309490, Total_Loss: 0.300639\n","Train Epoch: 37 [ 8/30]\tLambda: 0.5000, Class: 0.144737, CORAL: 0.201077, Total_Loss: 0.245276\n","Train Epoch: 37 [ 9/30]\tLambda: 0.5000, Class: 0.166302, CORAL: 0.237818, Total_Loss: 0.285211\n","Train Epoch: 37 [10/30]\tLambda: 0.5000, Class: 0.167256, CORAL: 0.203073, Total_Loss: 0.268792\n","Train Epoch: 37 [11/30]\tLambda: 0.5000, Class: 0.114500, CORAL: 0.482919, Total_Loss: 0.355960\n","Train Epoch: 37 [12/30]\tLambda: 0.5000, Class: 0.168804, CORAL: 0.186030, Total_Loss: 0.261819\n","Train Epoch: 37 [13/30]\tLambda: 0.5000, Class: 0.140519, CORAL: 0.171322, Total_Loss: 0.226181\n","Train Epoch: 37 [14/30]\tLambda: 0.5000, Class: 0.164644, CORAL: 0.119121, Total_Loss: 0.224204\n","Train Epoch: 37 [15/30]\tLambda: 0.5000, Class: 0.154215, CORAL: 0.203720, Total_Loss: 0.256075\n","Train Epoch: 37 [16/30]\tLambda: 0.5000, Class: 0.138770, CORAL: 0.182008, Total_Loss: 0.229774\n","Train Epoch: 37 [17/30]\tLambda: 0.5000, Class: 0.136244, CORAL: 0.222130, Total_Loss: 0.247309\n","Train Epoch: 37 [18/30]\tLambda: 0.5000, Class: 0.129936, CORAL: 0.121280, Total_Loss: 0.190576\n","Train Epoch: 37 [19/30]\tLambda: 0.5000, Class: 0.117277, CORAL: 0.507098, Total_Loss: 0.370826\n","Train Epoch: 37 [20/30]\tLambda: 0.5000, Class: 0.151982, CORAL: 0.165427, Total_Loss: 0.234695\n","Train Epoch: 37 [21/30]\tLambda: 0.5000, Class: 0.133865, CORAL: 0.165681, Total_Loss: 0.216705\n","Train Epoch: 37 [22/30]\tLambda: 0.5000, Class: 0.114722, CORAL: 0.264715, Total_Loss: 0.247079\n","Train Epoch: 37 [23/30]\tLambda: 0.5000, Class: 0.130547, CORAL: 0.256908, Total_Loss: 0.259001\n","Train Epoch: 37 [24/30]\tLambda: 0.5000, Class: 0.124812, CORAL: 0.270280, Total_Loss: 0.259952\n","Train Epoch: 37 [25/30]\tLambda: 0.5000, Class: 0.124900, CORAL: 0.479756, Total_Loss: 0.364778\n","Train Epoch: 37 [26/30]\tLambda: 0.5000, Class: 0.129891, CORAL: 0.236473, Total_Loss: 0.248127\n","Train Epoch: 37 [27/30]\tLambda: 0.5000, Class: 0.132099, CORAL: 0.319370, Total_Loss: 0.291784\n","Train Epoch: 37 [28/30]\tLambda: 0.5000, Class: 0.114634, CORAL: 0.586778, Total_Loss: 0.408023\n","Train Epoch: 37 [29/30]\tLambda: 0.5000, Class: 0.118270, CORAL: 0.424587, Total_Loss: 0.330564\n","Train Epoch: 37 [30/30]\tLambda: 0.5000, Class: 0.132368, CORAL: 0.360821, Total_Loss: 0.312778\n","\n","Test Epoch: 37 Average loss: 1.1047, Accuracy: 6963/10000 (69.63%)\n","\n","Test Epoch: 37, Accuracy: 69.63%\n","New best model saved with accuracy: 69.63%\n","Train Epoch: 38 [ 1/30]\tLambda: 0.5000, Class: 0.117794, CORAL: 0.350221, Total_Loss: 0.292904\n","Train Epoch: 38 [ 2/30]\tLambda: 0.5000, Class: 0.144100, CORAL: 0.190763, Total_Loss: 0.239481\n","Train Epoch: 38 [ 3/30]\tLambda: 0.5000, Class: 0.141934, CORAL: 0.172029, Total_Loss: 0.227949\n","Train Epoch: 38 [ 4/30]\tLambda: 0.5000, Class: 0.154741, CORAL: 0.127986, Total_Loss: 0.218734\n","Train Epoch: 38 [ 5/30]\tLambda: 0.5000, Class: 0.134559, CORAL: 0.257444, Total_Loss: 0.263280\n","Train Epoch: 38 [ 6/30]\tLambda: 0.5000, Class: 0.155403, CORAL: 0.222341, Total_Loss: 0.266574\n","Train Epoch: 38 [ 7/30]\tLambda: 0.5000, Class: 0.144826, CORAL: 0.173387, Total_Loss: 0.231519\n","Train Epoch: 38 [ 8/30]\tLambda: 0.5000, Class: 0.163361, CORAL: 0.140713, Total_Loss: 0.233718\n","Train Epoch: 38 [ 9/30]\tLambda: 0.5000, Class: 0.149264, CORAL: 0.169315, Total_Loss: 0.233921\n","Train Epoch: 38 [10/30]\tLambda: 0.5000, Class: 0.168365, CORAL: 0.211620, Total_Loss: 0.274175\n","Train Epoch: 38 [11/30]\tLambda: 0.5000, Class: 0.172519, CORAL: 0.438261, Total_Loss: 0.391650\n","Train Epoch: 38 [12/30]\tLambda: 0.5000, Class: 0.144433, CORAL: 0.186357, Total_Loss: 0.237612\n","Train Epoch: 38 [13/30]\tLambda: 0.5000, Class: 0.148693, CORAL: 0.214977, Total_Loss: 0.256181\n","Train Epoch: 38 [14/30]\tLambda: 0.5000, Class: 0.161805, CORAL: 0.312926, Total_Loss: 0.318268\n","Train Epoch: 38 [15/30]\tLambda: 0.5000, Class: 0.147968, CORAL: 0.133894, Total_Loss: 0.214915\n","Train Epoch: 38 [16/30]\tLambda: 0.5000, Class: 0.145122, CORAL: 0.154992, Total_Loss: 0.222618\n","Train Epoch: 38 [17/30]\tLambda: 0.5000, Class: 0.161580, CORAL: 0.454555, Total_Loss: 0.388858\n","Train Epoch: 38 [18/30]\tLambda: 0.5000, Class: 0.153277, CORAL: 0.191232, Total_Loss: 0.248893\n","Train Epoch: 38 [19/30]\tLambda: 0.5000, Class: 0.131781, CORAL: 0.279206, Total_Loss: 0.271384\n","Train Epoch: 38 [20/30]\tLambda: 0.5000, Class: 0.135704, CORAL: 0.182896, Total_Loss: 0.227152\n","Train Epoch: 38 [21/30]\tLambda: 0.5000, Class: 0.131683, CORAL: 0.153094, Total_Loss: 0.208230\n","Train Epoch: 38 [22/30]\tLambda: 0.5000, Class: 0.142430, CORAL: 0.305647, Total_Loss: 0.295253\n","Train Epoch: 38 [23/30]\tLambda: 0.5000, Class: 0.123805, CORAL: 0.214503, Total_Loss: 0.231056\n","Train Epoch: 38 [24/30]\tLambda: 0.5000, Class: 0.120462, CORAL: 0.545683, Total_Loss: 0.393304\n","Train Epoch: 38 [25/30]\tLambda: 0.5000, Class: 0.128320, CORAL: 0.261303, Total_Loss: 0.258971\n","Train Epoch: 38 [26/30]\tLambda: 0.5000, Class: 0.142653, CORAL: 0.107202, Total_Loss: 0.196254\n","Train Epoch: 38 [27/30]\tLambda: 0.5000, Class: 0.139611, CORAL: 0.255874, Total_Loss: 0.267548\n","Train Epoch: 38 [28/30]\tLambda: 0.5000, Class: 0.139449, CORAL: 0.303114, Total_Loss: 0.291006\n","Train Epoch: 38 [29/30]\tLambda: 0.5000, Class: 0.136105, CORAL: 0.250590, Total_Loss: 0.261400\n","Train Epoch: 38 [30/30]\tLambda: 0.5000, Class: 0.121638, CORAL: 0.391632, Total_Loss: 0.317454\n","\n","Test Epoch: 38 Average loss: 1.1419, Accuracy: 6860/10000 (68.60%)\n","\n","Test Epoch: 38, Accuracy: 68.60%\n","Train Epoch: 39 [ 1/30]\tLambda: 0.5000, Class: 0.132110, CORAL: 0.156053, Total_Loss: 0.210136\n","Train Epoch: 39 [ 2/30]\tLambda: 0.5000, Class: 0.120716, CORAL: 0.193926, Total_Loss: 0.217679\n","Train Epoch: 39 [ 3/30]\tLambda: 0.5000, Class: 0.134559, CORAL: 0.246112, Total_Loss: 0.257615\n","Train Epoch: 39 [ 4/30]\tLambda: 0.5000, Class: 0.129255, CORAL: 0.267966, Total_Loss: 0.263238\n","Train Epoch: 39 [ 5/30]\tLambda: 0.5000, Class: 0.132244, CORAL: 0.091151, Total_Loss: 0.177819\n","Train Epoch: 39 [ 6/30]\tLambda: 0.5000, Class: 0.139381, CORAL: 0.224100, Total_Loss: 0.251431\n","Train Epoch: 39 [ 7/30]\tLambda: 0.5000, Class: 0.152391, CORAL: 0.225315, Total_Loss: 0.265048\n","Train Epoch: 39 [ 8/30]\tLambda: 0.5000, Class: 0.126349, CORAL: 0.467972, Total_Loss: 0.360336\n","Train Epoch: 39 [ 9/30]\tLambda: 0.5000, Class: 0.125930, CORAL: 0.267271, Total_Loss: 0.259566\n","Train Epoch: 39 [10/30]\tLambda: 0.5000, Class: 0.139303, CORAL: 0.201970, Total_Loss: 0.240288\n","Train Epoch: 39 [11/30]\tLambda: 0.5000, Class: 0.127574, CORAL: 0.278784, Total_Loss: 0.266966\n","Train Epoch: 39 [12/30]\tLambda: 0.5000, Class: 0.125405, CORAL: 0.219636, Total_Loss: 0.235223\n","Train Epoch: 39 [13/30]\tLambda: 0.5000, Class: 0.123463, CORAL: 0.269013, Total_Loss: 0.257969\n","Train Epoch: 39 [14/30]\tLambda: 0.5000, Class: 0.145016, CORAL: 0.266135, Total_Loss: 0.278084\n","Train Epoch: 39 [15/30]\tLambda: 0.5000, Class: 0.156393, CORAL: 0.225269, Total_Loss: 0.269027\n","Train Epoch: 39 [16/30]\tLambda: 0.5000, Class: 0.134752, CORAL: 0.190112, Total_Loss: 0.229808\n","Train Epoch: 39 [17/30]\tLambda: 0.5000, Class: 0.129096, CORAL: 0.126696, Total_Loss: 0.192444\n","Train Epoch: 39 [18/30]\tLambda: 0.5000, Class: 0.126021, CORAL: 0.151068, Total_Loss: 0.201555\n","Train Epoch: 39 [19/30]\tLambda: 0.5000, Class: 0.129696, CORAL: 0.250955, Total_Loss: 0.255174\n","Train Epoch: 39 [20/30]\tLambda: 0.5000, Class: 0.127959, CORAL: 0.291155, Total_Loss: 0.273537\n","Train Epoch: 39 [21/30]\tLambda: 0.5000, Class: 0.136199, CORAL: 0.148906, Total_Loss: 0.210652\n","Train Epoch: 39 [22/30]\tLambda: 0.5000, Class: 0.110992, CORAL: 0.474957, Total_Loss: 0.348470\n","Train Epoch: 39 [23/30]\tLambda: 0.5000, Class: 0.125188, CORAL: 0.227472, Total_Loss: 0.238924\n","Train Epoch: 39 [24/30]\tLambda: 0.5000, Class: 0.129519, CORAL: 0.261315, Total_Loss: 0.260176\n","Train Epoch: 39 [25/30]\tLambda: 0.5000, Class: 0.110348, CORAL: 0.321633, Total_Loss: 0.271165\n","Train Epoch: 39 [26/30]\tLambda: 0.5000, Class: 0.119493, CORAL: 0.326596, Total_Loss: 0.282791\n","Train Epoch: 39 [27/30]\tLambda: 0.5000, Class: 0.138368, CORAL: 0.255942, Total_Loss: 0.266339\n","Train Epoch: 39 [28/30]\tLambda: 0.5000, Class: 0.143799, CORAL: 0.257989, Total_Loss: 0.272794\n","Train Epoch: 39 [29/30]\tLambda: 0.5000, Class: 0.132953, CORAL: 0.197046, Total_Loss: 0.231476\n","Train Epoch: 39 [30/30]\tLambda: 0.5000, Class: 0.134435, CORAL: 0.247648, Total_Loss: 0.258259\n","\n","Test Epoch: 39 Average loss: 1.1209, Accuracy: 6912/10000 (69.12%)\n","\n","Test Epoch: 39, Accuracy: 69.12%\n","Train Epoch: 40 [ 1/30]\tLambda: 0.5000, Class: 0.140823, CORAL: 0.146089, Total_Loss: 0.213867\n","Train Epoch: 40 [ 2/30]\tLambda: 0.5000, Class: 0.143784, CORAL: 0.421727, Total_Loss: 0.354647\n","Train Epoch: 40 [ 3/30]\tLambda: 0.5000, Class: 0.139286, CORAL: 0.311020, Total_Loss: 0.294796\n","Train Epoch: 40 [ 4/30]\tLambda: 0.5000, Class: 0.132754, CORAL: 0.313946, Total_Loss: 0.289727\n","Train Epoch: 40 [ 5/30]\tLambda: 0.5000, Class: 0.151492, CORAL: 0.309887, Total_Loss: 0.306436\n","Train Epoch: 40 [ 6/30]\tLambda: 0.5000, Class: 0.129819, CORAL: 0.270171, Total_Loss: 0.264904\n","Train Epoch: 40 [ 7/30]\tLambda: 0.5000, Class: 0.132549, CORAL: 0.346828, Total_Loss: 0.305963\n","Train Epoch: 40 [ 8/30]\tLambda: 0.5000, Class: 0.129118, CORAL: 0.201731, Total_Loss: 0.229983\n","Train Epoch: 40 [ 9/30]\tLambda: 0.5000, Class: 0.144962, CORAL: 0.133723, Total_Loss: 0.211824\n","Train Epoch: 40 [10/30]\tLambda: 0.5000, Class: 0.155052, CORAL: 0.194245, Total_Loss: 0.252175\n","Train Epoch: 40 [11/30]\tLambda: 0.5000, Class: 0.151277, CORAL: 0.253315, Total_Loss: 0.277935\n","Train Epoch: 40 [12/30]\tLambda: 0.5000, Class: 0.136482, CORAL: 0.178047, Total_Loss: 0.225506\n","Train Epoch: 40 [13/30]\tLambda: 0.5000, Class: 0.152917, CORAL: 0.124961, Total_Loss: 0.215397\n","Train Epoch: 40 [14/30]\tLambda: 0.5000, Class: 0.126710, CORAL: 0.293589, Total_Loss: 0.273504\n","Train Epoch: 40 [15/30]\tLambda: 0.5000, Class: 0.153009, CORAL: 0.176977, Total_Loss: 0.241498\n","Train Epoch: 40 [16/30]\tLambda: 0.5000, Class: 0.139661, CORAL: 0.143769, Total_Loss: 0.211545\n","Train Epoch: 40 [17/30]\tLambda: 0.5000, Class: 0.155487, CORAL: 0.155755, Total_Loss: 0.233364\n","Train Epoch: 40 [18/30]\tLambda: 0.5000, Class: 0.115108, CORAL: 0.815825, Total_Loss: 0.523020\n","Train Epoch: 40 [19/30]\tLambda: 0.5000, Class: 0.151545, CORAL: 0.195996, Total_Loss: 0.249543\n","Train Epoch: 40 [20/30]\tLambda: 0.5000, Class: 0.122454, CORAL: 0.183786, Total_Loss: 0.214347\n","Train Epoch: 40 [21/30]\tLambda: 0.5000, Class: 0.124578, CORAL: 0.685148, Total_Loss: 0.467152\n","Train Epoch: 40 [22/30]\tLambda: 0.5000, Class: 0.142498, CORAL: 0.231297, Total_Loss: 0.258147\n","Train Epoch: 40 [23/30]\tLambda: 0.5000, Class: 0.153354, CORAL: 0.346673, Total_Loss: 0.326690\n","Train Epoch: 40 [24/30]\tLambda: 0.5000, Class: 0.137959, CORAL: 0.165474, Total_Loss: 0.220696\n","Train Epoch: 40 [25/30]\tLambda: 0.5000, Class: 0.156640, CORAL: 0.343971, Total_Loss: 0.328626\n","Train Epoch: 40 [26/30]\tLambda: 0.5000, Class: 0.144074, CORAL: 0.285121, Total_Loss: 0.286634\n","Train Epoch: 40 [27/30]\tLambda: 0.5000, Class: 0.162673, CORAL: 0.293863, Total_Loss: 0.309605\n","Train Epoch: 40 [28/30]\tLambda: 0.5000, Class: 0.150833, CORAL: 0.233054, Total_Loss: 0.267360\n","Train Epoch: 40 [29/30]\tLambda: 0.5000, Class: 0.155273, CORAL: 0.168908, Total_Loss: 0.239727\n","Train Epoch: 40 [30/30]\tLambda: 0.5000, Class: 0.131231, CORAL: 0.261648, Total_Loss: 0.262055\n","\n","Test Epoch: 40 Average loss: 1.1274, Accuracy: 6900/10000 (69.00%)\n","\n","Test Epoch: 40, Accuracy: 69.00%\n","Train Epoch: 41 [ 1/30]\tLambda: 0.5000, Class: 0.159634, CORAL: 0.158196, Total_Loss: 0.238732\n","Train Epoch: 41 [ 2/30]\tLambda: 0.5000, Class: 0.165151, CORAL: 0.237624, Total_Loss: 0.283964\n","Train Epoch: 41 [ 3/30]\tLambda: 0.5000, Class: 0.183745, CORAL: 0.141921, Total_Loss: 0.254705\n","Train Epoch: 41 [ 4/30]\tLambda: 0.5000, Class: 0.149579, CORAL: 0.176356, Total_Loss: 0.237756\n","Train Epoch: 41 [ 5/30]\tLambda: 0.5000, Class: 0.155346, CORAL: 0.183369, Total_Loss: 0.247030\n","Train Epoch: 41 [ 6/30]\tLambda: 0.5000, Class: 0.148943, CORAL: 0.117761, Total_Loss: 0.207823\n","Train Epoch: 41 [ 7/30]\tLambda: 0.5000, Class: 0.152373, CORAL: 0.200739, Total_Loss: 0.252742\n","Train Epoch: 41 [ 8/30]\tLambda: 0.5000, Class: 0.144468, CORAL: 0.330530, Total_Loss: 0.309733\n","Train Epoch: 41 [ 9/30]\tLambda: 0.5000, Class: 0.151863, CORAL: 0.147493, Total_Loss: 0.225609\n","Train Epoch: 41 [10/30]\tLambda: 0.5000, Class: 0.147510, CORAL: 0.327028, Total_Loss: 0.311023\n","Train Epoch: 41 [11/30]\tLambda: 0.5000, Class: 0.141305, CORAL: 0.223331, Total_Loss: 0.252971\n","Train Epoch: 41 [12/30]\tLambda: 0.5000, Class: 0.117995, CORAL: 0.260517, Total_Loss: 0.248253\n","Train Epoch: 41 [13/30]\tLambda: 0.5000, Class: 0.126297, CORAL: 0.313956, Total_Loss: 0.283275\n","Train Epoch: 41 [14/30]\tLambda: 0.5000, Class: 0.121144, CORAL: 0.325695, Total_Loss: 0.283991\n","Train Epoch: 41 [15/30]\tLambda: 0.5000, Class: 0.153798, CORAL: 0.174005, Total_Loss: 0.240801\n","Train Epoch: 41 [16/30]\tLambda: 0.5000, Class: 0.133745, CORAL: 0.152620, Total_Loss: 0.210055\n","Train Epoch: 41 [17/30]\tLambda: 0.5000, Class: 0.139343, CORAL: 0.243956, Total_Loss: 0.261321\n","Train Epoch: 41 [18/30]\tLambda: 0.5000, Class: 0.143030, CORAL: 0.382082, Total_Loss: 0.334071\n","Train Epoch: 41 [19/30]\tLambda: 0.5000, Class: 0.135562, CORAL: 0.367134, Total_Loss: 0.319129\n","Train Epoch: 41 [20/30]\tLambda: 0.5000, Class: 0.130700, CORAL: 0.153640, Total_Loss: 0.207520\n","Train Epoch: 41 [21/30]\tLambda: 0.5000, Class: 0.122909, CORAL: 0.221966, Total_Loss: 0.233892\n","Train Epoch: 41 [22/30]\tLambda: 0.5000, Class: 0.116227, CORAL: 0.233237, Total_Loss: 0.232846\n","Train Epoch: 41 [23/30]\tLambda: 0.5000, Class: 0.118695, CORAL: 0.291598, Total_Loss: 0.264494\n","Train Epoch: 41 [24/30]\tLambda: 0.5000, Class: 0.138520, CORAL: 0.264160, Total_Loss: 0.270600\n","Train Epoch: 41 [25/30]\tLambda: 0.5000, Class: 0.119353, CORAL: 0.375046, Total_Loss: 0.306876\n","Train Epoch: 41 [26/30]\tLambda: 0.5000, Class: 0.117527, CORAL: 0.377921, Total_Loss: 0.306487\n","Train Epoch: 41 [27/30]\tLambda: 0.5000, Class: 0.124149, CORAL: 0.248255, Total_Loss: 0.248277\n","Train Epoch: 41 [28/30]\tLambda: 0.5000, Class: 0.131622, CORAL: 0.577739, Total_Loss: 0.420491\n","Train Epoch: 41 [29/30]\tLambda: 0.5000, Class: 0.118419, CORAL: 0.272010, Total_Loss: 0.254424\n","Train Epoch: 41 [30/30]\tLambda: 0.5000, Class: 0.138614, CORAL: 0.158918, Total_Loss: 0.218073\n","\n","Test Epoch: 41 Average loss: 1.1455, Accuracy: 6854/10000 (68.54%)\n","\n","Test Epoch: 41, Accuracy: 68.54%\n","Train Epoch: 42 [ 1/30]\tLambda: 0.5000, Class: 0.130177, CORAL: 0.185848, Total_Loss: 0.223101\n","Train Epoch: 42 [ 2/30]\tLambda: 0.5000, Class: 0.143871, CORAL: 0.176882, Total_Loss: 0.232312\n","Train Epoch: 42 [ 3/30]\tLambda: 0.5000, Class: 0.150805, CORAL: 0.158641, Total_Loss: 0.230125\n","Train Epoch: 42 [ 4/30]\tLambda: 0.5000, Class: 0.140112, CORAL: 0.214706, Total_Loss: 0.247465\n","Train Epoch: 42 [ 5/30]\tLambda: 0.5000, Class: 0.140541, CORAL: 0.225871, Total_Loss: 0.253476\n","Train Epoch: 42 [ 6/30]\tLambda: 0.5000, Class: 0.127922, CORAL: 0.167495, Total_Loss: 0.211669\n","Train Epoch: 42 [ 7/30]\tLambda: 0.5000, Class: 0.142412, CORAL: 0.232348, Total_Loss: 0.258586\n","Train Epoch: 42 [ 8/30]\tLambda: 0.5000, Class: 0.144105, CORAL: 0.257654, Total_Loss: 0.272933\n","Train Epoch: 42 [ 9/30]\tLambda: 0.5000, Class: 0.142525, CORAL: 0.159829, Total_Loss: 0.222439\n","Train Epoch: 42 [10/30]\tLambda: 0.5000, Class: 0.133082, CORAL: 0.141370, Total_Loss: 0.203767\n","Train Epoch: 42 [11/30]\tLambda: 0.5000, Class: 0.152229, CORAL: 0.322730, Total_Loss: 0.313593\n","Train Epoch: 42 [12/30]\tLambda: 0.5000, Class: 0.134511, CORAL: 0.148194, Total_Loss: 0.208607\n","Train Epoch: 42 [13/30]\tLambda: 0.5000, Class: 0.147655, CORAL: 0.259225, Total_Loss: 0.277268\n","Train Epoch: 42 [14/30]\tLambda: 0.5000, Class: 0.146669, CORAL: 0.068023, Total_Loss: 0.180681\n","Train Epoch: 42 [15/30]\tLambda: 0.5000, Class: 0.129020, CORAL: 0.154674, Total_Loss: 0.206357\n","Train Epoch: 42 [16/30]\tLambda: 0.5000, Class: 0.116146, CORAL: 0.438946, Total_Loss: 0.335619\n","Train Epoch: 42 [17/30]\tLambda: 0.5000, Class: 0.119531, CORAL: 0.157743, Total_Loss: 0.198403\n","Train Epoch: 42 [18/30]\tLambda: 0.5000, Class: 0.135385, CORAL: 0.148269, Total_Loss: 0.209519\n","Train Epoch: 42 [19/30]\tLambda: 0.5000, Class: 0.129545, CORAL: 0.355183, Total_Loss: 0.307136\n","Train Epoch: 42 [20/30]\tLambda: 0.5000, Class: 0.108392, CORAL: 0.174891, Total_Loss: 0.195837\n","Train Epoch: 42 [21/30]\tLambda: 0.5000, Class: 0.115711, CORAL: 0.413669, Total_Loss: 0.322545\n","Train Epoch: 42 [22/30]\tLambda: 0.5000, Class: 0.105801, CORAL: 0.397408, Total_Loss: 0.304505\n","Train Epoch: 42 [23/30]\tLambda: 0.5000, Class: 0.110957, CORAL: 0.179618, Total_Loss: 0.200766\n","Train Epoch: 42 [24/30]\tLambda: 0.5000, Class: 0.118948, CORAL: 0.201658, Total_Loss: 0.219778\n","Train Epoch: 42 [25/30]\tLambda: 0.5000, Class: 0.118521, CORAL: 0.302083, Total_Loss: 0.269562\n","Train Epoch: 42 [26/30]\tLambda: 0.5000, Class: 0.104104, CORAL: 0.248423, Total_Loss: 0.228315\n","Train Epoch: 42 [27/30]\tLambda: 0.5000, Class: 0.121544, CORAL: 0.131325, Total_Loss: 0.187206\n","Train Epoch: 42 [28/30]\tLambda: 0.5000, Class: 0.138502, CORAL: 0.220994, Total_Loss: 0.249000\n","Train Epoch: 42 [29/30]\tLambda: 0.5000, Class: 0.125219, CORAL: 0.153363, Total_Loss: 0.201901\n","Train Epoch: 42 [30/30]\tLambda: 0.5000, Class: 0.106936, CORAL: 0.261077, Total_Loss: 0.237475\n","\n","Test Epoch: 42 Average loss: 1.0681, Accuracy: 7077/10000 (70.77%)\n","\n","Test Epoch: 42, Accuracy: 70.77%\n","New best model saved with accuracy: 70.77%\n","Train Epoch: 43 [ 1/30]\tLambda: 0.5000, Class: 0.129853, CORAL: 0.253845, Total_Loss: 0.256775\n","Train Epoch: 43 [ 2/30]\tLambda: 0.5000, Class: 0.132197, CORAL: 0.249911, Total_Loss: 0.257152\n","Train Epoch: 43 [ 3/30]\tLambda: 0.5000, Class: 0.126920, CORAL: 0.249838, Total_Loss: 0.251839\n","Train Epoch: 43 [ 4/30]\tLambda: 0.5000, Class: 0.109727, CORAL: 0.238651, Total_Loss: 0.229052\n","Train Epoch: 43 [ 5/30]\tLambda: 0.5000, Class: 0.141747, CORAL: 0.193762, Total_Loss: 0.238628\n","Train Epoch: 43 [ 6/30]\tLambda: 0.5000, Class: 0.112295, CORAL: 0.184748, Total_Loss: 0.204669\n","Train Epoch: 43 [ 7/30]\tLambda: 0.5000, Class: 0.117727, CORAL: 0.219807, Total_Loss: 0.227630\n","Train Epoch: 43 [ 8/30]\tLambda: 0.5000, Class: 0.123277, CORAL: 0.254242, Total_Loss: 0.250398\n","Train Epoch: 43 [ 9/30]\tLambda: 0.5000, Class: 0.124453, CORAL: 0.204114, Total_Loss: 0.226510\n","Train Epoch: 43 [10/30]\tLambda: 0.5000, Class: 0.114838, CORAL: 0.213214, Total_Loss: 0.221445\n","Train Epoch: 43 [11/30]\tLambda: 0.5000, Class: 0.117825, CORAL: 0.248373, Total_Loss: 0.242012\n","Train Epoch: 43 [12/30]\tLambda: 0.5000, Class: 0.108340, CORAL: 0.187951, Total_Loss: 0.202316\n","Train Epoch: 43 [13/30]\tLambda: 0.5000, Class: 0.106567, CORAL: 0.214817, Total_Loss: 0.213976\n","Train Epoch: 43 [14/30]\tLambda: 0.5000, Class: 0.111615, CORAL: 0.242952, Total_Loss: 0.233091\n","Train Epoch: 43 [15/30]\tLambda: 0.5000, Class: 0.115866, CORAL: 0.459272, Total_Loss: 0.345502\n","Train Epoch: 43 [16/30]\tLambda: 0.5000, Class: 0.135099, CORAL: 0.225176, Total_Loss: 0.247686\n","Train Epoch: 43 [17/30]\tLambda: 0.5000, Class: 0.126965, CORAL: 0.452154, Total_Loss: 0.353042\n","Train Epoch: 43 [18/30]\tLambda: 0.5000, Class: 0.123092, CORAL: 0.168766, Total_Loss: 0.207475\n","Train Epoch: 43 [19/30]\tLambda: 0.5000, Class: 0.118157, CORAL: 0.240684, Total_Loss: 0.238499\n","Train Epoch: 43 [20/30]\tLambda: 0.5000, Class: 0.136375, CORAL: 0.226421, Total_Loss: 0.249585\n","Train Epoch: 43 [21/30]\tLambda: 0.5000, Class: 0.138621, CORAL: 0.240036, Total_Loss: 0.258638\n","Train Epoch: 43 [22/30]\tLambda: 0.5000, Class: 0.134349, CORAL: 0.180343, Total_Loss: 0.224520\n","Train Epoch: 43 [23/30]\tLambda: 0.5000, Class: 0.150606, CORAL: 0.181761, Total_Loss: 0.241487\n","Train Epoch: 43 [24/30]\tLambda: 0.5000, Class: 0.116828, CORAL: 0.255274, Total_Loss: 0.244465\n","Train Epoch: 43 [25/30]\tLambda: 0.5000, Class: 0.138362, CORAL: 0.444063, Total_Loss: 0.360394\n","Train Epoch: 43 [26/30]\tLambda: 0.5000, Class: 0.140091, CORAL: 0.139751, Total_Loss: 0.209966\n","Train Epoch: 43 [27/30]\tLambda: 0.5000, Class: 0.135956, CORAL: 0.216590, Total_Loss: 0.244251\n","Train Epoch: 43 [28/30]\tLambda: 0.5000, Class: 0.132439, CORAL: 0.255805, Total_Loss: 0.260341\n","Train Epoch: 43 [29/30]\tLambda: 0.5000, Class: 0.151943, CORAL: 0.502826, Total_Loss: 0.403356\n","Train Epoch: 43 [30/30]\tLambda: 0.5000, Class: 0.122845, CORAL: 0.283131, Total_Loss: 0.264411\n","\n","Test Epoch: 43 Average loss: 1.1618, Accuracy: 6840/10000 (68.40%)\n","\n","Test Epoch: 43, Accuracy: 68.40%\n","Train Epoch: 44 [ 1/30]\tLambda: 0.5000, Class: 0.131114, CORAL: 0.203895, Total_Loss: 0.233061\n","Train Epoch: 44 [ 2/30]\tLambda: 0.5000, Class: 0.119196, CORAL: 0.351594, Total_Loss: 0.294993\n","Train Epoch: 44 [ 3/30]\tLambda: 0.5000, Class: 0.134705, CORAL: 0.129697, Total_Loss: 0.199554\n","Train Epoch: 44 [ 4/30]\tLambda: 0.5000, Class: 0.146139, CORAL: 0.272638, Total_Loss: 0.282458\n","Train Epoch: 44 [ 5/30]\tLambda: 0.5000, Class: 0.133942, CORAL: 0.175862, Total_Loss: 0.221873\n","Train Epoch: 44 [ 6/30]\tLambda: 0.5000, Class: 0.152616, CORAL: 0.335790, Total_Loss: 0.320511\n","Train Epoch: 44 [ 7/30]\tLambda: 0.5000, Class: 0.158246, CORAL: 0.337135, Total_Loss: 0.326813\n","Train Epoch: 44 [ 8/30]\tLambda: 0.5000, Class: 0.126421, CORAL: 0.225535, Total_Loss: 0.239188\n","Train Epoch: 44 [ 9/30]\tLambda: 0.5000, Class: 0.140296, CORAL: 0.157994, Total_Loss: 0.219293\n","Train Epoch: 44 [10/30]\tLambda: 0.5000, Class: 0.142533, CORAL: 0.113037, Total_Loss: 0.199051\n","Train Epoch: 44 [11/30]\tLambda: 0.5000, Class: 0.136429, CORAL: 0.108195, Total_Loss: 0.190526\n","Train Epoch: 44 [12/30]\tLambda: 0.5000, Class: 0.145193, CORAL: 0.160676, Total_Loss: 0.225531\n","Train Epoch: 44 [13/30]\tLambda: 0.5000, Class: 0.141183, CORAL: 0.127164, Total_Loss: 0.204765\n","Train Epoch: 44 [14/30]\tLambda: 0.5000, Class: 0.137411, CORAL: 0.310744, Total_Loss: 0.292783\n","Train Epoch: 44 [15/30]\tLambda: 0.5000, Class: 0.141474, CORAL: 0.109160, Total_Loss: 0.196054\n","Train Epoch: 44 [16/30]\tLambda: 0.5000, Class: 0.130166, CORAL: 0.413079, Total_Loss: 0.336705\n","Train Epoch: 44 [17/30]\tLambda: 0.5000, Class: 0.139953, CORAL: 0.305111, Total_Loss: 0.292509\n","Train Epoch: 44 [18/30]\tLambda: 0.5000, Class: 0.126603, CORAL: 0.148764, Total_Loss: 0.200985\n","Train Epoch: 44 [19/30]\tLambda: 0.5000, Class: 0.133497, CORAL: 0.363099, Total_Loss: 0.315047\n","Train Epoch: 44 [20/30]\tLambda: 0.5000, Class: 0.134930, CORAL: 0.160013, Total_Loss: 0.214937\n","Train Epoch: 44 [21/30]\tLambda: 0.5000, Class: 0.109449, CORAL: 0.378373, Total_Loss: 0.298635\n","Train Epoch: 44 [22/30]\tLambda: 0.5000, Class: 0.128172, CORAL: 0.270003, Total_Loss: 0.263174\n","Train Epoch: 44 [23/30]\tLambda: 0.5000, Class: 0.140453, CORAL: 0.487136, Total_Loss: 0.384021\n","Train Epoch: 44 [24/30]\tLambda: 0.5000, Class: 0.126789, CORAL: 0.205830, Total_Loss: 0.229705\n","Train Epoch: 44 [25/30]\tLambda: 0.5000, Class: 0.129335, CORAL: 0.264459, Total_Loss: 0.261565\n","Train Epoch: 44 [26/30]\tLambda: 0.5000, Class: 0.142586, CORAL: 0.595015, Total_Loss: 0.440094\n","Train Epoch: 44 [27/30]\tLambda: 0.5000, Class: 0.130106, CORAL: 0.450461, Total_Loss: 0.355336\n","Train Epoch: 44 [28/30]\tLambda: 0.5000, Class: 0.136311, CORAL: 0.304272, Total_Loss: 0.288447\n","Train Epoch: 44 [29/30]\tLambda: 0.5000, Class: 0.122071, CORAL: 0.260747, Total_Loss: 0.252444\n","Train Epoch: 44 [30/30]\tLambda: 0.5000, Class: 0.149913, CORAL: 0.117024, Total_Loss: 0.208425\n","\n","Test Epoch: 44 Average loss: 1.1012, Accuracy: 7011/10000 (70.11%)\n","\n","Test Epoch: 44, Accuracy: 70.11%\n","Train Epoch: 45 [ 1/30]\tLambda: 0.5000, Class: 0.141665, CORAL: 0.199568, Total_Loss: 0.241449\n","Train Epoch: 45 [ 2/30]\tLambda: 0.5000, Class: 0.146834, CORAL: 0.194544, Total_Loss: 0.244106\n","Train Epoch: 45 [ 3/30]\tLambda: 0.5000, Class: 0.145265, CORAL: 0.351438, Total_Loss: 0.320985\n","Train Epoch: 45 [ 4/30]\tLambda: 0.5000, Class: 0.147496, CORAL: 0.270058, Total_Loss: 0.282525\n","Train Epoch: 45 [ 5/30]\tLambda: 0.5000, Class: 0.155854, CORAL: 0.115658, Total_Loss: 0.213683\n","Train Epoch: 45 [ 6/30]\tLambda: 0.5000, Class: 0.144422, CORAL: 0.259418, Total_Loss: 0.274131\n","Train Epoch: 45 [ 7/30]\tLambda: 0.5000, Class: 0.141597, CORAL: 0.199595, Total_Loss: 0.241395\n","Train Epoch: 45 [ 8/30]\tLambda: 0.5000, Class: 0.158661, CORAL: 0.099043, Total_Loss: 0.208182\n","Train Epoch: 45 [ 9/30]\tLambda: 0.5000, Class: 0.139994, CORAL: 0.191224, Total_Loss: 0.235606\n","Train Epoch: 45 [10/30]\tLambda: 0.5000, Class: 0.161963, CORAL: 0.181864, Total_Loss: 0.252895\n","Train Epoch: 45 [11/30]\tLambda: 0.5000, Class: 0.164704, CORAL: 0.133705, Total_Loss: 0.231556\n","Train Epoch: 45 [12/30]\tLambda: 0.5000, Class: 0.150776, CORAL: 0.410021, Total_Loss: 0.355786\n","Train Epoch: 45 [13/30]\tLambda: 0.5000, Class: 0.139340, CORAL: 0.176512, Total_Loss: 0.227596\n","Train Epoch: 45 [14/30]\tLambda: 0.5000, Class: 0.149065, CORAL: 0.130787, Total_Loss: 0.214458\n","Train Epoch: 45 [15/30]\tLambda: 0.5000, Class: 0.137814, CORAL: 0.424559, Total_Loss: 0.350094\n","Train Epoch: 45 [16/30]\tLambda: 0.5000, Class: 0.148417, CORAL: 0.188520, Total_Loss: 0.242677\n","Train Epoch: 45 [17/30]\tLambda: 0.5000, Class: 0.154518, CORAL: 0.184214, Total_Loss: 0.246625\n","Train Epoch: 45 [18/30]\tLambda: 0.5000, Class: 0.135350, CORAL: 0.217474, Total_Loss: 0.244088\n","Train Epoch: 45 [19/30]\tLambda: 0.5000, Class: 0.124837, CORAL: 0.282118, Total_Loss: 0.265896\n","Train Epoch: 45 [20/30]\tLambda: 0.5000, Class: 0.133790, CORAL: 0.322054, Total_Loss: 0.294817\n","Train Epoch: 45 [21/30]\tLambda: 0.5000, Class: 0.139266, CORAL: 0.141336, Total_Loss: 0.209934\n","Train Epoch: 45 [22/30]\tLambda: 0.5000, Class: 0.118380, CORAL: 0.156948, Total_Loss: 0.196854\n","Train Epoch: 45 [23/30]\tLambda: 0.5000, Class: 0.131177, CORAL: 0.215387, Total_Loss: 0.238870\n","Train Epoch: 45 [24/30]\tLambda: 0.5000, Class: 0.122280, CORAL: 0.314838, Total_Loss: 0.279699\n","Train Epoch: 45 [25/30]\tLambda: 0.5000, Class: 0.122934, CORAL: 0.269323, Total_Loss: 0.257596\n","Train Epoch: 45 [26/30]\tLambda: 0.5000, Class: 0.138128, CORAL: 0.220065, Total_Loss: 0.248160\n","Train Epoch: 45 [27/30]\tLambda: 0.5000, Class: 0.119097, CORAL: 0.275173, Total_Loss: 0.256683\n","Train Epoch: 45 [28/30]\tLambda: 0.5000, Class: 0.115536, CORAL: 0.170384, Total_Loss: 0.200728\n","Train Epoch: 45 [29/30]\tLambda: 0.5000, Class: 0.127945, CORAL: 0.274582, Total_Loss: 0.265236\n","Train Epoch: 45 [30/30]\tLambda: 0.5000, Class: 0.122429, CORAL: 0.259809, Total_Loss: 0.252333\n","\n","Test Epoch: 45 Average loss: 1.1217, Accuracy: 6902/10000 (69.02%)\n","\n","Test Epoch: 45, Accuracy: 69.02%\n","Train Epoch: 46 [ 1/30]\tLambda: 0.5000, Class: 0.116637, CORAL: 0.247342, Total_Loss: 0.240308\n","Train Epoch: 46 [ 2/30]\tLambda: 0.5000, Class: 0.133329, CORAL: 0.261335, Total_Loss: 0.263997\n","Train Epoch: 46 [ 3/30]\tLambda: 0.5000, Class: 0.139118, CORAL: 0.124085, Total_Loss: 0.201161\n","Train Epoch: 46 [ 4/30]\tLambda: 0.5000, Class: 0.125955, CORAL: 0.303031, Total_Loss: 0.277470\n","Train Epoch: 46 [ 5/30]\tLambda: 0.5000, Class: 0.134215, CORAL: 0.285502, Total_Loss: 0.276966\n","Train Epoch: 46 [ 6/30]\tLambda: 0.5000, Class: 0.119010, CORAL: 0.404575, Total_Loss: 0.321298\n","Train Epoch: 46 [ 7/30]\tLambda: 0.5000, Class: 0.131169, CORAL: 0.111221, Total_Loss: 0.186779\n","Train Epoch: 46 [ 8/30]\tLambda: 0.5000, Class: 0.136107, CORAL: 0.215712, Total_Loss: 0.243962\n","Train Epoch: 46 [ 9/30]\tLambda: 0.5000, Class: 0.110706, CORAL: 0.227861, Total_Loss: 0.224636\n","Train Epoch: 46 [10/30]\tLambda: 0.5000, Class: 0.113383, CORAL: 0.241397, Total_Loss: 0.234082\n","Train Epoch: 46 [11/30]\tLambda: 0.5000, Class: 0.127517, CORAL: 0.196162, Total_Loss: 0.225599\n","Train Epoch: 46 [12/30]\tLambda: 0.5000, Class: 0.119393, CORAL: 0.198393, Total_Loss: 0.218589\n","Train Epoch: 46 [13/30]\tLambda: 0.5000, Class: 0.126049, CORAL: 0.241454, Total_Loss: 0.246776\n","Train Epoch: 46 [14/30]\tLambda: 0.5000, Class: 0.123195, CORAL: 0.337386, Total_Loss: 0.291887\n","Train Epoch: 46 [15/30]\tLambda: 0.5000, Class: 0.140636, CORAL: 0.376058, Total_Loss: 0.328666\n","Train Epoch: 46 [16/30]\tLambda: 0.5000, Class: 0.112937, CORAL: 0.183752, Total_Loss: 0.204813\n","Train Epoch: 46 [17/30]\tLambda: 0.5000, Class: 0.116622, CORAL: 0.171203, Total_Loss: 0.202224\n","Train Epoch: 46 [18/30]\tLambda: 0.5000, Class: 0.122392, CORAL: 0.273809, Total_Loss: 0.259296\n","Train Epoch: 46 [19/30]\tLambda: 0.5000, Class: 0.120478, CORAL: 0.356876, Total_Loss: 0.298916\n","Train Epoch: 46 [20/30]\tLambda: 0.5000, Class: 0.134230, CORAL: 0.190611, Total_Loss: 0.229535\n","Train Epoch: 46 [21/30]\tLambda: 0.5000, Class: 0.132070, CORAL: 0.337370, Total_Loss: 0.300755\n","Train Epoch: 46 [22/30]\tLambda: 0.5000, Class: 0.117957, CORAL: 0.161948, Total_Loss: 0.198931\n","Train Epoch: 46 [23/30]\tLambda: 0.5000, Class: 0.135520, CORAL: 0.259385, Total_Loss: 0.265213\n","Train Epoch: 46 [24/30]\tLambda: 0.5000, Class: 0.126677, CORAL: 0.401303, Total_Loss: 0.327328\n","Train Epoch: 46 [25/30]\tLambda: 0.5000, Class: 0.134410, CORAL: 0.208261, Total_Loss: 0.238541\n","Train Epoch: 46 [26/30]\tLambda: 0.5000, Class: 0.143996, CORAL: 0.484210, Total_Loss: 0.386101\n","Train Epoch: 46 [27/30]\tLambda: 0.5000, Class: 0.126811, CORAL: 0.183617, Total_Loss: 0.218620\n","Train Epoch: 46 [28/30]\tLambda: 0.5000, Class: 0.142345, CORAL: 0.209423, Total_Loss: 0.247056\n","Train Epoch: 46 [29/30]\tLambda: 0.5000, Class: 0.120224, CORAL: 0.423498, Total_Loss: 0.331972\n","Train Epoch: 46 [30/30]\tLambda: 0.5000, Class: 0.142360, CORAL: 0.121404, Total_Loss: 0.203062\n","\n","Test Epoch: 46 Average loss: 1.1419, Accuracy: 6853/10000 (68.53%)\n","\n","Test Epoch: 46, Accuracy: 68.53%\n","Train Epoch: 47 [ 1/30]\tLambda: 0.5000, Class: 0.134019, CORAL: 0.164725, Total_Loss: 0.216381\n","Train Epoch: 47 [ 2/30]\tLambda: 0.5000, Class: 0.154810, CORAL: 0.188114, Total_Loss: 0.248867\n","Train Epoch: 47 [ 3/30]\tLambda: 0.5000, Class: 0.145217, CORAL: 0.210307, Total_Loss: 0.250371\n","Train Epoch: 47 [ 4/30]\tLambda: 0.5000, Class: 0.140489, CORAL: 0.276575, Total_Loss: 0.278777\n","Train Epoch: 47 [ 5/30]\tLambda: 0.5000, Class: 0.144428, CORAL: 0.157709, Total_Loss: 0.223283\n","Train Epoch: 47 [ 6/30]\tLambda: 0.5000, Class: 0.137158, CORAL: 0.415946, Total_Loss: 0.345131\n","Train Epoch: 47 [ 7/30]\tLambda: 0.5000, Class: 0.154990, CORAL: 0.147524, Total_Loss: 0.228751\n","Train Epoch: 47 [ 8/30]\tLambda: 0.5000, Class: 0.130143, CORAL: 0.118532, Total_Loss: 0.189409\n","Train Epoch: 47 [ 9/30]\tLambda: 0.5000, Class: 0.116198, CORAL: 0.183828, Total_Loss: 0.208112\n","Train Epoch: 47 [10/30]\tLambda: 0.5000, Class: 0.142992, CORAL: 0.209518, Total_Loss: 0.247751\n","Train Epoch: 47 [11/30]\tLambda: 0.5000, Class: 0.134168, CORAL: 0.197348, Total_Loss: 0.232842\n","Train Epoch: 47 [12/30]\tLambda: 0.5000, Class: 0.135061, CORAL: 0.180276, Total_Loss: 0.225199\n","Train Epoch: 47 [13/30]\tLambda: 0.5000, Class: 0.127623, CORAL: 0.144032, Total_Loss: 0.199639\n","Train Epoch: 47 [14/30]\tLambda: 0.5000, Class: 0.132348, CORAL: 0.126857, Total_Loss: 0.195776\n","Train Epoch: 47 [15/30]\tLambda: 0.5000, Class: 0.124269, CORAL: 0.312804, Total_Loss: 0.280672\n","Train Epoch: 47 [16/30]\tLambda: 0.5000, Class: 0.135266, CORAL: 0.262855, Total_Loss: 0.266694\n","Train Epoch: 47 [17/30]\tLambda: 0.5000, Class: 0.109476, CORAL: 0.183787, Total_Loss: 0.201370\n","Train Epoch: 47 [18/30]\tLambda: 0.5000, Class: 0.136185, CORAL: 0.542418, Total_Loss: 0.407394\n","Train Epoch: 47 [19/30]\tLambda: 0.5000, Class: 0.125395, CORAL: 0.304152, Total_Loss: 0.277470\n","Train Epoch: 47 [20/30]\tLambda: 0.5000, Class: 0.124466, CORAL: 0.377275, Total_Loss: 0.313103\n","Train Epoch: 47 [21/30]\tLambda: 0.5000, Class: 0.125963, CORAL: 0.215794, Total_Loss: 0.233860\n","Train Epoch: 47 [22/30]\tLambda: 0.5000, Class: 0.119569, CORAL: 0.368772, Total_Loss: 0.303955\n","Train Epoch: 47 [23/30]\tLambda: 0.5000, Class: 0.133213, CORAL: 0.253852, Total_Loss: 0.260139\n","Train Epoch: 47 [24/30]\tLambda: 0.5000, Class: 0.120157, CORAL: 0.193482, Total_Loss: 0.216898\n","Train Epoch: 47 [25/30]\tLambda: 0.5000, Class: 0.106626, CORAL: 0.201283, Total_Loss: 0.207267\n","Train Epoch: 47 [26/30]\tLambda: 0.5000, Class: 0.125765, CORAL: 0.144448, Total_Loss: 0.197989\n","Train Epoch: 47 [27/30]\tLambda: 0.5000, Class: 0.124290, CORAL: 0.310566, Total_Loss: 0.279573\n","Train Epoch: 47 [28/30]\tLambda: 0.5000, Class: 0.133802, CORAL: 0.241182, Total_Loss: 0.254393\n","Train Epoch: 47 [29/30]\tLambda: 0.5000, Class: 0.128573, CORAL: 0.327979, Total_Loss: 0.292563\n","Train Epoch: 47 [30/30]\tLambda: 0.5000, Class: 0.132145, CORAL: 0.203797, Total_Loss: 0.234044\n","\n","Test Epoch: 47 Average loss: 1.0940, Accuracy: 7008/10000 (70.08%)\n","\n","Test Epoch: 47, Accuracy: 70.08%\n","Train Epoch: 48 [ 1/30]\tLambda: 0.5000, Class: 0.148837, CORAL: 0.135214, Total_Loss: 0.216444\n","Train Epoch: 48 [ 2/30]\tLambda: 0.5000, Class: 0.127549, CORAL: 0.339418, Total_Loss: 0.297258\n","Train Epoch: 48 [ 3/30]\tLambda: 0.5000, Class: 0.139527, CORAL: 0.337641, Total_Loss: 0.308348\n","Train Epoch: 48 [ 4/30]\tLambda: 0.5000, Class: 0.131924, CORAL: 0.096569, Total_Loss: 0.180209\n","Train Epoch: 48 [ 5/30]\tLambda: 0.5000, Class: 0.127323, CORAL: 0.131284, Total_Loss: 0.192965\n","Train Epoch: 48 [ 6/30]\tLambda: 0.5000, Class: 0.122312, CORAL: 0.242922, Total_Loss: 0.243773\n","Train Epoch: 48 [ 7/30]\tLambda: 0.5000, Class: 0.116687, CORAL: 0.108885, Total_Loss: 0.171130\n","Train Epoch: 48 [ 8/30]\tLambda: 0.5000, Class: 0.126453, CORAL: 0.347227, Total_Loss: 0.300067\n","Train Epoch: 48 [ 9/30]\tLambda: 0.5000, Class: 0.123627, CORAL: 0.158944, Total_Loss: 0.203099\n","Train Epoch: 48 [10/30]\tLambda: 0.5000, Class: 0.133477, CORAL: 0.138116, Total_Loss: 0.202535\n","Train Epoch: 48 [11/30]\tLambda: 0.5000, Class: 0.116894, CORAL: 0.248188, Total_Loss: 0.240988\n","Train Epoch: 48 [12/30]\tLambda: 0.5000, Class: 0.121382, CORAL: 0.202531, Total_Loss: 0.222648\n","Train Epoch: 48 [13/30]\tLambda: 0.5000, Class: 0.140111, CORAL: 0.301307, Total_Loss: 0.290765\n","Train Epoch: 48 [14/30]\tLambda: 0.5000, Class: 0.118191, CORAL: 0.331291, Total_Loss: 0.283836\n","Train Epoch: 48 [15/30]\tLambda: 0.5000, Class: 0.117824, CORAL: 0.129441, Total_Loss: 0.182545\n","Train Epoch: 48 [16/30]\tLambda: 0.5000, Class: 0.145895, CORAL: 0.726307, Total_Loss: 0.509049\n","Train Epoch: 48 [17/30]\tLambda: 0.5000, Class: 0.110965, CORAL: 0.228426, Total_Loss: 0.225178\n","Train Epoch: 48 [18/30]\tLambda: 0.5000, Class: 0.126547, CORAL: 0.199115, Total_Loss: 0.226104\n","Train Epoch: 48 [19/30]\tLambda: 0.5000, Class: 0.126417, CORAL: 0.318322, Total_Loss: 0.285578\n","Train Epoch: 48 [20/30]\tLambda: 0.5000, Class: 0.124455, CORAL: 0.343272, Total_Loss: 0.296091\n","Train Epoch: 48 [21/30]\tLambda: 0.5000, Class: 0.135006, CORAL: 0.407586, Total_Loss: 0.338799\n","Train Epoch: 48 [22/30]\tLambda: 0.5000, Class: 0.118594, CORAL: 0.362718, Total_Loss: 0.299953\n","Train Epoch: 48 [23/30]\tLambda: 0.5000, Class: 0.146382, CORAL: 0.139379, Total_Loss: 0.216072\n","Train Epoch: 48 [24/30]\tLambda: 0.5000, Class: 0.131937, CORAL: 0.113241, Total_Loss: 0.188557\n","Train Epoch: 48 [25/30]\tLambda: 0.5000, Class: 0.132052, CORAL: 0.435581, Total_Loss: 0.349843\n","Train Epoch: 48 [26/30]\tLambda: 0.5000, Class: 0.128876, CORAL: 0.331388, Total_Loss: 0.294570\n","Train Epoch: 48 [27/30]\tLambda: 0.5000, Class: 0.155289, CORAL: 0.120910, Total_Loss: 0.215744\n","Train Epoch: 48 [28/30]\tLambda: 0.5000, Class: 0.133261, CORAL: 0.213655, Total_Loss: 0.240089\n","Train Epoch: 48 [29/30]\tLambda: 0.5000, Class: 0.145473, CORAL: 0.317320, Total_Loss: 0.304133\n","Train Epoch: 48 [30/30]\tLambda: 0.5000, Class: 0.153895, CORAL: 0.415387, Total_Loss: 0.361588\n","\n","Test Epoch: 48 Average loss: 1.1204, Accuracy: 6928/10000 (69.28%)\n","\n","Test Epoch: 48, Accuracy: 69.28%\n","Train Epoch: 49 [ 1/30]\tLambda: 0.5000, Class: 0.153673, CORAL: 0.218829, Total_Loss: 0.263087\n","Train Epoch: 49 [ 2/30]\tLambda: 0.5000, Class: 0.159217, CORAL: 0.113855, Total_Loss: 0.216145\n","Train Epoch: 49 [ 3/30]\tLambda: 0.5000, Class: 0.145246, CORAL: 0.235294, Total_Loss: 0.262893\n","Train Epoch: 49 [ 4/30]\tLambda: 0.5000, Class: 0.157287, CORAL: 0.211860, Total_Loss: 0.263217\n","Train Epoch: 49 [ 5/30]\tLambda: 0.5000, Class: 0.130699, CORAL: 0.126839, Total_Loss: 0.194119\n","Train Epoch: 49 [ 6/30]\tLambda: 0.5000, Class: 0.150897, CORAL: 0.170188, Total_Loss: 0.235991\n","Train Epoch: 49 [ 7/30]\tLambda: 0.5000, Class: 0.159340, CORAL: 0.252421, Total_Loss: 0.285551\n","Train Epoch: 49 [ 8/30]\tLambda: 0.5000, Class: 0.139424, CORAL: 0.157167, Total_Loss: 0.218008\n","Train Epoch: 49 [ 9/30]\tLambda: 0.5000, Class: 0.150224, CORAL: 0.236821, Total_Loss: 0.268634\n","Train Epoch: 49 [10/30]\tLambda: 0.5000, Class: 0.133191, CORAL: 0.159923, Total_Loss: 0.213152\n","Train Epoch: 49 [11/30]\tLambda: 0.5000, Class: 0.153235, CORAL: 0.135359, Total_Loss: 0.220914\n","Train Epoch: 49 [12/30]\tLambda: 0.5000, Class: 0.124373, CORAL: 0.366411, Total_Loss: 0.307578\n","Train Epoch: 49 [13/30]\tLambda: 0.5000, Class: 0.111770, CORAL: 0.131132, Total_Loss: 0.177336\n","Train Epoch: 49 [14/30]\tLambda: 0.5000, Class: 0.155044, CORAL: 0.161283, Total_Loss: 0.235685\n","Train Epoch: 49 [15/30]\tLambda: 0.5000, Class: 0.124906, CORAL: 0.179630, Total_Loss: 0.214721\n","Train Epoch: 49 [16/30]\tLambda: 0.5000, Class: 0.118095, CORAL: 0.203614, Total_Loss: 0.219902\n","Train Epoch: 49 [17/30]\tLambda: 0.5000, Class: 0.110223, CORAL: 0.205581, Total_Loss: 0.213013\n","Train Epoch: 49 [18/30]\tLambda: 0.5000, Class: 0.106416, CORAL: 0.408258, Total_Loss: 0.310546\n","Train Epoch: 49 [19/30]\tLambda: 0.5000, Class: 0.121566, CORAL: 0.157442, Total_Loss: 0.200287\n","Train Epoch: 49 [20/30]\tLambda: 0.5000, Class: 0.114365, CORAL: 0.443745, Total_Loss: 0.336238\n","Train Epoch: 49 [21/30]\tLambda: 0.5000, Class: 0.112236, CORAL: 0.078651, Total_Loss: 0.151561\n","Train Epoch: 49 [22/30]\tLambda: 0.5000, Class: 0.118133, CORAL: 0.335546, Total_Loss: 0.285906\n","Train Epoch: 49 [23/30]\tLambda: 0.5000, Class: 0.116418, CORAL: 0.528090, Total_Loss: 0.380463\n","Train Epoch: 49 [24/30]\tLambda: 0.5000, Class: 0.117256, CORAL: 0.248760, Total_Loss: 0.241636\n","Train Epoch: 49 [25/30]\tLambda: 0.5000, Class: 0.113449, CORAL: 0.171596, Total_Loss: 0.199247\n","Train Epoch: 49 [26/30]\tLambda: 0.5000, Class: 0.114864, CORAL: 0.219627, Total_Loss: 0.224677\n","Train Epoch: 49 [27/30]\tLambda: 0.5000, Class: 0.143926, CORAL: 0.157500, Total_Loss: 0.222676\n","Train Epoch: 49 [28/30]\tLambda: 0.5000, Class: 0.119993, CORAL: 0.482131, Total_Loss: 0.361058\n","Train Epoch: 49 [29/30]\tLambda: 0.5000, Class: 0.145854, CORAL: 0.274850, Total_Loss: 0.283279\n","Train Epoch: 49 [30/30]\tLambda: 0.5000, Class: 0.132287, CORAL: 0.131567, Total_Loss: 0.198070\n","\n","Test Epoch: 49 Average loss: 1.0960, Accuracy: 6975/10000 (69.75%)\n","\n","Test Epoch: 49, Accuracy: 69.75%\n","Train Epoch: 50 [ 1/30]\tLambda: 0.5000, Class: 0.125384, CORAL: 0.400348, Total_Loss: 0.325558\n","Train Epoch: 50 [ 2/30]\tLambda: 0.5000, Class: 0.150976, CORAL: 0.230727, Total_Loss: 0.266339\n","Train Epoch: 50 [ 3/30]\tLambda: 0.5000, Class: 0.126553, CORAL: 0.168388, Total_Loss: 0.210747\n","Train Epoch: 50 [ 4/30]\tLambda: 0.5000, Class: 0.139521, CORAL: 0.223449, Total_Loss: 0.251246\n","Train Epoch: 50 [ 5/30]\tLambda: 0.5000, Class: 0.133722, CORAL: 0.117187, Total_Loss: 0.192315\n","Train Epoch: 50 [ 6/30]\tLambda: 0.5000, Class: 0.123362, CORAL: 0.151765, Total_Loss: 0.199245\n","Train Epoch: 50 [ 7/30]\tLambda: 0.5000, Class: 0.117757, CORAL: 0.249738, Total_Loss: 0.242626\n","Train Epoch: 50 [ 8/30]\tLambda: 0.5000, Class: 0.125061, CORAL: 0.178906, Total_Loss: 0.214515\n","Train Epoch: 50 [ 9/30]\tLambda: 0.5000, Class: 0.136723, CORAL: 0.222350, Total_Loss: 0.247898\n","Train Epoch: 50 [10/30]\tLambda: 0.5000, Class: 0.137362, CORAL: 0.119821, Total_Loss: 0.197272\n","Train Epoch: 50 [11/30]\tLambda: 0.5000, Class: 0.137577, CORAL: 0.332649, Total_Loss: 0.303901\n","Train Epoch: 50 [12/30]\tLambda: 0.5000, Class: 0.111930, CORAL: 0.251733, Total_Loss: 0.237797\n","Train Epoch: 50 [13/30]\tLambda: 0.5000, Class: 0.120121, CORAL: 0.196270, Total_Loss: 0.218256\n","Train Epoch: 50 [14/30]\tLambda: 0.5000, Class: 0.126861, CORAL: 0.148857, Total_Loss: 0.201289\n","Train Epoch: 50 [15/30]\tLambda: 0.5000, Class: 0.115151, CORAL: 0.201070, Total_Loss: 0.215686\n","Train Epoch: 50 [16/30]\tLambda: 0.5000, Class: 0.117720, CORAL: 0.139934, Total_Loss: 0.187688\n","Train Epoch: 50 [17/30]\tLambda: 0.5000, Class: 0.124936, CORAL: 0.197274, Total_Loss: 0.223573\n","Train Epoch: 50 [18/30]\tLambda: 0.5000, Class: 0.129014, CORAL: 0.328492, Total_Loss: 0.293261\n","Train Epoch: 50 [19/30]\tLambda: 0.5000, Class: 0.121402, CORAL: 0.180992, Total_Loss: 0.211898\n","Train Epoch: 50 [20/30]\tLambda: 0.5000, Class: 0.096869, CORAL: 0.294827, Total_Loss: 0.244282\n","Train Epoch: 50 [21/30]\tLambda: 0.5000, Class: 0.111635, CORAL: 0.120027, Total_Loss: 0.171648\n","Train Epoch: 50 [22/30]\tLambda: 0.5000, Class: 0.128435, CORAL: 0.275717, Total_Loss: 0.266294\n","Train Epoch: 50 [23/30]\tLambda: 0.5000, Class: 0.118475, CORAL: 0.169656, Total_Loss: 0.203303\n","Train Epoch: 50 [24/30]\tLambda: 0.5000, Class: 0.110193, CORAL: 0.193124, Total_Loss: 0.206755\n","Train Epoch: 50 [25/30]\tLambda: 0.5000, Class: 0.099115, CORAL: 0.244477, Total_Loss: 0.221353\n","Train Epoch: 50 [26/30]\tLambda: 0.5000, Class: 0.107233, CORAL: 0.301060, Total_Loss: 0.257763\n","Train Epoch: 50 [27/30]\tLambda: 0.5000, Class: 0.103188, CORAL: 0.134077, Total_Loss: 0.170226\n","Train Epoch: 50 [28/30]\tLambda: 0.5000, Class: 0.103464, CORAL: 0.607573, Total_Loss: 0.407251\n","Train Epoch: 50 [29/30]\tLambda: 0.5000, Class: 0.112803, CORAL: 0.417865, Total_Loss: 0.321735\n","Train Epoch: 50 [30/30]\tLambda: 0.5000, Class: 0.119258, CORAL: 0.255716, Total_Loss: 0.247116\n","\n","Test Epoch: 50 Average loss: 1.1095, Accuracy: 6953/10000 (69.53%)\n","\n","Test Epoch: 50, Accuracy: 69.53%\n","\n","Test Epoch: 1 Average loss: 0.3995, Accuracy: 23744/26032 (91.21%)\n","\n","{'epoch': 1, 'average_loss': 0.3995200871651936, 'correct': 23744, 'total': 26032, 'accuracy': 91.21081745543945}\n"]}]},{"cell_type":"code","source":["prms = {'device': 'cuda' if torch.cuda.is_available() else 'cpu'}\n","\n","# deepcoral = DeepCORAL()  # Replace with your actual model initialization\n","# deepcoral.to(prms['device'])\n","\n","# Load the best model\n","# deepcoral.load_state_dict(torch.load('model_checkpoints/best_model_svhn.pth'))\n","\n","\n","\n","print(test_results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":375},"id":"qf02wbbb72vC","executionInfo":{"status":"error","timestamp":1718910385868,"user_tz":-180,"elapsed":2612,"user":{"displayName":"Dimitris Stathopoulos","userId":"13979344250441303776"}},"outputId":"632ee46c-e26a-48c4-8baf-e139159babb9"},"execution_count":27,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (32x10 and 4096x10)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-531adb39253f>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# deepcoral.load_state_dict(torch.load('model_checkpoints/best_model_svhn.pth'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcoral\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvhn_testloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-4f75d5253814>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, dataset_loader, epoch, params)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-e5b5f71103b2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, source, target)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msharedNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x10 and 4096x10)"]}]},{"cell_type":"code","source":[],"metadata":{"id":"cfKPzDOL-lvJ"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}